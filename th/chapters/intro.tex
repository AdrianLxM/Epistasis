%-----------------------------------------------------------------------------
\chapter{Introduction \label{ch:intro}}
%-----------------------------------------------------------------------------

How does one's DNA influence their risk of getting a disease? Contrary to popular belief, your future health is not ``hard wired" in your DNA. Only in a few diseases, referred as ``Mendelian diseases", are there well known, almost certain, links between genetic mutations and disease susceptibility. For the majority of what are known as ``complex traits", such as cancer or diabetes, genomic predisposition is subtle and, so far, not fully understood.

With the rapid decrease in the cost of DNA sequencing, the complete genome sequence of large cohorts of individuals can now be routinely obtained. This wealth of sequencing information is expected to ease the identification of genetic variations linked to complex traits. In this work, I investigate the analysis of genomic data in relation to complex diseases, which offers a number of important computational and statistical challenges. We tackle several steps necessary for the analysis of sequencing data and the identification of links to disease. Each step, which corresponds to a chapter in my thesis, is characterized by very different problems that need to be addressed.

\begin{itemize}

\item[i)] The first step is to analyze large amounts of information generated by DNA sequencers to obtain a set of ``genomic variants" present n each each individual. To address these big data processing problems, Chapter \ref{ch:bds} shows how we designed a programming language (BigDataScript \cite{cingolani2015bigdatascript}), that simplifies the creation robust, scalable data pipelines.

\item[ii)] Once genomic variants are obtained, we need to prioritize and filter them to discern which variants should be considered ``important" and which ones are likely to be less relevant. We created the SnpEff \& SnpSift \cite{cingolani2012program, cingolani2012using} packages that, using optimized algorithms, solve several annotation problems: a) standardizing the annotation process, b) calculating putative genetic effects, c) estimating genetic impact, d) adding several sources of genetic information, and e) facilitating variant filtering. 
					
\item[iii)] Finally, we address the problem of finding associations between interacting genetic loci and disease. One of the main problems in GWAS, known as ``missing heritability", is that most of the phenotypic variance attributed to genetic causes remains unexplained. Since interacting genetic loci (epistasis) have been pointed out as one of the possible causes of missing heritability, finding links between such interactions and disease has great significance in the field. We propose a methodology to increase the statistical power of this type of approaches by combining population-level genetic information with evolutionary information. 

\end{itemize}

In a nutshell, this thesis addresses computational, analytical, algorithmic and methodological problems of transforming raw sequencing data into biological insight in the aetiology of complex disease. In the rest of this introduction we give the background that provides motivation for our research. 

%---
\section{Genomes and genetic variants \label{sec:introRef}}
%---

DNA is composed of four basic building blocks, called ``bases'' or ``nucleotides'' \cite{alberts1995molecular}. These four nucleotides, usually abbreviated $\{A, C, G, T\}$, are Adenine, Cytosine, Guanine, and Thymine. Bases form pairs, either as $A-T$ or $C-G$, that pile-up forming two long polymers, with backbones that run in opposite directions giving rise to a double-helix structure \cite{watson1953molecular}. Arbitrarily, one of the polymers is called the positive strand and the other is called the negative strand. 

Proteins are composed by chains of amino acids and, as explained by the central dogma of biology \cite{alberts1995molecular},  DNA is the template that instructs cellular machinery how to produce proteins. There are 20 amino acids, which are the building blocks of all proteins. Each of the twenty amino acids is encoded by a group of three DNA bases called ``codon'' \cite{crick1961general}. More than one codon can code for the same amino acid (i.e. $4^3=64$ codons $ > 20 $ amino acids) allowing for code redundancy. Additionally, there are codons that mark the end of the protein, these are called ``STOP" and signal molecular machinery to end the translation process \cite{brenner1965genetic}.

Proteins compose up to 50\% of a cell's dry weight compared to 3\% of the DNA \cite{alberts1995molecular}. Proteins perform their functions mainly by interacting with other proteins, forming complex pathways that lead to a vast array of cellular functions including catalysis of chemical reactions, cell signaling, and structural conformation of the cell \cite{alberts1995molecular}. The 3-dimensional structure of the protein, also called ``tertiary structure", is tailored to bind to other proteins in a specific manner to accomplish a specific function. 

The human genome has a total of 3 Giga-base-pairs (Gb), and those bases are divided into 22 ``autosomal'' chromosome pairs (in each pair one chromsome is maternally inherited and the other paternally inherited) and ``sex" chromosomes. The longest of the autosomal chromosomes is roughly 250 Mega-bases (Mb) and the shortest one is ~50 Mb.

In order to compare DNA from different individuals (or samples), we need a ``reference genome". Having a standard reference sequence facilitates comparisons and analysis. For most well studied organisms, ``reference genome" sequences are available and current large scale sequencing projects are extending significantly the number of genomes known, e.g. one project seeks to sequence 10,000 mammalian genomes \cite{haussler2009genome}, another is targeting all microbes that live within the human gut \cite{turnbaugh2007human}. The human reference genome (e.g. GRCh37) does not correspond to the DNA of any particular person, but to a ``mosaic" of the genomes of thirteen anonymous volunteers from Buffalo, New York \cite{schneider2013genome}.

When the genome of an individual is sequenced, the DNA is compared to the ``reference genome". Most of the DNA is the same, but there are differences. These differences, generically known as ``genetic variants" (or ``variants", for short), describe the particular genetic make-up of each individual. There are several different ways a sample can differ from a reference genome. Each variant is the result of a mutations that happened at some point in the evolutionary history of the individual (or that of the reference genome). Variant types can be roughly categorized in the following way:

\begin{description}

	\item[Single nucleotide variants (SNV)] or Single nucleotide polymorphism (SNP) are the simplest and more common variants produced by single base difference (e.g. a base in the reference genome, at a given coordinate,  is an `A', whereas the sample is `C'). Depending on whether the variant was identified in an individual or in a population, it is called a Single Nucleotide Variant (SNV) or Single Nucleotide Polymorphism (SNP). It is estimated that there are roughly $3.6M$ SNPs per individual \cite{10002012integrated}. There are several biological mechanisms responsible for this type of variants: i) replication errors, ii) errors introduced by DNA repair mechanism, iii) deamination (a base is changed by hydrolysis which may not be corrected by DNA repair mechanisms), iv) tautomerism (and alteration on the hydrogen bond that results in an incorrect pairing) \cite{griffiths2005introduction}.

	\item[Multiple nucleotide polymorphism (MNP)] are sequence differences affecting several consecutive nucleotides and are typically treated as a single variant locus if they are in perfect linkage disequilibrium (e.g. reference is ‘ACG’ whereas the sample is ‘TGC’). .

	\item[Insertions (INS)] refer to a sample having one or more extra base(s) compared to the reference genome (e.g. the reference sequence is ‘AT’ and the sample is ‘ACT’). Short insertions and deletions (indels) of a chromosome region range from 1 to 20 bases in length are reported to be 10 to 30 times less frequent than SNV \cite{10002012integrated}. Small insertions are usually attributed to DNA polymerase slipping and replicating the same bases (this produces a type of insertion known as duplication). Large insertions can be caused by unequal cross-over event (during meiosis) or transposable elements.

	\item[Deletions (DEL)] are the opposite of insertions, the sample has some base(s) removed with respect to the reference genome (e.g. reference is ‘ACT’ and sample is ‘AT’). As in the case of insertions, deletions can also be caused by ribosomal slippage, cross-over events during meiosis. Those include large deletions, which can result in the loss of an exon or one or more whole genes \cite{alberts1995molecular}. Short deletions are 10 to 30 times less frequent than SNV \cite{10002012integrated}.

	\item[Copy number variations (CNVs)] arise when the sample has two or more copies of the same genomic region (e.g. a whole gene that has been duplicated or triplicated) or conversely, when the sample has fewer copies than the reference genome. Copy number variations are often attributed to homologous recombination events \cite{alberts1995molecular}.

	\item[Rearrangements] such as inversions and translocations are events that involve two or more genomic breakpoints and a reorganization of genomic segments, possibly resulting in gene fusions or loss of critical regulatory elements. Inversions, a type of rearrangement, result from a whole genomic region being inverted.

\end{description}

\noindent As humans have two copies of each autosome, variants could affect zero, one or two of the chromosomes and are called ``homozygous reference", ``heterozygous", and ``homozygous alternative" respectively. Variants are also classified based on how common they are within the population: common ($\ge 5\%$), low frequency ($\le 5\%$), or rare ($\le 0.1\%$). How these types of genetic variants influence traits or disease risk is a topic of intense research that is discussed throughout this thesis.

\section{DNA and disease risk}

It would be fair to say that the Garrod family was fascinated by urine. As a physician at King’s College, Alfred Baring Garrod, discovered gout related abnormalities in uric acid \cite{kennedy2001}. His son, Sir Archibald Garrod, was interested in a condition known as alkaptonuria, in which children are mostly asymptomatic except for producing brown or black urine, but by the age of 30 individuals develop pain in joints of the spine, hips and knees. In 1902, Archibald observed that the family inheritance pattern of alkaptonuria resembled Mendel’s recessive pattern and postulated that a mutation in a metabolic gene was responsible for the disease. Publishing his finding he gave birth to a new field of study known as ``Human biochemical genetics" \cite{kennedy2001}.

Diseases having simple inheritance patterns, such as alkaptonuria, cystic fibrosis, phenylketonuria and Huntington's are also known as Mendelian diseases \cite{kennedy2001}. The genetic components of several Mendelian diseases have been discovered since the mechanism was first elucidated by Garrod in 1902 and the process has been accelerated in recent years, thanks to the application of DNA sequencing techniques \cite{bamshad2011exome}.

In complex diseases (or complex traits), such as diabetes or Alzheimer's disease, affected individuals cannot be segregated within pedigrees (i.e. no simple pattern of inheritance can be identified). In contrast to Mendelian diseases the aetiology of complex traits is complicated due to factors such as: incomplete penetrance (symptoms are not always present in individuals who have the disease-causing mutation) and genetic heterogeneity (caused by any of a large number of alleles). This makes  it difficult to pinpoint the genetic variants that increase risk of complex disease.

\subsection{Heritability and Missing heritability}

We all know that ``tall parents tend to have tall children", which is an informal way to say that height is a highly heritable trait. It is said that there are 30 cm from the tallest 5\% to the shortest 5\% of the population and genetics account for 80\% to 90\% of this variation \cite{wood2014defining}, which means that 27cm of variance are assumed to be ``carried" by DNA variants from parents to offspring. Since 2010 the GIANT consortia has been investigating the genetic component of complex traits like height, body mass index (BMI) and waist to hip ratio (WHR). Even though they found many variants associated those traits, their findings only explain 10\% of the phenotypic variance which corresponds to only a few centimeters in height \cite{wood2014defining}.

In order to measure heritability we need a formal definition. Heritability is defined as the proportion of phenotypic variance that is attributed to genetic variations. The total phenotypic variation is assumed to be caused by a combination of ``environmental" and genetic variations $Var[P] = Var[G] + Var[E] + 2 Cov[G, E]$ 
\iffinal
\footnote{Although the referenced paper's notation does not seem absolutely consistent, we quote Emerson \textit{``A foolish consistency is the hobgoblin of little minds"} and proceed...}
\fi
.

The environmental variance $Var[E]$ is the phenotypic variance attributable only to environment, that is the variance for individuals having the same genome $Var[E] = Var[P|G]$. This can be estimated by studying monozygotic and dizygotic twins.

If the covariance factor $Cov[G, E]$ is assumed to be zero, we can define heritability as $H^2 = \frac{Var[G] }{ Var[P]}$. This is called ``broad sense heritability" because $Var[G]$ takes into account all possible forms of genetic variance: $Var[G] = Var[G_A] + Var[G_D] + Var[G_I]$, where $Var[G_A]$ is the additive variance, $Var[G_D]$ is the variance form dominant alleles, and $Var[G_I]$ is the variance form interacting alleles (epistasis). Non-additive terms are difficult to estimate, so a simpler form of heritability called ``narrow sense heritability" that only takes into account additive variance is defined as $h^2 = \frac{ Var[G_A] }{ Var[P] }$ \cite{zuk2012mystery}.

Focusing on narrow sense heritability, the concept of ``explained heritability" is defined as the part of heritability due to known variants with respect to phenotypic variation ($\pi_{explained} = h^2_{known} / h^2_{all}$). Similarly, missing heritability is defined as $\pi_{missing} = 1 - \pi_{explained} = 1 - h^2_{known} / h^2_{all}$. When all variants associated with traits are known, then $\pi_{missing} = 0$.

Until recently, it was widely assumed by the research community that the problem of missing heritability lied in finding the appropriate genetic variants to account for the numerator of the equation ($h^2_{known}$) \cite{zuk2012mystery}. However, in a series of theorems published recently, it has been proposed that there is a problem in the way the denominator is estimated \cite{zuk2012mystery}. The authors created a limiting pathway model ($LP(k)$) that accounts for epistasis (gene-gene interactions) in $k$ biological pathways. They showed that a severe inflation of $h^2_{all}$ estimators occurs even for small values of $k$ (e.g. $k \in [2,10]$). As a result, genetic variants estimated to account only for $20\%$ of heritability, could actually account for as much as $80\%$ using an appropriate model \cite{zuk2012mystery}.

Even though this result is encouraging, the problem is now shifted to detecting epistatic interactions, a problem that we discuss in section \ref{sec:epi} and Chapter \ref{ch:gwas}. In the same work \cite{zuk2012mystery}, the authors show an example of power calculation assuming relatively large genetic effect that would require sequencing roughly $5,000$ individuals to detect links to genetic variants, which is a large but nowadays not uncommon, sample size. Nevertheless other estimates place the sample size requirements as high as  $500,000$ individuals \cite{zuk2012mystery}. Even though this sounds as an extremely large number of samples, it is quickly becoming possible thanks to large technological advances and cost reductions in sequencing and genotyping technologies.

\subsection{Conclusions}

Although some genetic causes of complex traits, such as type II diabetes, have been found, only a small portion of the phenotypic variance can be explained. This might indicate that many risk variants are yet to be discovered. Recent studies on the topic of missing heritability report that these ``difficult to find genetic variants" might be in epistatic interaction (analyzed in section \ref{sec:epigwas}) or rare variants (see section \ref{sec:comonrare}). Analysis of either them requires more complex statistical models and larger sample sizes with the corresponding increase in computational requirements. In Chapter \ref{ch:gwas} of this thesis, we focus on methods for finding epistatic interactions related to complex disease and develop computationally tractable algorithms that can process data from sequencing experiments involving large number of samples in a reasonable amount of time.

%---
\section{Identification of genetic variants}
%---

Two of the main milestones in genetics were the discovery of the DNA structure in 1953 \cite{watson1953molecular}, followed by the first draft of the human genome in 2004 \cite{collins2004finishing}. The cost of sequencing the first human reference genome was around \$3 billion (unadjusted US dollars) and it was an endeavor that took around 10 years. Since that time, sequencing technology has evolved substantially so that a human genome can now be sequenced in three days for a price of less than \$1,000, according to prices estimated by Illumina, one of the main genome sequencer manufacturers.

The amount of information delivered by sequencing devices is growing faster than computer speed (Moore's law) and data storage capacity. Just as a crude example, a leading edge sequencing system is advertized to be capable of delivering 18,000 human genomes at $30 \times$ coverage per year, yielding over 3.2 PB of information. Having to process huge amounts of sequencing information poses several challenges, a problem informally known as ``data deluge''. In this section, we explain how sequencing data is generated and how the huge amount of information delivered by a sequencer can be handled in order to make the problem tractable. We want to transform this raw data into knowledge of genomic variants that contribute to disease risk with the ultimate goal to translate these risk variants into biological knowledge. As expected, processing huge datasets consisting of thousands of sample is a complex problem. In Chapter \ref{ch:bds} we show how mitigate or solve some of these issues, by designing a computer language specially tailored to tackle what are know as ``Big data" problems.

\subsection{Sequencing data}

DNA sequencing machines (or sequencers) are based on different technologies. In a nutshell, a sequencer detects a set of polymers (or chains) of DNA nucleotides and outputs a set of strings of A, C, G, and Ts. Unfortunately, current technological limitations make it impossible to ``read" a full chromosome as one long DNA sequence. Instead, modern sequencers produce a large number of ``short reads", which range from 100 bases to 20 Kilo-bases (Kb) in length, depending on the technology. Since sequencers are unable to read long DNA chains, preparing the DNA for sequencing involves fragmenting it into small pieces. These DNA fragments are a random sub-samples of the original chromosomes. Reading each part of the genome several times allows to increase accuracy and ensure that the sequencer reads as much as possible of the original chromosomes. The coverage of a sequencing experiment is defined as the number of times each base of the genome is read on average. For instance, if the sequencing experiment is designed to produce one billion reads, and each read is 150 bases long, then the total number of bases read is 150Gb. Since the human genome is 3Gb, the coverage is said to be $50 \times$.

After sequencing a sample, we have millions of reads but we do not know where these reads originate from in the genome. This is solved by aligning (also called mapping) reads to the reference genome, which is assumed to be very similar to the genome being sequenced. Once the reads are mapped, we can infer if the sample's DNA has any differences with respect to the reference genome, a problem is known as ``variant calling''. 

Although sequencing costs are dropping fast, it is still expensive to sequence thousands of samples and in some cases it makes sense to focus on specific areas of the genome. A popular experimental setup is to focus on coding regions (exons). A technique called ``exome sequencing" consists of capturing exons using a DNA chip and then sequencing the captured DNA fragments only. Exons are roughly 1.2\% of the genome, thus this technique reduces sequencing costs significantly, for which it has been widely used by many research groups although it has the disadvantage of only analysing coding genomic variation.

\subsection{Read mapping}

Once the samples have been sequenced, we have a set of reads from the sequencer. The first step in the analysis is finding the location in the reference genome where each read is supposed to originate from, a process that is complicated by a several factors: i) there are differences between the reference genome and the sample genome, ii) sequencing reads may contain errors, iii) several parts of the reference genome are quite similar making reads from those regions indistinguishable, and iv) a typical sequencing experiment generates millions of reads.

\paragraph{Local sequence alignment} We introduce a problem known as \textit{local sequence alignment}: Given two sequences $s_1$ and $s_2$ from an alphabet (e.g. $\Sigma = \{A,C,G,T\}$), the alignment problem is to add gap characters (`-') to both sequences, so that a distance, such as Levenshtein distance, $d(s_1,s_2)$ is minimized. This problem has a well known solution, the Smith-Waterman algorithm \cite{smith1981identification}, which is a variation of the global sequence alignment solution from Needleman-Wunsch \cite{needleman1970general}, having an algorithm complexity $O(l_1 . l_2)$ where $l_1$ and $l_2$ are the length of the sequences. So, Smith-Waterman algorithm is slow since in this case one of the sequences is the entire genome.

In order to speed up sequence alignments, several heuristic approaches emerged. Most notably, BLAST \cite{altschul1990basic}, which could be for mapping sequences to a reference genome. BLAST uses an index of the genome to map parts of the query sequence, called seeds, to the reference genome. Once these seeds have been positioned against the reference, BLAST joins the seeds performing an alignment only using a small part of the reference.

\paragraph{Read mapping} Sequence alignment has an exact algorithm solution and several faster heuristic solutions. But even the fastest solutions are too slow to be used with the millions of reads generated in a typical sequencing experiment. Faster algorithms can be used if we relax our requirements in two ways: i) we allow for sub-optimal results, and ii) instead of requiring the output to be a complete local alignment between a read and the genome, we just want to know the region in the reference genome where the read sequence is from. This relaxed version of the alignment algorithm is called ``read mapping'' and the reduced complexity is enough to speed up the computations significantly. In a nutshell, a read mapping is regarded as correct if it overlaps the true reference genome region where the read originated. Once the mapping is performed, the read is locally aligned, a strategy similar to BLAST algorithm \cite{li2010fast, langmead2009ultrafast}.

Reformulating this as a \textit{mapping} problem allows us to use data structures such as suffix trees to index the reference genome. Using suffix trees we can query for a substring (read) \cite{durbin1998biological} of the indexed string in $O(m)$ time, where $m$ is the length of the query. Alternatively, we can use suffix arrays which are a space optimized alternative to suffix trees \cite{durbin1998biological}. An implicit assumption in this solution, is that the read will be very similar to the reference and that there will be no big gaps. Suffix arrays algorithms are fast but, even though they are memory optimized versions of suffix trees, memory requirements are still high ($O[ n \; log(n) ]$, where $n$ is the length of the indexed sequence -the genome-) and this becomes the limiting factor. In order to reduce memory footprint of suffix arrays, Ferragina and Manzini \cite{ferragina2000opportunistic} created a data structure based on the Burrows-Wheeler transform.  This structure, known as an FM-Index, is memory efficient yet fast enough to allow mapping high number of reads.  An FM-index for the human genome can be built in only 1Gb of memory, compared to 12Gb required for an equivalent suffix array \cite{li2010fast}.  Given a genome $G$ and a read $R$, an FM-index search can find the $N_{occ}$ exact occurrences of $R$ in $G$ in $O(|R| + N_{occ} )$ time, where $|R|$ is the length of $R$ \cite{li2010fast}. 

We should keep in mind that suffix trees, suffix arrays and FM-indexes are guaranteed to find all matching substring occurrences, nevertheless a sequencing read may not be an exact substring of the reference genome (due to sample's genome differences with the reference genome, read errors, etc.). So, even if efficient indexing and heuristic algorithms can decrease mapping time considerably, these algorithms are not guaranteed to find an optimal mapping.  Several parameters, such as read length, sequencing error profile, and genome complexity profile can affect performance.  The most commonly used implementation of the FM-index mapping algorithms are BWA \cite{li2010fast, li2010fastlong} and Bowtie \cite{langmead2009ultrafast, langmead2012fast}.  Each of them provide optimized versions for the two most common sequencing types: i) short reads with high accuracy \cite{li2010fast,langmead2009ultrafast} or ii) longer reads with lower accuracy \cite{li2010fastlong, langmead2012fast}.

\paragraph{Mapping quality\label{sec:mapq}} Sequencers not only provide sequence information, but also provide an error estimate for each base \cite{li2011statistical}.  This is often referred as a quality ($Q$) value, which is the probability of an error, measured in negative decibels $Q = -10 \; log_{10}(\epsilon)$, where $\epsilon$ is the error probability. Mapping quality is an estimation of the probability that a read is incorrectly mapped to the reference genome. Mapping algorithms provide estimates of mapping errors. In the MAQ model \cite{li2008mapping}, which is one of the earliest models for calculating mapping quality, three main sources of error are explored: i) the probability that a read does not originate from the reference genome (e.g. sample contamination); ii) the probability that the true position is missed by the algorithm (e.g. mapping error); and iii) the probability that the mapping position is not the true one (e.g. if we have several possible mapping positions). It is assumed that the total error probability can be approximated as $\epsilon \approx max(\epsilon_1,\epsilon_2, \epsilon_3)$.

\subsection{Variant calling}

Genome-wide variant calling has until recently largely been done using genotyping arrays (for SNVs) or Comparative Genomic Hybridization arrays (for CNVs). The inherent limitations of these technologies, particularly their ability to only assay genotypes at sites that are known in advance to be polymorphic, combined with the declining cost of sequencing, have now made approaches based on high-throughput resequencing the tool of choice for variant calling in clinical studies. 

Once the sequencing reads have been mapped to the reference genome, we can try to find the differences between a sequenced sample and the reference genome. This process is called ``variant calling" \cite{nielsen2011genotype}.  Several factors complicate this task, the two main ones being sequencing errors and mapping errors, described in \ref{sec:mapq}. Based on sequencing data and mapping error estimates, tools such as GATK \cite{mckenna2010genome} and SamTools/BcfTools \cite{li2008mapping} use maximum likelihood models can infer when there is a mismatch between a sample and the reference genome and whether the sample is homozygous or heterozygous for the variant. This method works best for differences of a single base (SNV), but it can also work with different degrees of success for short insertions or deletions (InDels) usually consisting of less than 10 bases. 

Aligning sequences that contain InDels (gaps) is more difficult than ungapped alignments since finding optimal gap boundary depends on the scoring method being used. This biases variant calling algorithms towards detecting false SNVs near InDels \cite{depristo2011framework}.  An approach to reduce this problem is to look for candidate InDels and perform a local realignment in those regions.  This local re-alignment process reduces significantly the number of false positive SNVs \cite{depristo2011framework}. Another approach to reduce the number of false positive SNVs calls near InDels involves the ``Base Alignment Quality" (BAQ) \cite{li2011improving}, which is the probability of misalignment for each base.  It can be shown that replacing the original base quality with the minimum between base quality and BAQ produces an improvement in SNV calling accuracy.  The BAQ can be calculated using a special type of ``Hidden Markov Model" (HMM) designed for sequence alignment \cite{li2011improving, durbin1998biological}. A more sophisticated option for reducing errors consist of performing a local genome re-assembly on each polymorphic region (e.g. HaplotypeCaller algorithm \cite{GATK}).

Finally, one should note that the error probabilities inferred by the sequencers are far from perfect.  Once the variants have been called, empirical error probabilities can be easily calculated \cite{mckenna2010genome} by comparing sequenced variants to a set of ``gold standard variants" (i.e. variants that have been extensively validated).  This allows to re-calibrate or re-estimate the error profile of the reads.  This is know as a re-calibration step, and usually improves the number of false positives calls \cite{depristo2011framework}.

Due to the nature of short reads, this family of methods does not work for structural genomic variants, such as large insertions, deletions, copy number variations, inversions, or translocations.  A different family of algorithms are used to identify structural variants generally making use of pair end reads or split reads, but their accuracy so far has been low compared to SNV calling algorithm \cite{o2013low}.

\textbf{Caveats}
	\begin{enumerate}[label=\roman*]
		\item Using current technologies and computational methods for variant calling, detection accuracy varies significantly for different variant types. SNV are by far the most accurately detected. Insertions and deletions, collectively referred as InDels, can be detected less efficiently depending on their sizes. Small InDels consisting of ten bases or less are easier to detect than large InDels consisting of 200 bases or more. The reason being that the most commonly used sequencers reads DNA in stretches roughly 200 bases long. Due to this technological limitations, detection is less reliable for more complex variant types.
	\end{enumerate}

%---
\section{Functional annotations of genomic variants \label{sec:funann}}
%---

The development of cost-effective, high-throughput next generation sequencing (NGS) technologies is poised to have a profound impact on our ability to study the effects of individual genetic variants on the pathogenesis and progression of both monogenic and common polygenic diseases. As sequencing costs decrease and throughput increases, it has now become possible to quickly identify a large number of sequence polymorphisms (SNVs, indels, structural) using samples from affected and unaffected subjects and investigate these in epidemiologic studies to identify genomic regions where mutations increase disease risk. However, translating this information into biological or clinical insights is challenging as it is often difficult to determine which specific polymorphisms are the main pathogenetic drivers of disease across a population; and more importantly, how they affect the activity of disease-related molecular pathways in tissues and organism a specific patient. In part, this difficulty results from the large number of genetic variants that are observed in individual genomes (the human population is believed to contain approximately 3.5 million polymorphic sites with minor allele frequency above 5\%) combined with the limited ability of computational approaches to distinguish variants with no impact on genome function (the vast majority) from variants affecting gene function or expression that may be associated with disease risk or drug response (the minority). The development of algorithms for automated variant annotation,which link each variant with information that may help predict its molecular and phenotypic impact, is a critical step towards prioritizing variants that may have a functional impact from those that are harmless or have irrelevant functional effects. In this section we review the key concepts and existing approaches in this important field. In Chapter \ref{ch:snpeff} we introduce an approach to collect relevant information that will help answer questions about genetic variants discovered in next-generation sequencing studies, including: (i) will a given coding variant affect the ability of a protein to carry its functions; (ii) will a given non-coding variant affect the expression or processing of a given gene; and ultimately (iii) will a given coding or non-coding variant have any impact on phenotypes of interest?

Answering these questions is essential for many types of analyses that use large-scale genomics datasets to study quantitative traits and diseases, particularly when only a small number of individuals is studied comprehensively at a genome-wide level. For example, most genome-wide association studies (GWAS) or exome sequencing studies lack the statistical power to identify rare variants or variants with small effects associated with a disease, in part due to the large number of variants assayed. This limitation can be addressed by directing both statistical analysis and subsequent experimental steps to focus on smaller sets of genetic variants that have been prioritized based on external evidence of their putative impact. The common impairment of DNA repair mechanisms and chromatin stability in malignant cells leads to a similar challenge in cancer genomics, where the hundreds or thousands of mutations that distinguish an individual's tumor and germline genomes need to be classified on the basis of their putative phenotypic effects and potential roles in carcinogenesis.

The large number of databases containing potentially helpful information about a given variant make the process of gathering and presenting relevant data challenging, despite excellent tools that already exist to analyze large genomics datasets (including GATK \cite{mckenna2010genome} and Galaxy \cite{goecks2010galaxy}) and visualize the results (such as the UCSC \cite{karolchik2014ucsc} or Ensembl \cite{flicek2012ensembl} genome browsers). Each of these databases uses its own format and is updated asynchronously, which makes it difficult for any analysis to remain up to date. In addition, the lack of comprehensive and computationally efficient models that allow integrative analyses using these resources, makes the task of comprehensive variant annotation overwhelming. By efficiently combining information from tens or hundreds of genome-wide databases, the tools described here are designed to greatly facilitate the process of variant annotation, and make it accessible to groups with limited bioinformatics expertise or resources.

%---
\subsection{Variant types}
%---

Although variant calling is a challenging task and remains an important area of research, many high-quality tools exist for calling SNVs and indels.
We discuss here the problem of annotating the variants identified by some of these tools.
The most common type of variant identified by current technologies and analysis approaches is a single base difference with respect to the reference genome (SNV) followed by multiple base differences (MNP), as well as small insertions and deletions (InDels). Here, we focus on annotating those variants (or combinations of them, called "Mixed" variants), which comprise most of the variants in a typical sequencing experiment. We do not address the annotation or large rearrangements due to the challenges involved in their identification and functional characterization and their relative rarity in the germ line.

\subsection{Types of genetic annotations}

The process of genetic variant annotation consists of the collection, integration, and presentation of experimental and computational evidence that may shed light on the impact of each variant on gene or protein activity and ultimately on disease risk or other phenotypes. Variant annotation has traditionally been divided in two apparently independent but actually interrelated tasks based on the variant's location with respect to known protein-coding genes (see Table 1 for a list of commonly used variant annotations).Coding variant annotation focuses on variants that are located within coding regions of annotated protein-coding genes and attempts to assess their impact on the function of the encoded protein. In contrast,non-coding variant annotation focuses on variants located outside the coding portion of genes (i.e. in intergenic regions, UTRs, introns, or non-protein-coding genes) and aims to assess their potential impact on transcriptional and post-transcriptional gene regulation. These two categories of variant annotations are not mutually exclusive, as variants located within exons can often have an impact on the gene transcript's processing (splicing). In addition, some transcripts can have both protein-coding and non-coding functions. Despite the intermingling of the notion of coding and non-coding variants, we will consider each type of annotation separately as assessing their impact requires different sources of data and algorithms.

The ultimate goal of variant annotation is to predict the impact of a sequence variant, although this is an ill-defined term. One the one hand, one may be interested in the molecular impact of a variant on the activity of a protein. On the other, others may be interested in a variant's impact on much higher-level phenotypes such as disease risk. Mutations that are predicted to completely abrogate a gene's activity are called loss-of-function (LOF) mutations. Those that are predicted to have less severe consequences are called moderate or low impact mutations. In practice, a variant will be predicted to cause LOF if it has two properties: (i) its molecular impact is reliably predictable by existing computational approaches (e.g. gain of stop-codon); and (ii) its functional impact, reflected by altered protein activity or expression levels, is expected to be large. Many types of variants, including most non-coding variants, may have a large functional impact but lack predictability, and as a consequence are typically not predicted to be LOF variants.

\subsection{Coding variant annotation}

Coding variants occur in translated exons. When a reliable gene annotation is available, their main impact can be classified by determining their effect on the translated amino acid sequence (if any). A synonymous coding variant (also called silent) does not change the sequence of amino acids encoded by the gene, although it may impact aspects of post-transcriptional regulation such as splicing and translation efficiency and can affect the total protein activity through changes in the amount of translated protein that is made in the cell. In contrast, a non-synonymous coding variant changes one or more amino acids encoded by the gene and can directly alter the protein's activity, localization or stability. Non-synonymous variants include missense substitutions that change a single amino acid, nonsense substitutions that lead to the gain of a stop codon, frame-preserving indels that insert or delete one or more amino acids, and frame-shifting indels that may completely alter the protein's amino acid sequence. Primary annotation and assessment of impact, determines whether a variant falls in any of these categories.

\textbf{Caveats}
	\begin{enumerate}[label=\roman*]
	
	\item \textit{Gene misannotation.} Genomic variants that have a significant effect on a protein's expression or function represent a very small fraction of all variants. Assembly and gene annotation errors or genomic oddities that break classical computational models are also rare, but lead to false positives. This implies that one is likely to find a non-negligible fraction of false-positive high-impact variants among the list of what appear to be the strongest candidates for variants with severe effects. Tools such as SnpEff can anticipate some of the most common causes of misannotation, but the number and diversity of the type of events that can lead to false-positives makes the task very challenging. As a consequence, one should always manually inspect the top candidates to ensure that they have been assigned to the correct genes and transcripts.
	
	\item \textit{Gene isoforms.} In higher eukaryotes, most genes have more than one transcript (or isoform), due to alternative promoters, splicing, or polyadenylation sites. For example, a human gene has an average of 8.8 annotated messenger RNA (mRNA) isoforms and some genes are believed to have over 4,000 isoforms resulting from complex splicing programs. For these genes, a variant may be coding with respect to one mRNA isoform and non-coding with respect to another. There are two frequent approaches to address this situation: (i) annotate a variant using the most severe functional effect predicted for at least one mRNA isoform; or (ii) use only a single canonical transcript per gene to perform primary annotation. 
	
	\item \textit{Variant calling for indels.} Variant annotation relies on knowing the exact genomic coordinates of the variant: this is rarely a problem for isolated SNVs; however, insertions and deletions often cannot be located unambiguously. Consider for example the variant $AA \rightarrow A$. This mutation results in the loss of a single base, but was it the first or second A that was deleted? From the standpoint of the cell, this question is irrelevant and deletion of any A will have the same effect. In contrast, from the standpoint of most variant annotation software, deleting the first A is different from deleting the second. Consider the scenario of a previously annotated transcript where the first A is part of the 5' UTR and the second is the first base of a start codon. If the missing base is assigned to the leftmost position in the motif (as is the current convention), the deletion would be annotated as a low impact 5'UTR variant. However, assigning it to the rightmost A would make it appear (incorrectly) to be a high-impact start-codon deletion. Similar issues may arise when considering conservation scores or transcription factor binding site (TFBS) predictions.
	
		\end{enumerate}

\subsection{Loss of function variants}

True LOF variants are difficult to predict computationally, but specific types of genetic changes will frequently lead to severely impaired protein activity. These include (i) stop-gains (nonsense mutations) and start-loss; (ii) indels causing frameshifts; (iii) large deletions that remove either the first exon or at least 50\% of the protein coding sequence; and (iv) loss of splice acceptor or donor sites that alter the protein-coding sequence. Variants that introduce premature in-frame stop codons (nonsense mutations and most frameshift indels) are expected to abolish protein function, unless the variant is very near the C-terminus of the coding region \cite{yamaguchi2008distribution} (effectively, downstream of the last functional domain in the protein). Such mutations may have severe consequences in affected cells, tissues or organism, as is seen for mutations that cause monogenic diseases \cite{scheper2007translation}. In addition, a new stop codon that lies upstream of the last exon will likely trigger nonsense mediated decay (NMD), a process that degrades mRNA before protein synthesis occurs \cite{nagy1998rule}. NMD predictions are not exact and many factors can affect mRNA degradation, including the variant's distance from the last exon-exon junction or poly-A tail, and the possibility that transcription may re-initiate downstream of the LOF variant \cite{brogna2009nonsense}.

A variant that leads to the loss of a stop codon, sometimes called read-through mutation, will result in an elongated protein-coding transcript that terminates at the next in-frame stop codon. While there are no general models that predict how deleterious this may be, such variants can also result in aberrant folding and degradation of the nascent proteins, leading to activation of cellular stress response pathways, in addition to their direct effects on protein activity and expression levels \cite{scheper2007translation}.

The effect of the loss of a start codon depends on the location of a replacement start codon with respect to the translation start site and reading frame of the native protein. If the new start codon maintains the reading frame, the only consequence may be the loss of a few amino acids in the protein transcript; however, in many cases, the new start codon will not be in-frame, thus producing a frame-shifted protein that is later degraded. In addition, the new start codon may lack an appropriate regulatory context (for example, if there is no Kozak sequence nearby or if it disrupts 5' UTR folding) leading to reduced expression of an N-terminally truncated protein. Consequently, losing a start codon is thought to be highly deleterious in most cases, due to the potential that it may reduce both protein production and activity.

\textbf{Caveats}
	\begin{enumerate}[label=\roman*]
	
	\item \textit{Rare amino acids.} Through a process called translational recoding, a UGA ``Stop" codon located in the appropriate mRNA context (determined by both primary mRNA sequence and secondary structure) may be translated to incorporate a selenocysteine amino acid (Sec / U) \cite{alberts1995molecular}. In humans, it is known to occur 100 codons located in mRNAs whose 3' UTR contains a Selenocysteine insertion sequence element (SECIS). Since the translation machinery goes so far to encode these special rare amino acids, the expectation is that mutations at those sites would be highly deleterious. This is supported by evidence that reduced efficiency of selenocysteine incorporation is linked to severe clinical outcomes, such as early onset myopathy  \cite{maiti2009mutation} and progressive cerebral atrophy  \cite{agamy2010mutations}.
	
	\item \textit{False-positives in LOF predictions.} Variants predicted to result in a LOF sometimes actually produce proteins that are partially functional  \cite{macarthur2012systematic}. In fact, an apparently healthy individual is typically heterozygous for around 100 predicted LOF variants, and homozygous for roughly 10, but many of those are unlikely to completely abolish the protein function. Indeed, these variants are enriched toward the 3' end of the gene, where they are likely to be less deleterious. 
	
	\end{enumerate}

\subsection{Variants with low or moderate impact}

Compared to the high impact variants discussed above, where extensive prior biological evidence strongly suggests that a specific type of variant will severely impair protein activity, there are few guidelines that can reliably predict how the majority of nonsynonymous (missense) variants will alter protein function or expression. As a result, the primary annotation performed by SnpEff and most related software packages will broadly categorize missense substitutions and their accompanying amino acid changes (e.g. $K154 \rightarrow L154$) as moderate impact variants. Short indels whose length is a multiple of three are treated similarly, unless they introduce a stop codon, as their effect will usually be localized.

Once missense and frame-preserving indel variants are identified, a more detailed estimation of their impact on protein function can be performed using heuristic and statistical models. The most common approaches are based on conservation, either amongst orthologous or homologous proteins, or protein domains, sometimes adding information of the physio-chemical properties of the reference and variant amino acids (e.g. differences in side chain charge, hydrophobicity, or size). The SIFT algorithm \cite{kumar2009predicting} assesses the degree of selection against specific amino acid changes at a given position of a protein sequence by analyzing the substitution process at that site throughout a collection of predicted homologous proteins identified by PSI-BLAST \cite{altschul1997gapped}. Based on this multiple sequence alignment and the highly conserved regions it contains, SIFT calculates a normalized probability of amino acid replacement (called the SIFT score), which estimates the mutation's effect on protein function. Polyphen \cite{adzhubei2010method}, another commonly used tool, takes the process one step further by searching UniProtKB/Swiss-Prot \cite{uniprot2013update} and the DSSP database of secondary structure assignments \cite{joosten2011series} to determine if the variant is located in a known active site in the protein. In contrast to other methods that categorize each variant individually, VAAST \cite{rope2011using}, a commercially available package, computes scores for groups of variants located within a given gene and ``collapses" them into a single category, a concept similar to burden testing performed for rare variants identified in exome sequencing studies. For human proteins, SnpEff makes use of the Database for Nonsynonymous SNVs' Functional Predictions \cite{liu2011dbnsfp} (dbNSFP), which collects scores produced by several impact assessment algorithms in a single database. Individually, impact assessment methods usually have an estimated accuracy of 60\% to 80\% when compared to manually curated databases of human variants, but predictions from several algorithms can be combined to provide a stringent, but more accurate estimate of impact \cite{choi2012predicting}.

In most cases these algorithms apply best to SNVs since these are common in populations and there is more genomic sequence and experimental data available to refine the statistical methods. However, some recently developed algorithms are capable of assessing variants other than SNVs, including PROVEAN \cite{choi2012predicting}, which extends SIFT to assess the functional impact of indels.

\textbf{Caveats}
	\begin{enumerate}[label=\roman*]
	
	\item \textit{Imprecise models of protein function.} Accurate impact assessment of coding variants remains an open problem and most computational predictions are riddled with both false positives and false negatives. While both missense variants and frame-preserving indels are broadly cataloged as having moderate effects, this is mostly due to lack of a comprehensive model and the extremely complex computations that would be required for an in-depth analysis (such as protein structure predictions). In these cases, proteomic information can be revealing. SnpEff adds annotations from curated proteomic databases, such as NextProt  \cite{lane2012nextprot}, which can help to elucidate if a mutation alters a critical protein amino acid or domain (such as amino acids that are post-translationally modified as part of a signaling cascade or that are form the active site of an enzyme) resulting in a protein may no longer function.
	
	\item \textit{Gain of deleterious function.} Computational variant annotation may eventually be able to fairly accurately predict the molecular impact of a variant in terms of the degree to which it translates in a loss of function for the encoded protein. However, gains of function, including the acquired ability to interact with new partners and disrupt their function, remain vastly more difficult to tackle, although several such variants have been linked to disease \cite{whitcomb1996hereditary}.
	
	\item \textit{Unanticipated effects of synonymous variants.} In most cases, synonymous variants are regarded as non-deleterious (or low impact); however, one needs to seriously consider the possibility that they may have greater functional effects by altering mRNA splicing  \cite{coulombe2009fine} or secondary structure  \cite{sabarinathan2013rnasnp}. Synonymous SNVs may also alter translation efficiency, by changing a frequently used to a rarely used codon and have been linked to changes in protein expression  \cite{sauna2011understanding}.
	
	\end{enumerate}

\subsection{Non-coding variant annotation}

Although coding variants represent less than 2\% of variants in the human genome, they make up the vast majority of confirmed disease-related variants that have been validated at a functional level. This may result from ascertainment bias (since variants in coding regions are straightforward to discover and characterize at a basic level and many studies have largely ignored non-coding variants); or may be explained by the increased complexity of computational approaches and lab assays required to predict and validate the impact of non-coding variants; or by their potentially more subtle impact on gene expression or cell function. Nonetheless, in a compendium of current GWAS studies, roughly 40\% of the variants are intergenic and 30\% intronic. Functional studies of these variants are increasingly emphasizing the importance of non-coding genetic variation at risk loci for complex genetic diseases and traits \cite{hindorff2009potential}.

Functional non-coding regions of the genome encompass a wide variety of regulatory elements contained in DNA and RNA molecules that are involved in transcriptional and post-transcriptional regulation. Cis-regulatory elements include (i) binding sites for DNA-binding proteins such as transcription factors and chromatin remodelers; (ii) binding sites for RNA-binding proteins involved in splicing, mRNA localization, or translational regulation; (iii) micro RNA (miRNA) target sites; and (iv) long non-coding RNA (lncRNA) targets on DNA, RNA and proteins. Non-coding transcripts include well-characterized regulatory RNAs (e.g. miRNA, snoRNA, snRNA, piRNA and lncRNAs) as well as RNAs involved directly in protein synthesis (e.g. tRNA and rRNA).  The annotation and impact assessment of non-coding variants presents a significant challenge for several reasons: (i) reliable technologies to study transcriptional regulatory regions on a genome-wide basis are only just reaching maturity and provide limited resolution of binding sites for individual transcription factors and regulatory RNA molecules; (ii) non-coding functional regions of most genomes remain incompletely mapped as they vary widely among different cell types and cell states (for example, in diseased and healthy tissues); (iii) non-coding regulatory elements often are part of complex transcriptional programs that are time-dependent, contain many redundant linkages or reciprocal connections between genes and respond to a wide range of intra- and extracellular signals; and (iv) genomic regulatory elements rarely have a strict consensus sequence (for example, compare the position weight matrices used to identify transcription factor or miRNA binding sites with the amino acid triplet code) making the effect of a mutation on gene regulatory programs difficult to predict. As a result, high-quality annotation of non-coding variants relies more heavily on experimental data than is the case for coding variants: since many of these experimental techniques did not study the effects of SNVs on gene regulatory programs, they can only be used to annotate variants and not to predict their effects on gene transcription. In the few cases where the effects of SNVs have been studied (for example, the effects of SNVs that are common in a population and located in genetic loci associated with complex diseases), experimental approaches provide highly accurate functional assessment at a cost of reduced coverage compared to computational approaches.

Large-scale projects such as ENCODE \cite{encode2012integrated} and modENCODE \cite{celniker2009unlocking} have made major steps toward mapping gene transcription and transcriptional regulatory regions in many tissues and cell types, but similar studies in diseased tissues remain at an early stage (for example, the growing collection of disease-related epigenomes from the Epigenome Roadmap \cite{bernstein2010nih}). The base-by-base resolution and number of cell states studied for different types of regulatory elements and non-coding transcripts varies widely among datasets; in part due to the lack of sensitive, comprehensive and high-resolution technologies to study the different molecular species and modes of interaction that can be altered by non-coding variants. Efficient technologies for genome-wide, high-throughput mapping of binding sites for RNA-binding proteins (PAR-CLiP \cite{ascano2012identification}), miRNAs (PAR-CLiP \cite{hafner2012genome} and CLASH \cite{helwak2013mapping}) are starting to be applied on a broad scale as are protocols to map transcription factor binding sites (TFBS) which can improve resolution to a single base (Chip-exo \cite{rhee2012chip}). However, in most cases, DNA and RNA binding sites are only imprecisely located within Chip-Seq peaks that span genomic regions hundreds of base pairs in length, with computational approaches being used to pinpoint the bases most likely mediating the interaction. In the absence of more precise localization data, de novo computational prediction of binding sites for DNA and RNA binding proteins remains insufficiently accurate to be of much use in annotating single noncoding variants.

This limitation is particularly critical for functional predictions of putative target sites for microRNAs and other regulatory RNA species. MicroRNAs are short RNA molecules that regulate gene expression post-transcriptionally by binding the messenger RNA of a gene through complementary, usually in the 3' region of the transcript, which leads to mRNA degradation or inhibits translation. Sequence variants that cause the loss or gain of a miRNA target site would lead to dysregulation of the gene, with likely deleterious effects. Although miRNAs are relatively well documented in most model organisms including human, their binding sites are only starting to be mapped experimentally, and computational predictions has very low specificity. Meaningful information regarding the possible role of a variant in disrupting a miRNA target site is starting to emerge \cite{liu2012mirsnp}, although variants that create new miRNA binding sites remain under the radar.

Even if the position of a functional element could be perfectly determined, predicting a variant's impact on chromatin conformation, promoter activity, gene expression, or transcript processing remains challenging. For transcription factors, this involves predicting whether the protein will still be able to recognize its mutated site (and with what affinity), as well as predicting the impact of these changes on gene expression levels. The latter is particularly hard to predict as a result of interactions, competition, and redundancy contained in regulatory networks of transcription factors or RNA binding proteins. As a consequence, computational prediction of the functional impact of non-coding variants remains a very active area of research and there is no broad consensus on the best methodology to use \cite{ward2012interpreting}. One significant exception is the identification of variants affecting canonical splice sites, defined as two bases on the 3' end on the intron (splice site acceptor) and 5' end of the intron (splice site donor). Variants that affect canonical splice sites are easily detected and typically lead to abnormal mRNA processing, involving exon loss or extension that leads to loss of function of the encoded protein.

\subsection{Impact assessment of non-coding variants}

Two broad classes of publicly available genome-wide datasets are commonly combined to assess the functional impact of non-coding genetic variants: (i) computational predictions of sequence conservation and sites involved in molecular interactions such as transcription factor and RBP binding, as well as miRNA-mRNA target interactions; and (ii) experimental genome-wide localization assays for DNA binding proteins, histone modifications, and chromatin accessibility.

\paragraph{Computational sources of evidence:} Interspecies sequence conservation plays a key role in scoring and prioritizing non-coding variants. This is based on the assumption is that sites or regions that have been more conserved across species than expected under a neutral model of evolution are likely to be functional; suggesting that mutations contained in them are likely to be deleterious. In the absence of strong experimental data, sequence conservation measures calculated from whole genome multiple alignments, (for example using PhastCons  \cite{siepel2005evolutionarily}, SciPhy  \cite{garber2009identifying}, PhyloP  \cite{pollard2010detection} , and GERP  \cite{davydov2010identifying}), have been developed to provide a generic indicator of function for non-coding variants. Although high conservation scores generally mean that a genomic region may be functional, the converse is not true and many experimentally proven functional noncoding regions show only modest sequence conservation (for example due to binding site turnover events). Finally, some regulatory regions (e.g. specific elements regulating immune response  \cite{raj2013common}) are under positive selection and may thus show less conservation than surrounding neutral regions. 

In human, genome-wide computational predictions of transcription factor binding sites based on matching to publicly available position weight matrices are available from variety of sources, including Ensembl \cite{flicek2012ensembl} and Jaspar  \cite{bryne2008jaspar}.  Because of the low information content of most binding affinity profiles, the specificity of the predictions is generally very low. Related approaches exist to predict splicing regulatory regions  \cite{fairbrother2002predictive} and miRNA target sites \cite{ziebarth2011polymirts}, some of which are precomputed for whole genomes and available from the UCSC or Ensembl genome browsers. Recent efforts to determine RNA-binding protein sequence affinities can also be used to identify putative binding sites for these proteins in mRNA  \cite{ray2013compendium}.

\paragraph{Experimental sources of evidence:} To investigate the potential impact of variants on transcriptional regulation, many published experimental data sets produced by large-scale projects such as ENCODE \cite{encode2012integrated}, modENCODE \cite{celniker2009unlocking} and Roadmap Epigenomics \cite{bernstein2010nih}, can be used directly by annotation packages. These include: (i) ChIP-seq or ChIP-exo experiments that identify TFBSs on a genome-wide basis; (ii) DNAseI hypersensitivity or Formaldehyde-Assisted Isolation of Regulatory Elements (FAIRE) assays that identify regions with open chromatin; and (iii) ChIP-seq studies to identify the presence of specific promoter or enhancer-associated histone post-translational modifications, which can be combined to identify active, poised, and inactive enhancers and promoters \cite{ray2013compendium}. Most of these data sets are easily available through Galaxy \cite{goecks2010galaxy} (as tracks from the UCSC Genome Browser) or through SnpEff (as downloadable pre-computed datasets). In parallel with the types of studies described above, expression quantitative trait loci (eQTLs) represent an agnostic way to map putative regulatory regions. An increasing number of such loci are available through the GTEX database  \cite{lonsdale2013genotype}. Experimental data that may support assessment of the impact of variants on post-transcriptional regulation remain sparser, although databases such as doRiNa  \cite{anders2011dorina} or starBase  \cite{yang2011starbase} contain genome-wide datasets obtained by CLIP-Seq and degradome sequencing. To our knowledge, these data have yet to be used in the context of variant annotation studies.

\paragraph{Combining sources of evidence:} Despite the variety of computational and experimental sources of evidence available, impact assessment for non-coding variants remains relatively crude, due to the fact that biological models of gene regulation remain fairly simple. Nonetheless, significant steps forward have been made recently, and two web-based tools, HaploReg  \cite{ward2012haploreg} and RegulomeDb  \cite{boyle2012annotation}, perform SNV and indel impact assessment for variants from dbSNV on the basis of a broad body of computational and experimental evidence. Both use pre-computed scores for variants from dbSnp and therefore cannot be used for rare variants, but they are extremely valuable for exploration by associating the variant of interest with a variant in dbSnp via linkage disequilibrium. 

\textbf{Caveats}
	\begin{enumerate}[label=\roman*]
	
	\item \textit{Sparseness of functional sites within ChIP-seq peaks.} Even if a noncoding variant is located in a region that contains a ChIP-seq peak for a given TF and has all the hallmark signatures of regulatory chromatin, the likelihood that it is deleterious remains low, because most DNA bases contained within a peak are non-functional. 
	
	\item \textit{Gain of function mutations.} While this section has focused on variants causing the loss of a functional regulatory element, genetic variants may also create new or more effective transcription factor binding sites. These are substantially harder to detect as they can occur in regions that show no evidence of function in individuals possessing the reference allele, and show little conservation across species. Furthermore, computational methods to predict gain of affinity for a given TF caused by a variant have insufficient specificity to be of much use on their own. 
	
	\end{enumerate}

%---
\subsection{Clinical effect of variants}
%---

One of the most revealing types of annotation of both coding and noncoding variants reports whether the variant has previously been implicated in a phenotype or disease. Although such information is available for only a small minority of all deleterious variants, their number is growing and should be the first type of annotation one seeks out. Clinical annotations, until recently, have been scattered in a large number of specialized databases of medical conditions with a genetic basis, including the comprehensive, manually curated collection of genetic loci, variants and phenotypes in the Online Mendelian Inheritance in Man database (OMIM, www.omim.org); web pages containing detailed clinical and genetic information about uncommon disorders in the Swedish National Board of Health and Welfare Database for Rare Diseases (www.socialstyrelsen.se/rarediseases) and the peer-reviewed NIH GeneReviews collection \cite{bryne2008jaspar} (www.ncbi.nlm.nih.gov/books/NBK1116); and a curated collection of over 140,000 mutations associated with common and rare genetic disorders in the commercial Human Gene Mutation Database (HGMD, www.hgmd.org/). In most cases, these datasets do not use standardized data collection or reporting formats; are designed to primarily provide information to patients and health professionals through a web interface; and rely on heterogeneous criteria to describe disease phenotypes and clinical outcomes; pathological and other clinical laboratory data; as well as the genetic and biologic experiments that have been used to demonstrate disease mechanisms at a molecular or cellular level. These shortcomings are being addressed by initiatives that provide centralized, evidence-based, comprehensive collections of known relationships between human genetic variants and their phenotype that are suitable for computational analysis, such as the NIH effort to aggregate records from OMIM, GeneReviews and locus-specific databases in ClinVar (www.ncbi.nlm.nih.gov/clinvar). 

Another important application of variant detection and annotation is in the study of cancer genomes, which is occurring increasingly in clinical settings to support treatment decisions for advanced tumors. Annotation of variants detected in tumor sequences can be analyzed for clinical cohorts, using similar techniques as other complex traits, as well as for individual patients, using techniques to identify differences between somatic (tumor) and germline (healthy) tissues. In the latter case, one looks for cancer-associated mutations that distinguish the somatic genome of cancer cells of an individual from the germline genome in order to find the driving mutations that pinpoint the specific mechanisms underlying tumorigenesis or metastasis. Ideally, these mutations can be used to select a treatment for the patient, establish prognosis, or to identify causative mutations that have led to the cancer's progression. In such a setting, given that sequence differences between the cancer and germline genomes are of greater interest than the background genetic changes between the germline and a reference genome, variant calling is performed using specialized algorithms, such as MuTect  \cite{cibulskis2013sensitive} and SomaticSniper  \cite{larson2012somaticsniper}.

\textbf{Caveats}
	\begin{enumerate}[label=\roman*]

	\item \textit{Annotation accuracy.} Biological knowledge, as well as molecular and phenotypic evidence supports the identification of certain groups of high impact variants based on simple criteria (such as premature stops, frameshifts, start lost and rare amino acid mutations); however, it is often hard to predict whether non-synonymous variants will have equally large effects on an organism's health. Even when the accepted ``rules of thumb" used in the primary annotation indicate that protein function is impaired, we should consider that these predictions may be based on a small number of model genes and will require appropriate wet-lab validation or confirmatory studies in cohorts. In addition, as more human genomes are sequenced, it is likely that some genetic variants that have been linked to Mendelian diseases will be found in healthy individuals  \cite{riggs2013towards}; and in many cases, may not actually be disease-causing mutations  \cite{bell2011carrier}.
	
	\end{enumerate}

\subsection{Data structures and computational efficiency}

Most computational pipelines for genomic variant annotation and primary impact assessment are relatively efficient and can annotate variants obtained from large resequencing projects involving thousands of samples within a few minutes or hours even using a moderately powered laptop. This is typically achieved through two key optimizations: (i) creation of reference annotation databases and (ii) implementation of efficient search algorithms. Reference database creation refers to the process of creating and storing precomputed genomic data from the reference genome, which can be searched quickly to extract information relevant to each variant. This process needs to be performed only once per reference genome and most annotation tools have pre-computed databases for many organisms available for users to download.

Since these databases are typically quite large, efficient search algorithms are used together with appropriate data structures to optimize the search process. In ANNOVAR \cite{wang2010annovar}, each chromosome is subdivided in a set of intervals of size $k$ and genomic features for a given chromosome are stored in a hash table of size $L/k$, where $L$ is the length of the chromosome. Another approach, used by SnpEff, is to use an ``interval forest", which is a hash of interval trees  \cite{cormen2001introduction} indexed by chromosome. Querying an interval tree requires $O[log(n) + m]$ time, where $n$ is the number of features in the tree and m is the number of features in the result. 

\subsection{Conclusions}

In Chapter \ref{ch:snpeff} we introduce SnpEff \& SnpSift, two approaches we designed for efficiently performing functional annotations of sequencing variants. These packages allow annotating, prioritizing, filtering and manipulating variant annotations as well as combining several public or custom-created databases. It should be noted SnpEff was one of the first annotation packages and has become one of the most widely used annotation software in both research and clinical environments. 

%---
\section{Genome wide association studies}
%---

A genome wide association study aims at identifying genetic variants associated to a particular phenotype. First, the genomes (or exome, depending on the study design) of affected individuals (cases) and healthy individuals (controls) need to be sequenced, variants called, annotated and filtered. Then, the goal is to find variants that exhibit some statistical association with the trait or phenotype of interest, which could be a disease status (e.g. diabetic vs healthy), a biomedical measurement (e.g. cholesterol level), or any measurable characteristic (e.g. height). Since the genome is so large, patterns of mutations that suggest correlation may be encountered by chance, so we need to establish statistical significance in order to distinguish true association from spurious ones. Like most studies, we will focus our discussion on SNVs, but most methods can be extended to other genomic variants.

\subsection{Single variant tests and models \label{sec:single}}

Consider a simple situation where there is only one variant in the whole genome for the cohort we are analysing. Since each individual has two sets of chromosomes, the variant can be present in one, both, or neither chromosomes, so the number of times a non-reference allele is present in an individual, is $ N_{nr} = \{0, 1,2\}$.

When the trait of interest is binary (e.g healthy vs disease), a cohort can be divided into cases and controls and we can build a 3 by 2 contingency table:

\[
\begin{array}{l|c|c|c|}
	& Homozygous Reference & Heterozygous & Homozygous non-reference\\
	& (N_{variant} = 0) & (N_{nr} = 1) & (N_{nr} = 2) \\
    \hline 
    Cases & N_{ca,ref} & N_{ca,het} & N_{ca,hom} \\ 
    \hline 
    Controls & N_{co,ref} & N_{co,het} & N_{co,hom} \\
    \hline 
\end{array} 
\]

Further assumptions about how many variants are required to increase disease risk can reduce this $3 \times 2$ table to a $2 \times 2$ table. In the ``dominant model'', the effect of a mutated gene dominates over the healthy one, so one variant is enough to increase risk. The opposite, called ``recessive model", is when both chromosomes have to be mutated in order to increase risk \cite{balding2006tutorial, clarke2011basic}. In these models, we can count how many cases and controls have at least one variant (dominant model) or two variants (recessive model). This simplifies the previous table, yielding a $2 \times 2$ contingency table, than can be tested using either a $\chi^2$ test or a Fisher exact test \cite{balding2006tutorial}.

Two other commonly used models, are the ``multiplicative" and the ``additive" models \cite{balding2006tutorial,clarke2011basic}. In these models, a disease risk is assumed to be multiplied (or increased) by a factor $\gamma$ with every variant present. We cannot simplify the contingency table, so we assess significance using a Cochran-Armitrage test \cite{clarke2011basic}.

\subsection{Multiple variant tests}

In a real case scenario there are thousands or millions of variants. We can extend the concept shown in the previous section by performing individual tests for each variant present in the cohort. Multiple testing can be addressed either by performing a correction, such as False Discovery Rate \cite{balding2006tutorial, clarke2011basic}, or using a stricter genome wide significance level. There are $3 \times 10^9$ bases in the genome, but taking into account the correlation between nearby variants (linkage disequilibrium), the genome wide significance level is generally accepted to be $p_{value} \leq 10^{-8}$.

In order to check if the null hypothesis of a significance tests is adequate, a QQ-plot is used (i.e. plotting the $y = -log(p_{value})$ vs $x = -log[ rank(p_{value}) / (N+1) ]$, where $N$ is the total number of variants). Adherence of the p-values to a 45 degree line on most of the range implies few systematic sources of association \cite{balding2006tutorial, clarke2011basic}. If the p-values have a higher slope than the $y=x$ line, there might be ``inflation", possibly due to co-factors, such as population structure (see section \ref{sec:popStruct}). If the inflation is not too high (e.g. less than $5\%$), this bias can be corrected by shifting the p-values towards the 45 degree slope. More sophisticated methods are explained in section \ref{sec:popStruct}.

\subsection{Continuous traits and correcting for co-factors \label{sec:cofactors}}

Methods described so far are suitable for binary ``traits" or ``phenotypes". Statistical methods that link genetic information to traits can also be used on continuous or ``quantitative" traits (e.g. weight, height, cholesterol level, etc.). A linear regression can be used assuming the traits are approximately normally distributed \cite{balding2006tutorial, clarke2011basic}. A significance test ($p_{value}$) for linear models can be calculated using an $F$ statistic, but more sophisticated methods are also available \cite{balding2006tutorial, clarke2011basic}.

Using linear models, it is easy to include known co-factors to correct for biases or inflation. For instance, if it is known that a risk increases with age or that males are more susceptible than females, age and sex can be added to the linear equation in order to correct for these effects \cite{balding2006tutorial,clarke2011basic}. In a similar manner, we can add co-factors to binary traits using logistic regression.

\subsection{Population structure \label{sec:popStruct}}

It is widely accepted that humans started in Africa and migrated to Europe, then to Asia and later to America \cite{hartl1997principles}. Out of an initial population, a few individuals migrate and colonize a new territory. This implies that the genetic variety of the new colony is significantly reduced, compared to the previous population, since the genetic pool is only a small ``founder population". The ``Out of Africa" hypothesis implies that each new migration produced a reduction in genetic variety, also known as a ``population bottleneck'' \cite{hartl1997principles}.

As we previously mentioned, each individual inherits two chromosome sets, a maternal and a paternal one. Through recombination a chromosome is formed by a crossover combining maternal and paternal chromosomes and then passed down, thus the offspring has two sets of chromosomes (one from each parent) that on average have half chromosome from each grandparent. This breaking and shuffling of chromosomes every generation, increases genetic diversity. Nevertheless if variants are located nearby in the chromosome, the chances that they are broken apart by recombination event are smaller than if they are further away from each other. This produces a correlation of close variants or ``linkage disequilibrium" (LD). Nearby highly correlated variants are said to be in the same ``LD-block" \cite{hartl1997principles}. If a population has low genetic variety, the LD-blocks are large. So African population has more variety (smaller LD-blocks) and conversely, European, Asian and Amerindian populations have less variety (larger LD-blocks) \cite{hartl1997principles}.

\subsection{Population as confounding variable }

Imagine that we have a cohort of individuals drawn from two populations ($P_A$ and $P_B$) and that individuals in $P_A$ have much higher risk of diabetes than individuals from $P_B$. Now imagine that individuals from $P_A$ have a variant $v_A$ more often, but $v_A$ is actually neutral and has no health effects whatsoever. If we do not take into account population factors, our study would conclude that $v_A$ is the cause of diabetes, just because we see $v_A$ more often in affected individuals. In this case it is clear that population structure is a confounding variable. We could avoid this problem by analyzing each population separately \cite{patterson2006population}, but this would cause a loss of statistical power since we have fewer samples.

A population that is a mixture of two or more population is known as an ``admixed population''. For instance the ``African-American'' population is a mixture of, roughly, $80\%$ African and $20\%$ European genomes \cite{hartl1997principles,balding2006tutorial}. This means that analyzing a cohort of African-American individuals, we would get population structure as a confounding variable because of population admixture \cite{hartl1997principles}. Obviously, in this case we cannot analyze each population separately, because each individual in the sample is a mixture of two populations.

The admixed population problem can be studied by performing a correction using the eigen-structure of the sample covariance matrix \cite{patterson2006population}. Samples can be arranged as a matrix $C$ where each row is a sample and each column represents a position in the genome where there is a variant. The numbers $C_{i,j}$ in the matrix indicate the number of non-reference alleles in a sample (row) at a genomic position (column $j$). Since the allele can be present in zero, one, or two chromosomes in each individual, the possible values for $C_{i,j}$ are $\{0, 1, 2\}$. The covariance matrix is calculated as $M= \hat{C}^T . \hat{C}$, where $\hat{C}$ is the matrix $C$ corrected to have zero mean columns. Usually, the first two to ten principal components of $M$ are used as factors in linear models (see section \ref{sec:cofactors}) to correct for population structure \cite{patterson2006population}.

Whether a cohort has any population structure and needs correction or not can be tested using two methods: a) plotting the projections of the first two principal components and empirically observing the number of clusters in the chart, or b) using a statistic of the eigenvalues of $M$ based on Tracy-Widom's distribution \cite{patterson2006population}.

\subsection{Common and Rare variants\label{sec:comonrare}}

The ``allele frequency" (AF) is defined as the frequency a variant appears in a population. Variants are usually categorized according to AF into three groups: i) Common variants ($AF \geq 5\%$), ``low frequency" ($1\% < AF < 5\%$), and iii) ``rare variants" ($AF < 1\%$). Common variants originated earlier in the population while rare variants are either relatively recent or selected against.

There are three main models for disease susceptibility  \cite{hartl1997principles, gibson2012rare}:i) the Common-Disease-Common-Variant hypothesis (CDCV) assumes that if disease is common, it must be caused by a common variant; ii) the ``infinitesimal hypothesis" proposes that there are many common variants each having small risk effects; and iii) the Common-Disease-Rare-Variant hypothesis proposes that there exists many rare variants, each one having large risk effects.

\subsection{Rare variants test}

The ``rare variant model'' assumes that multiple rare variants have large effects on a trait. The problem is that, since these variants are rare, huge sample sizes are required for tests to identify statistically significant associations. To overcome this problem, methods known as ``burden tests" collapse groups of rare variants that are hypothesised to have  similar effect and perform statistical significance tests on the group \cite{li2008methods}. An example of collapsing technique is to count the number or rare variant in a given window and apply a Fisher exact test, as shown in section \ref{sec:single}. A limitation of some burden tests is that they implicitly assume that all rare variants have the same direction of effect, although in reality they might have no effect, be deleterious, or protective \cite{li2008methods,wu2011rare}.

Several techniques allow weighting rare variants by collapsing them using a kernel matrix. This allows to incorporate other information, such as allele frequency and functional annotations. It can be shown that the statistic induced by kernel weighting functions follows a mixture of $\chi^2$ distributions and there is an efficient way to approximate it \cite{li2008methods,wu2011rare}, avoiding computationally expensive permutations tests.

%---
\section{Epistasis \label{sec:epi}}
%---

In this section we introduced the basic concepts and methodologies used in GWAS. Although fairly mature, there is still heavy research and continuous improvement on GWAS statistical methods. Not only it is well known that traditional (i.e. single marker) GWAS methods fail under non-additive models \cite{culverhouse2002perspective}, but also variants so far discovered using these methods do not account for all the expected phenotypic variance attributed to genetic causes (i.e. missing heritability). As other authors pointed out \cite{cordell2009detecting, zuk2012mystery, zuk2014searching}, this might be because we need to look for epistatic variants which are not taken into account using these methods. In the next section, and in Chapter \ref{ch:gwas}, we cover the topic of epistatic GWAS analysis.

\subsection{What is epistasis and why it is important}


%------------------------------------------------------------
% From: Epistasis — the essential role of gene interactions in the structure and evolution of genetic systems (2008)
%

%------------------------------------------------------------
% From: \cite{zuk2012mystery}
% TOPIC: MISSING HERITABILITY
%
%		- missing heritability: overestimation of the denominator happens when epistasis is ignored (phantom)
%		- phantom heritability could be 62.8\% in Cohn's disease, thus accounting for 80\% of the current missing heritability
%		- Until recently "The pre- vailing view among human geneticists appears to be that inter- actions play at most a minor  part in explaining missing heritability."	
%		- But "[they] show that simple and plausible models can give rise to substantial phantom heritability."	
%		- Epistasis is common [SEE MARKED PARAGRAPHS IN LANDER'S PAPER, ADD ORIGINAL REFERENCES]
%		- From a biological standpoint, there is no a priori reason to expect that traits should be additive. Biology is filled with non- linearity: The saturation of enzymes with substrate concentration and receptors with ligand concentration yields sigmoid response curves; cooperative binding of proteins gives rise to sharp tran- sitions; the outputs of pathways are constrained by rate-limiting inputs; and genetic networks exhibit bistable states.
%		- Genetic studies in model organisms have long identified specific instances of interacting genes (17). Important examples include synthetic traits (e.g., 18), which occur only when multiple loci or pathways are all disrupted.
%		- Studies have begun to reveal that epistasis is pervasive. 
%			- In the yeast Saccharomyces cerevisiae, Brem et al. (19) analyzed as quantitative traits the levels of gene transcripts in segregants of a cross between two strains. For each transcript, they found the strongest quanti- tative trait locus (QTL) in the cross and then, conditional on the genotype at this locus, identified the strongest remaining QTL. In 67% of cases, these two QTLs demonstrated epistatic interactions. In bacteria, Khan et al. (20) and Chou et al. (21) have recently demonstrated clear epistasis among collections of five mutations that increase growth rate. 
%			- In mouse and rat, Shao et al. (22) ana- lyzed a panel of chromosome substitution strains, with each strain carrying a different chromosome from a donor strain on a common recipient genetic background. For dozens of quantitative traits, the sum of the effect attributable to the individual donor chromosomes far exceeds (median eightfold) the total effect of the donor ge- nome, indicating strong epistasis. 
%			- Although genetic interactions are hard to detect in humans (see below), several cases involving var- iants with large marginal effects have been recently reported in Hirschsprung’s disease, ankylosing spondylitis, psoriasis, and type I diabetes 
%		- ...geneticists have tested for pairwise epis- tasis between loci, but have found few significant signals.
%		- ...The reason is that individual interaction effects are expected to be much smaller than linear effects, and the sample size re- quired to detect an effect scales inversely with the square of the effect size. If n loci had equivalent effects, the sample size to detect the n loci would thus scale with n^2, whereas the sample size to detect their n^2 interactions scales with n^4.
%		- Suppose that we consider two var- iants with frequency 20% that contribute to different pathways and increase risk by 1.3-fold (which is a large effect relative to those typically seen in GWAS). The sample size required to detect the variants is ∼4,900 (with 50% power and genome-wide significance level of α = 5 × 10−8 in a genome-wide association study with an equal number of cases and controls), whereas the sample size re- quired to detect their pairwise interaction is roughly 450,000 (at 50% power and an appropriate significance level to account for multiple hypothesis testing). A researcher who studied 100,000 samples would likely discover all of the loci but would find little evidence of epistatic interactions.
%		- In short, the failure to detect epistasis does not rule out the presence of genetic interactions sufficient to cause substantial phantom heritability
%		-...although the pervasiveness of epistasis in experimental organ- isms suggests that the true heritability h2 of traits may be much lower than current estimates
%		- Despite considerable efforts, few well-replicated instances of epistasis in common human disease and trait genetics have been discovered thus far.
%		- The only examples to date involve interactions featuring at least one locus with a large marginal e↵ect, such as HLA. 
%		- GWAS, in ankylosing spondylitis21 and psoriasis,22 discovered interactions between two di↵erent HLA alleles and ERAP1. (In ankylosing spondylitis, the HLA-B27 allele has an odds ratio of 40.8, and in psoriasis the HLA-C allele has an odds ratio of 4.66.) HLA also plays a role in an interaction e↵ect described in a GWAS of Type 1 diabetes. (In Type 1 diabetes, HLA has a main e↵ect of 5.5, but acts non-additively with the risk of all other alleles considered cumulatively.23). Finally, interaction between RET and EDNRB in Hirschsprung’s disease was discovered in a genome-wide linkage study,24 in which RET was strongly associated with disease (log-odds score of 5.6).

%------------------------------------------------------------
% From: "A Perspective on Epistasis: Limits of Models Displaying No Main Effect"
% TOPIC: HISTORICAL PERSPECTIVE (PRE-SEQUENCING / GENOTIPYNG) 
% IMPORTANT: Context was year 2002!!!
%
%		- for the abandonment of linkage studies in favor of genome scans for association. However, there exists a large class of genetic models for which this approach will fail: purely epistatic models with no additive or dominance variation at any of the susceptibility loci.
%		- Is it reasonable to suppose that an approach that must succeed in identi- fying fully penetrant Mendelian genes will also succeed for complex diseases? 
%		- The complex relationship between genotype and phe- notype, however, may ultimately prove to be inade- quately described by simply summing the modest effects from several contributing loci
%		- EXAMPLES OF EPISTASIS
%			- Indeed, it has been argued that epistatic interactions are a nearly universal component of the architecture of most com- mon traits. Templeton (2000), for instance, has listed a number of phenotypes in which epistasis plays a large role. 
%			- An example in insects is the abnormal-abdomen phenotype in Drosophila mercatorum (DeSalle and Templeton 1986; Hollocher et al. 1992; Hollocher and Templeton 1994).
%			- In humans, variation in triglyceride levels can be explained, in part, by two sets of inter- actions: between ApoB and ApoE in females and be- tween the ApoAI/CIII/AIV complex and low-density li- poprotein receptor in males (Nelson et al. 2001)
%			- Even the seemingly “simple” Mendelian trait of sickle-cell anemia is revealed to be greatly modified by epistatic interactions. Individuals with sickle-cell anemia who are homozygous for two polymorphisms near the Gg locus (leading to the persistence of fetal hemoglobin) have only mild clinical symptoms
%		- The main reason that most studies of complex human phenotypes fail to find evidence for epistatic interactions may simply be that commonly used designs and analytic methods inherently minimize or exclude the possibility of epistasis (Frankel and Schork 1996)
%		- The complex relationship between genotype and phe- notype, however, may ultimately prove to be inade- quately described by simply summing the modest effects from several contributing loci.
%		- HERITABILITY: 
%			- Thus, for fixed K, p , and p , maximizing the broad AB heritability (h 2 p V /V ) under the constraint repre- IT sented by formula (2) is equivalent to the maximizing of VI.
%			- TABLE 2 and 3: Maxima of heritability using epistasis.
%			-  Three-locus models can also give rise to higher relative risks than are possible in corresponding two-locus models. Three-locus penetrance models maximizing heritability at the low end of disease prevalence
%		- NUMBER OF TESTS:
%			- We note that the number of tests necessary to evaluate all two-, three-, and four-way interactions, for 30–60 candidate loci, has a range similar to the number of tests suggested for a single genomewide association scan using SNPs (Collins et al. 1999; Kruglyak 1999)
%			- Thus, al- though searching for two-, three-, four-, or n-way in- teractions among all the markers in a genome scan would not be practicable, a candidate-locus approach based on a genome scan for linkage may be.
%		- ANALYSIS METHODS:
%			- Cases only.—The most straightforward multilocus analysis of cases-only data is a x2 test of independent segregation for the loci. 
%			- Case-control.—A second approach is a multilocus case-control analysis. One method for doing this would be to compare the distribution of cases among the 3L genotypes, where L is the number of biallelic loci being simultaneously examined, versus the distribution of controls. In this analysis, a sample of N cases and N unrelated controls drawn from a population modeled by table 3 will, again, yield an expected x2 statistic 􏰃2N. However, the degrees of freedom under the null hypothesis are now 8.
%		- POWER: We have seen that, if the true genetic model underlying a disease is purely epistatic, with no additive or domi- nance variation at any of the susceptibility loci, then association methods analyzing one locus at a time will have no power to detect the loci. 
%		- First, we expect that, with a sufficient number of contributing loci, purely epistatic interac- tions could account for virtually all the variation in affection status for diseases with any prevalence
%		- Of course, there are subclasses of purely epistatic models (providing no marginal evidence for the involve- ment of any single locus) for which, in addition, no two, three, or L􏰂1 loci jointly give evidence of in- volvement in the disorder. This leads to the concern that even assessment of all two-, three-, and (L􏰂1)-way interactions among candidate loci may be insufficient for detection of the contributing loci.
%		- The restriction on maximum heritabilities in these models is most easily seen by examining L-locus models for which no collection of L 􏰂 1 loci shows mar- ginal deviations. 
%		- MISSING HERITABILITY: Researchers of many complex diseases (including non–insulin-dependent diabetes mellitus, prostate can- cer, and schizophrenia) face the conundrum of moder- ately heritable diseases for which locus-by-locus anal- yses have not accounted for the predicted genetic variance. The models discussed in the present article provide one possible explanation for this.
%		- These considerations lead us to believe that, in situ- ations in which heritability is moderate to high but in which locus-by-locus analyses do not account for the predicted genetic variance, it is worth pursuing a hy- pothesis of interacting loci [near the linkage peaks]


%------------------------------------------------------------
% From : Defining genetic interaction (2007)
%
%		- Sometimes mutations in two genes produce a phenotype that is surprising in light of each mutation’s individual effects. This phenom- enon, which defines genetic interaction, can reveal functional rela- tionships between genes and pathways.
%		- Recent studies have used four mathematically distinct definitions of genetic interaction (here termed Product, Additive, Log, and Min). Whether this choice holds practical consequences has not been clear, because the definitions yield identical results under some condition
%		- Here, we show that the choice among alternative definitions can have profound consequences.
%		- The study of genetic inter- action has become increasingly systematic and large-scale, espe- cially in the yeast Saccharomyces cerevisiae (6, 8–21).
%		- A quantitative genetic interaction definition has two compo- nents: a quantitative phenotypic measure and a neutrality function that predicts the phenotype of an organism carrying two noninter- acting mutations. Interaction is then defined by deviation of a double-mutant organism’s phenotype from the expected neutral phenotype
%		- A double mutant with a more extreme phenotype than expected defines a synergistic (or synthetic) interaction between the corresponding mutations (synthetic lethality, in the extreme case).
%		- Alleviating or “diminishing returns” interactions, in which the double-mutant phenotype is less severe than expected, often result when gene products operate in concert or in series within the same pathway. Alleviating interactions arise, for example, when a muta- tion in one gene impairs the function of a whole pathway, thereby masking the consequence of mutations in additional members of that pathway.
%		- One class of phenotype, fitness, has been central to many large-scale genetic interaction studies. Although fitness was origi- nally measured in terms of population allele frequencies (1, 22, 23), it can also be measured by using growth rates of isogenic microbial cultures.
%		- Genetic interaction studies have used different measures of fitness, including: (i) the exponential growth rate of the mutant strain relative to that of wild type (4, 9, 15, 19) (the relative-growth- rate measure); (ii) the increase in mutant population relative to wild type in one wild-type generation (the relative-population measure) (6); and (iii) the number of progeny per mutant organism relative to the number of progeny for wild type in one wild-type generation (the relative-progeny measure) (24)
%		- Genetic interaction studies have also differed in their choice of neutrality functions, generally using either a multiplicative or a minimum mathematical function.
%		- The multiplicative function, which was originally applied to fitness measures defined in terms of allele frequencies, predicts double-mutant fitness to be the product of the corresponding single-mutant fitness values. The multiplica- tive function can be combined with each of the three fitness measures above to yield three distinct definitions of genetic inter- action (4, 6, 15, 19, 24).
%		- A fourth (Min) definition of genetic interaction results from the minimum neutrality function, under which noninteracting muta- tions are expected to yield the fitness of the less-fit single mutant. Each fitness measure above yields an identical set of genetic interactions under this function. A hypothetical example illustrates one rationale for the Min definition: Two single mutations each disrupt a distinct cellular pathway that limits cell growth, such that one of these mutations is substantially more limiting than the other. The double mutant might then be expected to exhibit the phenotype of the most-limiting single mutant. 
%		- It has not been clear whether the choice of genetic interaction definition has any practical consequences. To evaluate the impact of definition choice, we applied each of the four definitions in turn to two reference studies.
%		- Here, we show that the choice of definition can dramatically alter the resulting set of genetic interactions and the extent to which they correspond to shared gene function. 
%		- For a gene pair (x, y), we refer to the fitness of the two single mutants and the double mutant, respectively, as Wx, Wy, and Wxy.
%		- The neutrality function E(Wxy), predicting double-mutant fitness for a strain with mutations in noninteracting genes x and y, is defined differently under the Min, Product, Log, and Additive 
%
%		- DATASET: To evaluate the impact of definition choice, we applied each of the four definitions in turn to two reference studies, St. Onge et al. (19) (Study S) and Jasnos and Korona (6) (Study J), both providing quantitative growth-rate measurements of isogenic wild-type and single- and double-mutant cell populations.
%
%		- RESULTS: The Choice of Genetic Interaction Definition Matters:
%			- Additive and Log Definitions Demonstrate Different Biases: However, we had observed that interaction strength had a significant positive bias (under all defi- nitions) for pairs involving mutations with extreme fitness effects.
%			- Product and Log Definitions Are Equivalent for Deleterious Mutations: 
%			- The Product Definition Reveals Functional Relationships Missed by the Min Definition.
%			- Genetic Interaction Networks from Min and Product Definitions Differ Greatly.
%
%		- WHICH DEFINITION TO USE?: We examined the distribution of 􏰍, the deviation of the expected double-mutant phenotype from the observed double mutant phenotype, and found the Product and Log definitions to be closest to this ideal in general. Additionally, we showed that the Log and Product definitions are practically equivalent when both single mutants are deleterious.

\subsection{Detecting Epistasis / interactions}

%------------------------------------------------------------
% From: \cite{rohlfs2010detecting}		Cited by: 16
%	Topic: DETECTING EPISTASIS (not GWAS)
%
%		- Distinct combinations of alleles in coevolving genes interact differently, conferring varying degrees of fitness. If this fitness differential is adequately large, the resulting selection for allele matching could maintain allelic association, even between physically unlinked loci.
%		- However, because the coevolving genes are not necessarily in physical linkage, this is not an appropriate measure of coevolution-induced allelic association
%		- Coevolving genes are expected to undergo compensatory mutations to maintain their interaction.
% 	- Methods have been developed for detecting coevolution by testing for high correlation of phylogenetic distance matrices between gene families, genes, or gene domains.1–6
%		- HLA and KIR are well established as interacting immune-response loci under intense diversi- fying selection. Although these genes are on different chromosomes, their allele frequencies are significantly cor- related within human populations, as one would expect under intense selection for allele matching.15
%		- Most cases of selec- tive advantage for specific allele pairing would be resolved with fixation of the optimal allele pair.7
%		- In this paper, we explore the ramifications of coevolu- tion between the genes mediating sperm-ZP binding in humans. Specifically, the ZP-located protein ZP3 (MIM 182889) has been shown to mediate sperm binding to the ZP19 
%		- Because ZP3 and ZP3R are putative interactors mediating gamete recognition, are polymorphic among humans, and are located on different chromosomes, they are excel- lent candidates for coevolution-induced allelic association
%		- WTCCC, Affymetrix 500K SNP genotyping platform for 1504 individuals in the 1958 Birth.
%		- General allelic association between a pair of SNPs is quantified by CLD. An estimate of CLD has been previously given35..
%		- To address this possibility, we use a standard con- tingency table for independence between the two genotypes (Table 1), resulting in the chi-square distributed test statistic with four degrees of freedom:
%		- The CLD and GA test statistics measure allelic association, but they are also dependent on marginal one-locus genotype counts
%		- To control for the one-locus genotype counts, X12 and X42 are used as test statistics in permutation tests. 
%		- Permutation p values approximate exact p values, which are the probabilities of an allelic association at least as strong as that observed, given the marginal genotypes at each locus. 
%		- Calculating the power of these exact tests can be prohibitively slow with a large sample size. As an alternative, we quickly esti- mate power by using theoretical test statistic distributions under the alternative hypothesis. Under the alternative hypothesis with genotype frequency matrix F, X 2 is approximately chi-square 1 distributed with one degree of freedom and noncentrality param- eter
%		- We ran a similar analysis on a secondary candidate gene pair implicated in maternal-fetal interactions: GHR (MIM 600946) and GH2 (MIM 139240).37
%		- Power: because of computational limitations, we were unable to perform the exact test for larger value of n; 
%		- Asymptotic Analysis: For a high but biologically reasonable s of 0.1,38 with a sample size of n 1⁄4 1480, the asymptotic CLD test has a power of 0.525 and the asymptotic GA test has a power of 0.327
%		- Population: Population structure could also cause allelic association between physically unlinked loci. 
%		- Allelic association would be observed if the alleles at each locus have different frequencies in different populations and those populations are pooled together. In this analysis, ZP3 and ZP3R are associated as compared to other genes in the same individuals. It is not likely that population structure would cause allelic association in our candidate gene pair but not in other gene pairs in the same population. It is possible that ZP3 and ZP3R are statistical outliers that we expect under no selection and are associated simply by chance. However, given our limited single-hypothesis candi- date gene approach, we find that unlikely.
%		- The field has yet to identify a gene pair that is certainly coevolving in which both genes are polymorphic. In the absence of a clear positive control,


%------------------------------------------------------------
% From \cite{petkov2005evidence}		Cited by: 91
%	Topic: DETECTING EPISTASIS (not GWAS)
%
%		- Evidence from inbred strains of mice indicates that a quarter or more of the mammalian genome consists of chromosome regions containing clusters of functionally related genes
%		- 60 genetically diverse inbred strains.
%		- forming networks with scale-free architecture. Combining LD data with pathway and genome annotation databases, we have been able to identify the biological functions underlying several domains and networks.
%		- As typified by the a and b globin gene clusters, tandem duplications can give rise to gene families whose members develop divergent, but still related, functions over time. 
%		- Gene clusters may arise as a means of promoting their coregulation through regional controls of chromatin structure and expression, and there is now considerable evidence, well summarized by Hurst et al. [1], that for variety of eukaryotes, including yeast, Caenorhabditis, Drosophila, high- er plants, and mammals, genes sharing expression patterns are more likely to be in proximity than would be expected by chance. 
%		- ...And finally, Fisher [2] and later Nei [3,4] have argued on theoretical grounds that when genes interact epistatically, evolutionary selection will promote their genetic linkage as a means of enhancing the coinheritance of favorable allelic combinations. 
%		- The process of inbreeding to homozygosity imposes intense selective pressures; all efforts among some species have failed, and with mice, only a fraction of initial attempts succeeded. 
%		- Accordingly, we can expect that if clustering of functionally related genes is a common feature of mammalian genomes, there is likely to be selection for coadapted allelic combinations among the genes encoding functions that influence fitness and survival during inbreeding. This would result in regions of linkage disequilibrium (LD) among inbred strain genomes; i.e., some allelic combinations should occur more often than expected by chance.
%		- Data: 1,456 SNPs, chosen for their high information content, among a set of 60 common and wild- derived inbred mouse strains chosen for their genetic diversity. 
%		-  The identity of these strains and the phylogenetic relationships among them are indicated in Figure 1, which was constructed using neighbor-joining
%		- LD calculation: estimated LD using D9, the difference between the observed frequency of an allelic combination and its random expectation, relative to the maximum deviation possible given the allele frequencies of the two markers [14,15]. D9 corrects for differences in allele frequencies and describes LD equally well when there is selection for or against the combination of majority alleles. A cumulative Fisher’s exact test (FET) was used to compute the probability (pFET) of obtaining an equally or more extreme distribution under the null hypothesis of random allelic association between pairs of SNPs. 
%		- Permutaation test: In one set, marker locations were randomized while maintaining the assignments of alleles to strains (Figure 2, red triangles), and in the other set the assignments of alleles to strains were randomized while preserving allele ratios and marker locations (Figure 2, solid circles)
%		- It is difficult to escape the conclusion that the selective factors acting to generate LD domains and networks during inbreeding reflect clustering and/or interaction of function- ally related elements along chromosomes


%------------------------------------------------------------
% From: \cite{wang2012ldgidb}		Cited by: 1
%	Topic: DETECTING EPISTASIS (not GWAS)
%
%		- non-physical linkages between different mutations (or single nucleotide polymorphisms, SNPs) 
%		-  These interactions can be physical protein interactions, regulatory interactions, functional compensation/antagonization or many other forms of interactions. 
%		- non-physical SNP linkages, coupled with knowledge of SNP-disease associations may shed more light on the role of gene interactions in human disorders.
%		- exonic regions of protein-coding genes from the HapMap database to construct a database named the Linkage-Disequilibrium-based Gene Interaction database (LDGIdb). The database stores 646,203 potential human gene interactions, which are potential interactions inferred from SNP pairs that are subject to long-range strong linkage disequilibrium (LD), or non-physical linkages. To minimize the possibility of hitchhiking, SNP pairs inferred to be non-physically linked were required to be located in different chromosomes or in different LD blocks 
%		- Here we consider only the subpopulations that con- tain at least 20 individuals.
%		- strong LD (r2 ≥ 0.8);

%------------------------------------------------------------
% From: \cite{koch2013long}		Cited by: 3
%	Topic: DETECTING EPISTASIS (not GWAS)
%
%		- Long-range linkage disequilibria (LRLD) between sites that are widely separated on chromosomes may suggest that population admixture, epistatic selection, or other evolutionary forces are at work. 
%		- We quantified patterns of LRLD on a chromosome-wide level in the YRI population of the HapMap dataset of single nucleotide polymorphisms (SNPs).
%		- We calculated the disequilibrium between all pairs of SNPs on each chromosome (a total of .261011 values) and evaluated significance of overall disequilibrium using randomization. 
%		- The results show an excess of associations between pairs of distant sites (separated by .0.25 cM) on all of the 22 autosomes. 
%		- Disequilibria between closely-linked sites result largely from random genetic drift or (equivalently) the common ancestry of unrecombined chromo- some blocks. 
%		- While these ‘‘long range haplotypes’’ can extend over a few hundred kb in unrelated humans [5], they still span only a very small fraction of an entire chromosome.
%		- Considerably less attention has been paid to patterns of LD between pairs of sites that are separated by much greater genetic distances (say, 1 cM or more).
%		- finding substantial long range linkage disequilibrium (LRLD) suggests that counter- vailing forces are at work.
%		- 1) One possibility is population admixture [6], which has been proposed to explain unusual patterns of LRLD in some human populations
%		- 2) A second contrib- uting force is drift. Even in a population at demographic equilibrium, recombination between distant chromosome blocks will largely but not completely erase LD caused by drift. Recurrent bottlenecks are particularly effective at generating LD [9], and may have contributed importantly to disequilibria in non- African populations of humans
%		- 3) Third, epistatic selection can maintain linkage disequilibrium indefinitely [11]. Epistasis has been implicated in the LD observed between two pairs of genes in humans [12,13]. 
%		- 4) Fourth, the hitchhiking of linked sites with a positively-selected mutation can generate large haplotype blocks that result in disequilibria over the region that they span [3,4].
%		- 5) Fifth, structural variation in chromosomes, such as inversions, can alter patterns of recombination and consequently cause LD to extend over unusually large regions of a chromosome [14–16].
%		-  to our knowledge there has been only one previous survey of associations between chromosomal regions across the entire human genome using high-density data. Sved [17] studied correlations in heterozygosity between chromosome blocks. His analysis of the HapMap phase 3 data found evidence of associations between blocks at distances of up to 10 cM and weak correlations between blocks on different chromosomes, but he did not attempt to assess their statistical significance. Lawrence et al. [18] provided a web-based tool for exploring long distance linkage disequilibria in the HapMap data, but did not go on to study patterns in the data.
%		- This paper investigates patterns of LRLD in the YRI population (the Yoruba in Ibadan, Nigeria) from the HapMap Phase 2 dataset of single nucleotide polymorphisms [23]. YRI also has weaker short-range disequilibria that might otherwise obscure the patterns of LRLD
%		- We calculated the disequilibria between all pairs of SNPs on the same chromosome, then analyze these data with new statistical methods. 
%		- Using null distributions generated by randomization, we find significant excess of disequilibria on all 22 autosomes in the Yoruba population. 
%
%		- Data: 120 YRI haplotypes that were genotyped at over 2.86106 SNPs in HapMap Phase 2 (data build 22)
%
%		- LD issues: Most commonly used measures of linkage disequilibria are not well suited for that purpose [8]. For example, a large value of D9 is likely to result from sampling if allele frequencies are near 0 or 1, while even a small value is unlikely to appear by chance if allele frequencies are intermediate and the sample size is large.
%		- We therefore use the probability that a value of the disequilibrium D as large or larger than that in the sample would be observed if there is no association in the population from which the sample is drawn, conditioned on the sampled allele frequencies at the two loci. This probability, which we denote pD, is given by the tail of Fisher’s exact test [8,28,29]
%		- As the distance between a pair of sites on a chromosome grows large (specifically, the product of the recombination rate and the effective population size becomes much greater than 1), the sampling distribution for two-locus haplotypes converges on that of Fisher’s exact test [30,31]. 
%
%		- Patches: When a pair of distant sites are in disequilibrium, it is likely that other sites near to them will also be associated as a result of short- range associations [17,32]. In effect, the underlying structure in the data is disequilibrium between pairs of chromosomal blocks rather than between pairs of individuals sites
%		- To control for this we used a simple and efficient ad hoc strategy that identifies ‘‘patches’’ of disequilibria.
%
%		- Results: We take two approaches to search for nonrandom patterns of LRLD. We first ask whether observed values of pD are more extreme than expected. For this purpose we determined the most extreme (that is, smallest) value of pD in each patch, then calculated the mean of these extreme values across all patches on a chromosome. We refer to this statistic as pDmax. 
%		-	Second, we ask whether the number of LRLD patches observed for a given chromosome is greater than expected by chance. We denote this statistic as nP.
%		- To test for the statistical significance of pDmax and nP, we generate their null distributions using a randomization method
%		- There are two motivations behind this method. First, it preserves the allele frequencies at each site. Second, it maintains the structure of short range disequilibria in the sample. 
%		- Computational time: Constructing these null distributions is the most computationally intensive part of our method. For the analyses reported below, over 4.861014 values of pD were computed, and the project consumed about 34,000 hours of CPU time.
%		- All of the 22 chromosomes show significant values for pDmax at the p,0.05 level, and all remain significant after a Bonferroni correction for multiple tests. For the second test statistic, n , 19 chromosomes show significant P values, 18 of which remain significant after the Bonferroni correction. These results suggest there is long-range linkage disequilibrium in the YRI population.
%		- [LRLD] have been little studied, they may be indicators of important evolutionary processes

\subsection{Epistatic GWAS \label{sec:epigwas}}

% GWAS & Epistasis
Genome wide association studies have traditionally focused on single variants or nearby groups of variants. An often cited reason for the lack of discovery of high impact risk factors in complex disease is that these models ignore loci interactions \cite{cordell2009detecting} which have recently been pointed out as a potential solution for the ``missing heritability" problem \cite{zuk2012mystery, zuk2014searching}. With interactions being so ubiquitous in cell function, one may wonder why they have been so neglected by GWAS. There are several reasons: i) models using interactions are much more complex \cite{gao2010classification} and by definition non-linear, ii) information on which proteins interacts with which other proteins is incomplete \cite{venkatesan2009empirical}, iii) in the cases where there protein-protein interaction information is available, precise interacting sites are rarely known \cite{venkatesan2009empirical}. Taking into account the last two items, we need to explore all possible loci combinations, thus the number of $N$ order interactions grows as $O(M^N)$ where $M$ is the number of variants \cite{de2013emerging}. This requires exponentially more computational power than single loci models. This also severely reduces statistical power, which translates into requiring larger cohort, thus increasing sample collection and sequencing costs \cite{de2013emerging}.

In Chapter \ref{ch:gwas} we develop a computationally tractable model for analysing putative interaction of pairs of variants from GWAS involving large case / control cohorts of complex disease. Our model is based on analysing cross-species multiple sequence alignments using a co-evolutionary model in order to obtain informative interaction prior probabilities that can be combined to perform GWAS analysis of pairs of non-synonymous variants that may interact.

%------------------------------------------------------------
% From: Detecting gene–gene interactions that underlie human diseases (Nature, 2009)
%	Topic: GWAS EPISTASIS (METHODS REVIEW)
%
%		-  Following the identification of several disease-associated polymorphisms by genome-wide association (GWA) analysis, interest is now focusing on the detection of effects that, owing to their interaction with other genetic or environmental factors, might not be identified by using standard single-locus tests
%		- ...it is hoped that detecting interactions between loci will allow us to elucidate the biological and biochemical pathways that underpin disease. 
%		- In recent years, the field has been revolution- ized by the success of genome-wide association (GWA) studies1–5. Most of these studies have used a single-locus analysis strategy, in which each variant is tested individu- ally for association with a specific phenotype
%		- However, a reason that is often cited for the lack of success in genetic studies of complex disease6,7 is the existence of interac- tions between loci. 
%		- If a genetic factor functions primarily through a complex mechanism that involves multiple other genes and, possibly, environmental factors, the effect might be missed if the gene is examined in isola- tion without allowing for its potential interactions with these other unknown factors.
%		- The purpose of this Review is to provide a survey of the methods and related software packages that are cur- rently being used to detect the interactions between the genetic loci that contribute to human genetic disease.
%		- Interaction as departure from a linear model. The most common statistical definition of interaction relies on the concept of a linear model that describes the relationship between an outcome variable and a predictor variable or variables
%		- Arguably the most well-known form of this type of analysis is simple linear or least squares regression26, in which we relate an observed quantitative outcome y (for example, weight) to a predictor variable x (for example, height) using a ‘best fit’ line or regression
%		- From a statistical point of view, interaction repre- sents departure from a linear model that describes how two or more predictors predict a phenotypic outcome
%		- For a disease outcome and case–control data, rather than modelling a quantitative trait y, the usual approach is to model the expected log odds of disease as a linear function of the relevant predictor variables
%		- DEFINITION Penetrance: The probability of displaying a particular phenotype (for example, succumbing to a disease) given that one has a specific genotype.
%		- DEFINITION: Marginal effects: The average effects (for example, penetrances) of a single variable, averaged over the possible values taken by other variables. These could be calculated for one locus of a two-locus system as the average of the two-locus penetrances, averaged over the three possible genotypes at the other locus.
%		- For or simplicity, I have concentrated here on defining interaction in relation to two genetic factors (two-locus interactions). In practice, however, for complex diseases we might also expect three-locus, four-locus and even higher-level interactions. Mathematically, such higher- level interactions are simple extensions to the two-locus models described earlier. 
%
%		- CASE ONLY METHODS:
%			- A case-only test of interaction can therefore be performed by testing the null hypothesis that there is no correlation between alleles or genotypes at the two loci in a sample that is restricted to cases alone. This test can easily be performed using a simple χ2 test of inde- pendence between genotypes (a four degrees of freedom test) or alleles (a one degree of freedom test), or using logistic or multinomial regression in any statistical analysis package.
%			- The main problem with the case-only test is its requirement that the genotype variables are not cor- related in the general population. It is this assumption, rather than the design per se, that provides the increased power compared with case–control analysis
%			- The case- only test is therefore unsuitable for loci that are either closely linked or show correlation for another reason (for example, if certain genotype combinations are related to viability).
%		- Tests for association allowing for interaction: From a mathematical point of view, a test for association at a given locus C while allow- ing for interaction with another locus B (a joint test16) corresponds to comparing the fit to the observed data of a linear model in which the main effects of B, C and their interactions are included 
%		- Theoretically, if no interaction effects exist, these joint tests will be less powerful than marginal single- locus association tests. However, if interaction effects exist, then the power of joint tests can be higher than that of single-locus approaches52.
%
%		- CLASSIFICATION TREE: Recursive partitioning approaches are based on classification and regression trees111. Trees are constructed (see the figure) using rules that determine how wella split at a node (based on the values of a predictor variable such as a SNP) can differentiate observations with respect to the outcome variable (such as case–control status). A popular splitting rule is to use the variable that maximizes the reduction in a quantity known as the Gini impurity111,112 at each node. 
%
%		- RAMDOM FOREST: A random forest is constructed by drawing with replacement several bootstrap samples of the same size (for example, the same number of cases and controls) from the original sample. An unpruned classification tree is grown for each bootstrap sample, but with the restriction that at each node, rather than considering all possible predictor variables, only a random subset of the possible predictor variables is considered. This procedure results in a ‘forest’ of trees, each of which will have been trained on a particular bootstrap sample of observations.
%
%		- BAYESIAN MODEL SELECTION: Bayesian model selection techniques92 offer an alterna- tive approach for selecting predictor variables and the interactions between them that are the best predictors of phenotype. The key difference between Bayesian model selection and simple comparisons of nested regression models using frequentist (non-Bayesian) procedures is the specification of prior distributions for the unknown regression parameters as well as for a dimension param- eter in a Bayesian approach. This dimension parameter specifies how many non-zero predictors are included
%			- A posterior distribution for these parameters, given the observed data, can then be calculated using Markov chain Monte Carlo (MCMC)93 simulation techniques, in which one traverses the space of the possible models (sets of parameter values), sam- pling the outputs of the simulation run at intervals. Although MCMC is a flexible approach, it can require some care with respect to the choice of prior distribu- tions, proposal schemes (determining how one moves between models) and the number of iterations required to achieve convergence.
%			- BEAM: Bayesian Epistasis Association Mapping. A recently proposed MCMC approach that is specifically designed to detect interacting, as well as non-interacting, loci is Bayesian epistasis Association Mapping13, which is implemented in the software package BeAM. In BeAM, predictors in the form of genetic marker loci are divided into three groups: group 0 contains markers that are not associated with disease, group 1 contains markers that contribute to disease risk only by main effects and group 2 contains markers that interact to cause disease by a satu- rated model. Given prior distributions that describe the membership of each marker in each of the three groups and prior distributions for the values of the relevant regres- sion coefficients given group membership, a posterior distribution for all relevant parameters can be generated using MCMC simulation. In addition to making infer- ences in a fully Bayesian inferential framework, one can use the results from BeAM in a frequentist hypothesis- testing framework by calculating a ‘B-statistic’13 that tests each marker or set of markers for significant association with a disease phenotype.
%			- EBAM LIMITATIONS: BeAM cannot currently handle the 500,000–1,000,000 markers that are now routinely being genotyped in genome scans of 5,000 or more individuals.


%------------------------------------------------------------
% From: Detecting epistatic effects in association studies at a genomic level based on an ensemble approach (2011)
%	Topic: A GWAS EPISTASIS METHOD
%
%		- We extend the basic AdaBoost algorithm by incorporating an intuitive importance score based on Gini impurity to select candidate SNPs. 
%		- Permutation tests are used to control the statistical significance.
%		- We have performed extensive simulation studies using three interaction models to evaluate the efficacy of our approach at realistic GWAS sizes, and have compared it with existing epistatic detection algorithms.
%		- CURRENT METHODS: Generally speaking, existing approaches for searching gene– gene or SNP–SNP interactions can be grouped into four broad categories.
%			- 1) Methods in the first category rely on exhaustive search. Classical statistics such as the Pearson’s χ2 test or the logistic regression that are commonly used as single-locus tests for GWAS can potentially be used in searching for pairwise interactions. Marchini et al. (2005) have shown that explicitly modeling of interactions between loci for GWAS with hundreds of thousands of markers is computationally feasible. They also showed that these simple methods explicitly considering interactions can actually achieve reasonably high power with realistic sample sizes under different interaction models with some marginal effects, even after adjustments of multiple testing using the Bonferroni correction.
%			- 2) The second category consists of methods relying on stochastic search, with BEAM (Zhang and Liu, 2007) as one representative of such algorithms. Later algorithms in this category [e.g. epiMODE (Tang et al., 2009)] largely adopted and extended BEAM. BEAM uses Markov chain Monte Carlo (MCMC) sampling to infer whether each locus is a disease locus, a jointly affecting disease locus, or a background (uncorrelated) locus. The algorithm begins by assigning each locus to each group according to a prior distribution. Using the Metropolis–Hastings algorithm, it attempts to reassign the group labels to each locus. At the end, it uses a special statistic, called the B-Statistic, to infer statistical significance from the hits sampled in MCMC. This approach avoids computing all interactions, but can still theoretically find high-order interactions. The number of MCMC rounds is the primary parameter that mediates runtime, as well as power. The suggested number of MCMC rounds is in the quadratic of the number of SNPs, which limits applicability of BEAM on large datasets.
%			- 3) Methods in the third category are machine learning approaches such as tree-based methods or support vector machines (SVM). For example, a popular ensemble approach, Random Forests 
%			- 4) Methods in the forth category rely on conditional search. In such a case, analyses are performed in stages (Evans et al., 2006; Li, 2008). A small subset of promising loci is identified in the first stage, normally using single locus methods, and multi-locus methods are used in the later stage(s) to model interactions based on the selection in the first stage. Stepwise regression has been widely used in this case and several different strategies have been studied in the literature. Methods based on conditional search can greatly reduce the computational burden by a couple of orders of magnitude, but with the risk of missing markers with small marginal effect. One should also notice that the conditional search category is more like a strategy rather than an approach. In addition to single-locus- based methods, any approaches discussed previously, especially the machine learning ones, can be used to search for candidates in the first stage.
%		- THIS METHOD: We extend the basic AdaBoost algorithm by incorporating an intuitive importance score based on Gini impurity to select candidate SNP
%		- Instead of trying to create a monolithic learner or model, ensemble systems attempt to create many heterogeneous versions of simpler learners, called weak learners. The opinions of these heterogeneous experts are then combined to formulate a complete picture of the data. 
%		- Usually, a SNP is selected to ensure largest homogeneity in the child nodes. In our implementation, we use the gain on Gini Impurity. Intuitively, when child nodes have lower impurity from a split based on an attribute (i.e. a SNP here), each child node will have purer classification. Therefore, the genotype frequencies from the two classes (case and control) are expected to be more different. 
%		- Usually decision trees are built with binary splits, where individuals with one value of the feature are placed into one group, and the remainder into the other. Since genotype data is three valued, we extend this to do a ternary split.
%		- Despite only using marginal effects to select SNPs, decision trees can still detect some interaction. Because of the recursive partitioning, lower nodes are effectively conditioned on the value of their parents.
%		- The core idea of AdaBoost is to draw bootstrap samples to increase the power of a weak learner. This is done by weighting the individuals when drawing the bootstrap sample. When a weak learner instance misclassifies an individual, the weight of that individual is increased (and increased more if the weak learner instance was otherwise accurate). Thus, hard to classify individuals are more likely to be included in future bootstrap samples. In the end, the ensemble votes for class labels weighting the weak learner instances by training set accuracy. 


%------------------------------------------------------------
% From: A Review for Detecting Gene-Gene Interactions Using Machine Learning Methods in Genetic Epidemiology (2013)
%	Topic: GWAS EPISTASIS METHODS (MACHINE LEARNING)
%
%		- EPISTASIS TYPES: 
%			- Moreover, there are various types of gene-gene interactions which are synthetic- interaction, epistatic interaction, and suppressive-interaction which are shown in Figure 1. These interactions are partic- ularly important due to the effect of a gene on individual phenotype is depending on more than one additional genes
%			- For instances, synthetic-interaction between two genes is that genes A and B are on different parallel pathways that can obtain the purple phenotype C. If either of the genes is knockout, the purple phenotype C still can be viewed. However, if both of the genes are knockout, it will result in a nonpurple phenotype. 
%			- Next, the example of epistatic-interaction that is the wild type holds a mixed purple and green phenotype of genes C and D. A gene knockout of gene B cannot obtain a purple phenotype of gene C, but green phenotype of gene D still can be seen. A gene knockout of gene A cannot obtain the green and purple phenotypes. 
%			- Furthermore, the example of suppressive-interaction is wild type phenotype showing a purple phenotype since gene A suppresses gene B and gene C is active. A gene knockout of gene B has no effect for result purple phenotype. A knockout of gene A results in a nonpurple phenotype since gene B is still suppressing gene C and if both of the genes A and B are knockout will result in wild type phenotype.
%
%		- ADD ALL METHODS FROM 
%			- Table 1: Summary of detect gene-gene interaction using neural network method.
%				COLUMN 1
%				No. Author
%				(1) Ritchie et al. [11]
%				(2) Tomita et al. [12]
%				Keedwell and Narayanan [13]
%				Motsinger et al. [14]
%				Ritchie et al. [15]
%				Motsinger-Reif et al. [16]
%				[17]
%				[18]
%				[4]
%				
%				COLUMN 2
%				Dataset Epistasimodel.
%				Childhood allergic asthma (CAA).
%				Artificial data experiments, rat spinal cord and yeast Saccharomyces Cerevisiae cell cycle.
%				Parkinson’s disease.
%				Alzheimer’s disease, breast’s disease, colorectal disease, and prostate’s disease.
%				Epitasis model.
%				Two-locus disease models, multiplicative and epistasis model.
%				Simulated human.
%				Genetic models.
%				
%				COLUMN 3
%				Description
%				GPNN and BPNN were used to model gene-gene interactions by using simulated data. The simulated data contains functional SNPs and nonfunctional SNPs which model the interaction between genes.
%				Artificial neural network was utilized with parameter decreasing method in order to analyse susceptible SNPs among the Japanese people.
%				Genetic algorithm which was implemented along with neural networks discovers gene-gene interactions in temporal gene expression dataset by elucidating the information between regulatory connections and interactions between genes, proteins, and other gene products.
%				GPNN had been used to optimize the architecture of neural network. This method can be used to enhance the identification of gene combinations associated with Parkinson’s disease.
%				GPNN had been used to detect gene-gene interactions and gene-environment interaction in studies of human disease to optimize the architecture of Neural Network by using simulated dataset.
%				GENN was utilized to discover gene-gene interactions that caused are by noise (for instance, genotyping error, missing data, phenocopy, and genetic heterogeneity) in high dimensional genetic epidemiological data.
%				NN had been used in simulation study to model the different kind of two-locus disease model by constructing six neural networks.
%				ATHENA had been used to discover the gene-gene interactions that influence complex human traits by integrating alternative tree-based crossover, back propagation, and domain knowledge in ATHENA.
%				QTGENN had applied GENN methods to quantitative traits in various types of simulated genetic models. This method had been successfully applied in single-locus models and two-locus models.
%
%
%			- ADD ALL: Table 2: Summary of detect gene-gene interaction using support vector machine method.
%				No. Author
%				Matchenko-
%				(1) Shimko and Dube
%				[23]
%				(2) Chen et al. [19]
%				(3) O ̈ zgu ̈ r et al. [24]
%				(4) Shen et al. [25]
%				(5) Ban et al. [26]
%				(6) Missiuro [21]
%				(7) Fang and Chiu [27]
%				(8) Zhang et al. [28] Marvel and
%				(9) Motsinger-Reif [29]
%				
%				Dataset
%				Simulated disease.
%				Real prostate cancer genotyping.
%				Prostate cancer.
%				Parkinson disease.
%				Type 2 diabetes mellitus-related genes.
%				Caenorhabditis elegans. COGA (genetics of
%				alcoholism). Human cancer.
%				Disease model, M1 and M2.
%				
%				Description
%				Both SVM and artificial neural network (ANN) were used to preselect the combination of SNP to test the importance of potential interactions between genes in complex disease.
%				SVM was applied in different kinds of combinatorial optimization methods which were recursive feature addition, recursive feature elimination, local search, and genetic algorithm.
%				Automatic method that was proposed to extract known genes-disease and infer unknown gene-disease association by using automatic literature mining based on dependency parsing and support vector machines.
%				Authors had employ two-stage method by using SVM with L1 penalty to detect gene-gene interactions for human complex disease.
%				SVM was used to predict the importance of gene-gene interactions in T2D in the studies of Korean cohort studies.
%				SVM was utilized in this research to detect interactions between gene in kinase families for Caenorhabditis elegans organism.
%				SVM-based PGMDR was introduced to study the interactions of gene-gene and gene-covariate in the presence or absence of main effects of genes.
%				Binary matrix shuffling filter (BMSF) as an efficient SVM search schemes was integrated with SVM to classify cancer tissue samples.
%				GESVM was applied in large dataset to select important features, parameters, or kernel in SVM.
%
%
%			- ADD ALL: %				Table 3: Summary of detect gene-gene interaction using random forest method.
%				No. Author
%				(1) Lunetta et al. [33]
%				(2) Jiang et al. [34]
%				(3) Schwarz et al. [35]
%				(4) Liu et al. [36]
%				(5) Winham et al. [32]
%				(6) Pan et al. [37]
%				(7) Staiano et al. [38] Chen and Ishwaran
%				
%				Dataset
%				H2M2, H4M2, H8M2, H16M2, H4M4, and H8M4.
%				Three simulated disease model.
%				Crohn’s disease.
%				NARAC1 and NARAC2.
%				Five models.
%				Bladder cancer.
%				Familial combined hyperlipidemia (FCH).
%				
%				Description
%				RF as a screening procedure to identify top-ranked true-associated SNPs which can cause disease without losing any interactions.
%				RF is used to recognize the cases that were against controls and to obtain the Gini importance which is used to measure the contribution of each SNP to the classification performance.
%				A new method of RJ based on basis RF knowledge was developed to facilitate a fast processing in the high-dimensional of genome-wide analysis data of gene-gene interactions.
%				RF is used to detect contributed gene-gene interactions for identifing RA susceptibility and to identify SNPs of RA patients to classify them into anticyclic citrullinated protein positive and healthy controls.
%				Focus on identifing rarely gene-gene interactions and detecting gene-gene interaction effects and their potential effectiveness on high-dimensional data using RF.
%				The proposed method of MINGRF is proposed to improve the performance of RF such as accuracy and computational time.
%				RF is used to identify gene-gene interactions that are involved in FCH. FCH increase the plasma triglycerides and/or total cholesterol level of patients and hence increase the risk of coronary heart disease.
%				RSF as new hunting pathway to detect gene correlation and genomic interactions from a high-dimensional genomic data.
%
%			- PROS AND CONS FOR EACH MOTHOD: ADD ALL!
%				Table 4: Strengths and weaknesses of neural networks, support vector machine, and random forests methods for detect gene-gene interactions.
%				
%				Methods
%				Neural network
%				Support vector machine (SVM)
%				Random forest (RF)
%				Random jungle (RJ)
%				
%				Strengths
%					Neural network
%					(i) NN is able to model the relationship between disease and single nucleotide polymorphism (SNP)
%					(ii) NN can make prediction on data where the disease outcome is unknown by learning the outcome given on a dataset (iii) NN is a method that can deal with large volumes of data
%					(iv) NN is suitable for genetic heterogeneity, high phenocopy rates, polygenic inheritance, and incomplete penetrance.
%					(v) GPNN and GENN are able to optimize the architecture of NN and possess high power to discover the presence of nonfunctional SNPs.
%					(vi) GPNN does not overfitting the data (vii) GPNN possesses high power in dealing with epitasis model with weak marginal effect
%					(viii) GENN outperform GPNN by optimiz NN in fewer generations
%					(ix) GENN possesses high power to detect high risk loci in complex disease
%
%					Support vector machine (SVM)
%					(i) SVM can deal with high dimension data set
%					(ii) SVM can be utilized to classify complex biological gene expression data (iii) Does not trap at local minima
%					(iv) Not prone to overfitting
%					(v) SVM is robust to noise
%					(vi) The output of SVM is more interpretable if compared to MDR (vii) Does not require user-defined decisions for classification
%					(viii) SVM is ready to be generalized to new structures
%
%					Random forest (RF)
%					(i) RF does not exhibit strong main effects which uncover interactions among genes. (ii) RF does not “overfit” the data.
%					(iii) SNPs predictive of a phenotype are identifying by RF.
%
%					Random jungle (RJ)
%					(i) RJ is able to analyze data on a genome-wide scale.
%					(ii) RJ has more computationally efficient than RF.
%
%			Weaknesses
%					Neural network
%					(i) Presence of black box
%					(ii) Difficult to list out all possible NN architecture and it causes the difficulty to find the optimal architecture
%					(iii) GPNN needed parallel processing environment
%					(iv) GPNN causes the high false positive rate to occur in three locus models
%					(v) The output of GPNN is binary expression, and it can be hard to interpret (for instance, up to 500 nodes)
%					(vi) Result of NN was hard to interpret due to the dimensionality problem
%					(vii) NN needs comprehensive cross-validation to confirm validity
%
%					Support vector machine (SVM)
%					(i) Presence of black box
%					(ii) SVM is restricted to pairwise classification
%					(iii) SVM cannot be directly used for feature selection
%					(iv) Result produced may be affected by the presence of missing data
%					(v) The power of SVM might reduce with the presence of genetic heterogeneity
%					(vi) Additional training maybe needed to correct the bias of prediction accuracy. However, it could be computationally expensive for the proposed procedure (vii) Accuracy produced by SVM might be suboptimal due to the SVM parameter C is forced to be one constant. Hence, a grid search for the parameter is needed by utilizing some promising SNP combinations in order to refine the results.
%
%					Random forest (RF) / Random jungle (RJ)
%					(i) Presence of black box
%					(ii) RF does not succeed in GWAS data. (iii) Sometimes RF is underestimating important scores of SNPs without marginal effects.
%					(iv) RF only detects interactions with large effect size.
%					If the main effects are weak, RJ fails to detect interactions.


%------------------------------------------------------------
% From: "Bayesian inference of epistatic interactions in case-control studies" (2007)
%	Topic: GWAS EPISTASIS METHOD
%
%		- Although some existing computational methods for identifying genetic interactions have been effective for small-scale studies, we here propose a method, denoted ‘bayesian epistasis association mapping’ (BEAM), for genome-wide case-control studies
%		- BEAM treats the disease-associated markers and their interactions via a bayesian partitioning model and computes, via Markov chain Monte Carlo, the posterior probability that each marker set is associated with the disease. 
%		- In the past century, scientists have made great progresses in mapping genes responsible for mendelian diseases. However, genetic variants underlying most common (or ‘complex’) diseases are non-mendelian.
%		- These variants are typically not rare in the population (42%). They show very little effect independently with low penetrance, but they may interact with each other in complex ways.
%		- It has been speculated that epistasis ubiquitously contributes to complex traits partly because of the sophisticated regulatory mechan- isms encoded in the human genome1. 
%		- EPI EXAMPLES: An increasing number of reports have indicated the presence of multilocus interactions in many human complex traits, such as breast cancer2, post-PTCA stenosis3, essential hypertension4, atrial fibrillation5 and type 2 diabetes6.
%		- GWAS EPISTASIS [Discussion]: We also applied BEAM to an association study of age-related macular degeneration (AMD)13, which included B100,000 SNP markers. Although BEAM did not find significant interactions in the AMD data set, it was able to discover two-way or three-way interactions among the B100,000 SNPs simu- lated based on the AMD data.
%
%		- EXISTING METHODS: 
%			- Several approaches have been developed to detect epistasis, including the combinatorial partitioning method (CPM)7, the restricted parti- tioning method (RPM)8, multifactor-dimensionality reduction (MDR)2, multivariate adaptive regression spline (MARS)9, the logistic regression method10 and backward genotype-trait association (BGTA)11. Although these methods all showed promise, they have been tested only on small data sets. 
%			- methods based on brute-force searches such as CPM and MDR are impractical for large data sets
%			- STEPWISE LOGISTIC REGRESSION: The stepwise logistic regression approach of ref. 12 works as follows: (i) all markers are individually tested and ranked for marginal associations with the disease; (ii) the top 10% of markers are selected, among which all k-way (k 1⁄4 2 or 3) interactions are tested and ranked for associations. The authors of ref. 12 also proposed an exhaustive logistic regression testing approach, which we choose not to consider in this study because of its prohibitive computational cost.  Note that even their stepwise approach can become computationally intractable for high-order interactions. 
%			- Recently, a simulation study12 explored the use of a stepwise logistic regression approach to identify two-way and three-way interactions. The authors demon- strated that searching for interactions in genome-wide association mapping can be more fruitful than traditional approaches that exclusively focus on marginal effects.
%
%		- BEAM METHOD:
%			- The BEAM algorithm takes case-control genotype marker data as input and produces, via MCMC simulations, posterior probabilities that each marker is associated with the disease and involved with other markers in epistasis. 
%			- The input genotyped markers should be in their natural genomic order when there is linkage disequilibrium (LD) among some of them. The method can be used either in a ‘pure’ bayesian sense or just as a tool to discover potential ‘hits’. For the former, one relies on the reported posterior probabilities to make inferential statements; as for the latter, one can take the reported hits and use another procedure to test whether these hits are statistically significant. 
%			- The latter approach is more robust to model selection and prior assumptions (such as Dirichlet priors with arbitrary parameters) and is less prone to the slow mixing problem in the MCMC computational procedure. We also propose the B statistic to facilitate the latter approach and show that it is more powerful than the standard w2 statistic for epistasis detections.
%			- For the non-epistasis model (model 1), all three epistasis mapping methods performed similarly to the single-marker w2 test (Fig. 1), indicating that the power for detecting marginal associations was not compromised by using the more complex models.
%			- Notably, results for model 4 suggest that stepwise methods can miss markers with small or no marginal effects, whereas BEAM can get these markers back through iterations.
%		- POWER ISSUES RELATED TO AF: 
%			- The power of association mapping can be greatly hampered by the discrepancy of allele frequencies between unobserved disease loci and associated genotyped markers15
%			- For data sets with large MAF discrepancies and moderate LD, the power of all methods suffered. 
%			- At the extreme case when the MAF discrepancy was maximized (that is, MAF 1⁄4 0.5), all methods had little power in detecting interaction associations
%			- The impact of LD on power seemed to be less profound than the effect of MAF discrepancy. 
%
% 	- GWAS ANALISYS:
%			- DATA: The data set contains 116,204 SNPs genotyped for 96 affected individuals and 50 controls.
%			- RESULTS: BEAM found no significant interactions associated with AMD from this data set. It is possible that the small sample size of 146 individuals is insufficient for detecting subtle epistasis interactions.


The definition of epistasis from a statistical perspective is a ``departure from a linear model" \cite{cordell2009detecting}. This means that in a logistic regression model the input for sample $s$ includes terms with each of the genotypes at loci $i$ and $j$), as well as an ``interaction term" $g_{s,i} \cdot g_{s,j}$ \cite{cordell2002epistasis}. 

\begin{eqnarray*} \label{eq:gwasLogRegH1}
    P( d_s | g_{s,i},g_{s,j}) & = & \phi[ \theta_0 + \theta_1 g_{s,i} + \theta_2 g_{s,j} + \theta_3 (g_{s,i} g_{s,j}) \\
    & & ... + \theta_4 c_{s,1} + ... + \theta_m c_{s,N_{cov}} ] \\
\end{eqnarray*}

where $d_s$ is disease status, $\phi(\cdot)$ is the sigmoid function, $c_{s,1}, c_{s,2}, ... $ are covariates for sample $s$.

Models involving interactions between more than two variants can be defined similarly, but require more parameters and extremely large samples are required to accurately fit them.

Several families of approaches for epistatic GWAS exist. Here we mention a few:

\begin{itemize}

\item Allele frequency: In \cite{ackermann2012systematic}, an analysis of imbalanced allele pair frequencies is performed under the assumption that an implicit test for fitness can be achieved looking for over/under-represented allele pairs in a given population. In another study \cite{zhao2006test} the authors infer that interactions can create LD in disease population under two-loci model, then they show how LD-based p-values can uncover interaction and sometimes (in their simulations) outperform logistic regression tests.

\item Bayesian model: In \cite{zhang2007bayesian}, a ``Bayesian partitioning model" is used by providing Dirichlet prior distributions for each partition and computing posterior probabilities using Markov chain Monte Carlo (MCMC) algorithms.  The methodology first test individual makers and picks only the top 10\% to further investigate for epistasis, because it is prohibitive to test all loci.

\item Machine learning: From a machine learning point of view, finding interacting variants is simply an \textit{``optimisation procedure is to find a set of parameters that allows the machine-learning model to most accurately predict class membership (e.g. affected vs unaffected)"} \cite{mckinney2006machine}. Several approaches have emerged to tackle the ``interaction problem" and used a variety of different techniques \cite{koo2013review, mckinney2006machine} , such as neural networks, cellular automata, random forests, multifactor dimensionality reduction, support vector machines, etc.

\end{itemize}

Although all these models have advantages under some assumptions, none of them seems to be a ``clear winner" over the rest \cite{cordell2009detecting}. All of these models suffer from the increase in number of tests that need to be performed, which raises two issues: i) multiple testing, which is often resolved by stringent significance threshold, and ii) computational feasibility, which is solved by efficient algorithms, parallelization, and heuristic approaches to quickly discard uninformative loci combinations. So far, no method for epistatic GWAS has been widely adopted and there is need of different approaches to be explored. In Chapter \ref{ch:gwas} we propose an approach to combine co-evolutionary models and GWAS epistasis of pairs of putatively interacting loci.

%---
\section{Thesis roadmap and Contributions}
%---

The original research presented in this thesis covers topics related to the computational and statistical methodologies related to the analysis of sequencing variants to unveil genetic links to complex disease. Broadly speaking, we address three types of problems: (i) Data processing of large datasets from high throughput biological experiments such as resequencing in the context of a GWAS (Chapter \ref{ch:bds}); (ii) functional annotations, i.e. calculating variant's impact at the molecular, cellular or even clinical level (Chapter \ref{ch:snpeff}); (iii) identification of genetic risk factors for complex disease using models that combine population-level and evolutionary-level data to detect putative epistatic interactions (Chapter \ref{ch:gwas}). When applicable, background material specific to each chapter is presented in a preface, together with an explanation of how that chapter ties in with the rest of the thesis.

This thesis comprises text and figures of articles that have either been published, submitted for publication, or ready to be submitted (waiting upon data embargo restrictions):
\\

\begin{description}
	
	\item[Chapter \ref{ch:bds}] ~ 
	
		\begin{enumerate}
			\item \textbf{P. Cingolani}, R. Sladek, and M. Blanchette. ``BigDataScript: a scripting language for data pipelines." Bioinformatics 31.1 (2015): 10-16.
		\end{enumerate}

		For this paper, PC conceptualized the idea and performed the language design and implementation. RS \& MB helped in designing robustness testing procedures. PC, RS \& MB wrote the manuscript.
		\\
	
	\item[Chapter \ref{ch:snpeff}] ~
	
		\begin{enumerate}[resume]
			\item \textbf{P. Cingolani}, A. Platts, M. Coon, T. Nguyen, L. Wang, S.J. Land, X. Lu, D.M. Ruden, et al. ``A program for annotating and predicting the effects of single nucleotide polymorphisms, snpeff: Snps in the genome of drosophila melanogaster strain $w^{1118}; iso-2; iso-3$". Fly, 6(2), 2012.
		\end{enumerate}

		For this paper, PC conceptualized the idea, implemented the program and performed testing.
		AP contributed several feature ideas, software testing and suggested improvements.
		XL, DR, SL, LW, TN, MC, LW performed mutagenesis and sequencing experiments.
		XL and DR performed the biological interpretation of the data.
		All authors contributed to the manuscript.
		\\

		SnpEff's accompanying publication (SnpSift):
	
		\begin{enumerate}[resume]		
			\item \textbf{P. Cingolani}, V. M. Patel, M. Coon, T. Nguyen, S. Land, D. M. Ruden, and X. Lu.`` Using drosophila melanogaster as a model for genotoxic chemical mutational studies with a new program, snpsift". Toxicogenomics in non-mammalian species, page 92, 2012.
		\end{enumerate}
		
		~ \\

		We used SnpEff \& SnpSift and developed a number of new functionalities in the context of two collaborative GWAS projects on type II diabetes:
	
		\begin{enumerate}[resume]
		
			\item M. McCarthy, T2D Genes Consortia. ``Variation in protein-coding sequence and predisposition to type 2 diabetes", Ready for submission.
			
			\item A. Mahajan, X. Sim, H. Ng, A. Manning, M. Rivas, H. Heather, A. Locke, N. Grarup, H. K. Im, \textbf{P. Cingolani}, et. al. ``Identification and Functional Characterization of G6PC2 Coding Variants Influencing Glycemic Traits Define an Effector Transcript at the G6PC2-ABCB11 Locus." PLoS genetics 11.1 (2015): e1004876-e1004876.
		
		\end{enumerate}
		~ \\
	
	\item[Chapter \ref{ch:gwas}] ~
	
		\begin{enumerate}[resume]
		\item \textbf{P. Cingolani}, R. Sladek, and M. Blanchette. ``A co-evolutionary approach for detecting epistatic interactions in genome-wide association studies". Ready for submission (data embargo restrictions).
		\end{enumerate}
	
		For this paper, PC designed the methodology under the supervision of MB and RS. PC implemented the algorithms. PC, RS \& MB wrote the manuscript. This work uses data from the T2D consortia, thus it cannot be published until the main T2D paper is accepted for publication (according to T2D data embargo).
		\\
	
	\item[Other contributions] ~	\linebreak
		During my thesis I have co-authored several other scientific articles (grouped by topic) published, submitted for publication, or ready to be submitted, not mentioned in this thesis:
		\\

	\item[Epigenetics] ~

		\begin{enumerate}[resume]
			\item \textbf{P. Cingolani}, X. Cao, R. Khetani, C.C. Chen, M. Coon, A. Bollig-Fischer, S. Land, Y. Huang, M. Hudson, M. Garfinkel, and others. ``Intronic Non-CG DNA hydroxymethylation and alternative mRNA splicing in honey bees." BMC genomics 14.1 (2013): 666.
			\item M. Senut, A. Sen, \textbf{P. Cingolani}, A. Shaik, S. Land, Susan J and D. M. Ruden. ``Lead exposure disrupts global DNA methylation in human embryonic stem cells and alters their neuronal differentiation." Toxicological Sciences (2014).
			\item D. M. Ruden, \textbf{P. Cingolani}, A. Sen, W. Qu, L. Wang, M. Senut, M. Garfinkel, V. Sollars, X. Lu, ``Epigenetics as an answer to Darwin's 'special difficulty' Part 2: Natural selection of metastable epialleles in honeybee castes", Frontiers in Genetics (2015).
			\item M. Senut, A. Sen, \textbf{P. Cingolani}, A. Shaik, S. Land, Susan J and D. M. Ruden. ``Lead exposure induces changes in 5-hydroxymethylcytosine clusters in CpG islands in human embryonic stem cells and umbilical cord blood", Submitted to `Epigenomics.
			\item M. Senut, \textbf{P. Cingolani}, A. Sen, Arko, A. Kruger, A. Shaik, H. Hirsch, S. Suhr, D. Ruden. ``Epigenetics of early-life lead exposure and effects on brain development." Epigenomics 4.6 (2012): 665-674.
		\end{enumerate}
		~ \\
	
	\item[GWAS \& Disease] ~
	
		\begin{enumerate}[resume]
			\item K. Oualkacha, Z. Dastani, R. Li, \textbf{P. Cingolani}, T. Spector, C. Hammond, J. Richards, A. Ciampi, C. Greenwood. ``Adjusted sequence kernel association test for rare variants controlling for cryptic and family relatedness." Genetic epidemiology 37.4 (2013): 366-376.
			\item S. Bongfen, I. Rodrigue-Gervais, J. Berghout, S. Torre, \textbf{P. Cingolani}, S. Wiltshire, G. Leiva-Torres, L. Letourneau, R. Sladek, M. Blanchette, and others. ``An N-ethyl-N-nitrosourea (ENU)-induced dominant negative mutation in the JAK3 kinase protects against cerebral malaria." PloS one 7.2 (2012): e31012.
			\item C. Meunier, L. Van Der Kraak, C. Turbide, N. Groulx, I. Labouba, Ingrid, \textbf{P. Cingolani}, M. Blanchette, G. Yeretssian, A. Mes-Masson, M. Saleh, and others. ``Positional mapping and candidate gene analysis of the mouse Ccs3 locus that regulates differential susceptibility to carcinogen-induced colorectal cancer." PloS one 8.3 (2013): e58733.
			\item G. Caignard, G. Leiva-Torres, M. Leney-Greene, B. Charbonneau, A. Dumaine, N. Fodil-Cornu, M. Pyzik, \textbf{P. Cingolani}, J. Schwartzentruber, J. Dupaul-Chicoine, and others. ``Genome-wide mouse mutagenesis reveals CD45-mediated T cell function as critical in protective immunity to HSV-1." PLoS pathogens 9.9 (2013): e1003637.
			\item M. Bouttier, D. Laperriere, M. Babak Memari, M. Verway, E. Mitchell, \textbf{P. Cingolani}, T. Wang, M. Behr, R. Sladek, M. Blanchette, S. Mader and J. White. ``Genomics analysis reveals elevated LXRα signaling reduces M. tuberculosis viability", Submitted to Journal of Clinical Investigation.
			\item M. Bouttier, D. Laperriere, M. Babak Memari, M. Verway, E. Mitchell, \textbf{P. Cingolani}, T. Wang, M. Behr, R. Sladek, M. Blanchette, S. Mader and J. White. ``Genomic analysis of enhancers engaged in M. tuberculosis-infected macrophages reveals that LXR signaling reduces mycobacterial burden", Submitted to PLOS Pathogens.
		\end{enumerate}	
		~ \\
	
	\item[Fuzzy logic] ~

		\begin{enumerate}[resume]
			\item \textbf{P. Cingolani} and Jesus Alcala-Fdez. ``jFuzzyLogic: a robust and flexible Fuzzy-Logic inference system language implementation." FUZZ-IEEE. 2012.
			\item \textbf{P. Cingolani} and Jesus Alcala-Fdez. ``jFuzzyLogic: a java library to design fuzzy logic controllers according to the standard for fuzzy control programming." International Journal of Computational Intelligence Systems (2013), vol 6, pages 65-75.
		\end{enumerate}	

\end{description}
