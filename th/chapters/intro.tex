%-----------------------------------------------------------------------------
\chapter{Introduction \label{ch:intro}}
%-----------------------------------------------------------------------------

How does one's DNA influence their risk of getting a disease? Contrary to popular belief, your future health is not ``hard wired" in your DNA. Only in a few diseases, referred as ``Mendelian diseases", are there well known, almost certain, links between genetic mutations and disease susceptibility. For the majority of what are known as ``complex traits", such as cancer or diabetes, genomic predisposition is subtle and, so far, not fully understood.

With the rapid decrease in the cost of DNA sequencing, the complete genome sequence of large cohorts of individuals can now be routinely obtained. This wealth of sequencing information is expected to ease the identification of genetic variations linked to complex traits. In this work, I investigate the analysis of genomic data in relation to complex diseases, which offers a number of important computational and statistical challenges. We tackle several steps necessary for the analysis of sequencing data and the identification of links to disease. Each step, which corresponds to a chapter in my thesis, is characterized by very different problems that need to be addressed.

\begin{itemize}

\item[i)] The first step is to analyze large amounts of information generated by DNA sequencers to obtain a set of ``genomic variants" present n each each individual. To address these big data processing problems, Chapter \ref{ch:bds} shows how we designed a programming language (BigDataScript \cite{cingolani2015bigdatascript}), that simplifies the creation robust, scalable data pipelines.

\item[ii)] Once genomic variants are obtained, we need to prioritize and filter them to discern which variants should be considered ``important" and which ones are likely to be less relevant. We created the SnpEff \& SnpSift \cite{cingolani2012program, cingolani2012using} packages that, using optimized algorithms, solve several annotation problems: a) standardizing the annotation process, b) calculating putative genetic effects, c) estimating genetic impact, d) adding several sources of genetic information, and e) facilitating variant filtering. 
					
\item[iii)] Finally, we address the problem of finding associations between interacting genetic loci and disease. One of the main problems in GWAS, known as ``missing heritability", is that most of the phenotypic variance attributed to genetic causes remains unexplained. Since interacting genetic loci (epistasis) have been pointed out as one of the possible causes of missing heritability, finding links between such interactions and disease has great significance in the field. We propose a methodology to increase the statistical power of this type of approaches by combining population-level genetic information with evolutionary information. 

\end{itemize}

In a nutshell, this thesis addresses computational, analytical, algorithmic and methodological problems of transforming raw sequencing data into biological insight in the aetiology of complex disease. In the rest of this introduction we give the background that provides motivation for our research. 

%---
\section{Genomes and genetic variants \label{sec:introRef}}
%---

DNA is composed of four basic building blocks, called ``bases'' or ``nucleotides'' \cite{alberts1995molecular}. These four nucleotides, usually abbreviated $\{A, C, G, T\}$, are Adenine, Cytosine, Guanine, and Thymine. Bases form pairs, either as $A-T$ or $C-G$, that pile-up forming two long polymers, with backbones that run in opposite directions giving rise to a double-helix structure \cite{watson1953molecular}. Arbitrarily, one of the polymers is called the positive strand and the other is called the negative strand. 

Proteins are composed by chains of amino acids and, as explained by the central dogma of biology \cite{alberts1995molecular},  DNA is the template that instructs cellular machinery how to produce proteins. There are 20 amino acids, which are the building blocks of all proteins. Each of the twenty amino acids is encoded by a group of three DNA bases called ``codon'' \cite{crick1961general}. More than one codon can code for the same amino acid (i.e. $4^3=64$ codons $ > 20 $ amino acids) allowing for code redundancy. Additionally, there are codons that mark the end of the protein, these are called ``STOP" and signal molecular machinery to end the translation process \cite{brenner1965genetic}.

Proteins compose up to 50\% of a cell's dry weight compared to 3\% of the DNA \cite{alberts1995molecular}. Proteins perform their functions mainly by interacting with other proteins, forming complex pathways that lead to a vast array of cellular functions including catalysis of chemical reactions, cell signaling, and structural conformation of the cell \cite{alberts1995molecular}. The 3-dimensional structure of the protein, also called ``tertiary structure", is tailored to bind to other proteins in a specific manner to accomplish a specific function. 

The human genome has a total of 3 Giga-base-pairs (Gb), and those bases are divided into 22 ``autosomal'' chromosome pairs (in each pair one chromsome is maternally inherited and the other paternally inherited) and ``sex" chromosomes. The longest of the autosomal chromosomes is roughly 250 Mega-bases (Mb) and the shortest one is ~50 Mb.

In order to compare DNA from different individuals (or samples), we need a ``reference genome". Having a standard reference sequence facilitates comparisons and analysis. For most well studied organisms, ``reference genome" sequences are available and current large scale sequencing projects are extending significantly the number of genomes known, e.g. one project seeks to sequence 10,000 mammalian genomes \cite{haussler2009genome}, another is targeting all microbes that live within the human gut \cite{turnbaugh2007human}. The human reference genome (e.g. GRCh37) does not correspond to the DNA of any particular person, but to a ``mosaic" of the genomes of thirteen anonymous volunteers from Buffalo, New York \cite{schneider2013genome}.

When the genome of an individual is sequenced, the DNA is compared to the ``reference genome". Most of the DNA is the same, but there are differences. These differences, generically known as ``genetic variants" (or ``variants", for short), describe the particular genetic make-up of each individual. There are several different ways a sample can differ from a reference genome. Each variant is the result of a mutations that happened at some point in the evolutionary history of the individual (or that of the reference genome). Variant types can be roughly categorized in the following way:

\begin{description}

	\item[Single nucleotide variants (SNV)] or Single nucleotide polymorphism (SNP) are the simplest and more common variants produced by single base difference (e.g. a base in the reference genome, at a given coordinate,  is an `A', whereas the sample is `C'). Depending on whether the variant was identified in an individual or in a population, it is called a Single Nucleotide Variant (SNV) or Single Nucleotide Polymorphism (SNP). It is estimated that there are roughly $3.6M$ SNPs per individual \cite{10002012integrated}. There are several biological mechanisms responsible for this type of variants: i) replication errors, ii) errors introduced by DNA repair mechanism, iii) deamination (a base is changed by hydrolysis which may not be corrected by DNA repair mechanisms), iv) tautomerism (and alteration on the hydrogen bond that results in an incorrect pairing) \cite{griffiths2005introduction}.

	\item[Multiple nucleotide polymorphism (MNP)] are sequence differences affecting several consecutive nucleotides and are typically treated as a single variant locus if they are in perfect linkage disequilibrium (e.g. reference is ‘ACG’ whereas the sample is ‘TGC’). .

	\item[Insertions (INS)] refer to a sample having one or more extra base(s) compared to the reference genome (e.g. the reference sequence is ‘AT’ and the sample is ‘ACT’). Short insertions and deletions (indels) of a chromosome region range from 1 to 20 bases in length are reported to be 10 to 30 times less frequent than SNV \cite{10002012integrated}. Small insertions are usually attributed to DNA polymerase slipping and replicating the same bases (this produces a type of insertion known as duplication). Large insertions can be caused by unequal cross-over event (during meiosis) or transposable elements.

	\item[Deletions (DEL)] are the opposite of insertions, the sample has some base(s) removed with respect to the reference genome (e.g. reference is ‘ACT’ and sample is ‘AT’). As in the case of insertions, deletions can also be caused by ribosomal slippage, cross-over events during meiosis. Those include large deletions, which can result in the loss of an exon or one or more whole genes \cite{alberts1995molecular}. Short deletions are 10 to 30 times less frequent than SNV \cite{10002012integrated}.

	\item[Copy number variations (CNVs)] arise when the sample has two or more copies of the same genomic region (e.g. a whole gene that has been duplicated or triplicated) or conversely, when the sample has fewer copies than the reference genome. Copy number variations are often attributed to homologous recombination events \cite{alberts1995molecular}.

	\item[Rearrangements] such as inversions and translocations are events that involve two or more genomic breakpoints and a reorganization of genomic segments, possibly resulting in gene fusions or loss of critical regulatory elements. Inversions, a type of rearrangement, result from a whole genomic region being inverted.

\end{description}

\noindent As humans have two copies of each autosome, variants could affect zero, one or two of the chromosomes and are called ``homozygous reference", ``heterozygous", and ``homozygous alternative" respectively. Variants are also classified based on how common they are within the population: common ($\ge 5\%$), low frequency ($\le 5\%$), or rare ($\le 0.1\%$). How these types of genetic variants influence traits or disease risk is a topic of intense research that is discussed throughout this thesis.

\section{DNA and disease risk}

It would be fair to say that the Garrod family was fascinated by urine. As a physician at King’s College, Alfred Baring Garrod, discovered gout related abnormalities in uric acid \cite{kennedy2001}. His son, Sir Archibald Garrod, was interested in a condition known as alkaptonuria, in which children are mostly asymptomatic except for producing brown or black urine, but by the age of 30 individuals develop pain in joints of the spine, hips and knees. In 1902, Archibald observed that the family inheritance pattern of alkaptonuria resembled Mendel’s recessive pattern and postulated that a mutation in a metabolic gene was responsible for the disease. Publishing his finding he gave birth to a new field of study known as ``Human biochemical genetics" \cite{kennedy2001}.

Diseases having simple inheritance patterns, such as alkaptonuria, cystic fibrosis, phenylketonuria and Huntington's are also known as Mendelian diseases \cite{kennedy2001}. The genetic components of several Mendelian diseases have been discovered since the mechanism was first elucidated by Garrod in 1902 and the process has been accelerated in recent years, thanks to the application of DNA sequencing techniques \cite{bamshad2011exome}.

In complex diseases (or complex traits), such as diabetes or Alzheimer's disease, affected individuals cannot be segregated within pedigrees (i.e. no simple pattern of inheritance can be identified). In contrast to Mendelian diseases the aetiology of complex traits is complicated due to factors such as: incomplete penetrance (symptoms are not always present in individuals who have the disease-causing mutation) and genetic heterogeneity (caused by any of a large number of alleles). This makes  it difficult to pinpoint the genetic variants that increase risk of complex disease.

\subsection{Heritability and Missing heritability}

We all know that ``tall parents tend to have tall children", which is an informal way to say that height is a highly heritable trait. It is said that there are 30 cm from the tallest 5\% to the shortest 5\% of the population and genetics account for 80\% to 90\% of this variation \cite{wood2014defining}, which means that 27cm of variance are assumed to be ``carried" by DNA variants from parents to offspring. Since 2010 the GIANT consortia has been investigating the genetic component of complex traits like height, body mass index (BMI) and waist to hip ratio (WHR). Even though they found many variants associated those traits, their findings only explain 10\% of the phenotypic variance which corresponds to only a few centimeters in height \cite{wood2014defining}.

In order to measure heritability we need a formal definition. Heritability is defined as the proportion of phenotypic variance that is attributed to genetic variations. The total phenotypic variation is assumed to be caused by a combination of ``environmental" and genetic variations $Var[P] = Var[G] + Var[E] + 2 Cov[G, E]$ 
\iffinal
\footnote{Although the referenced paper's notation does not seem absolutely consistent, we quote Emerson \textit{``A foolish consistency is the hobgoblin of little minds"} and proceed...}
\fi
.

The environmental variance $Var[E]$ is the phenotypic variance attributable only to environment, that is the variance for individuals having the same genome $Var[E] = Var[P|G]$. This can be estimated by studying monozygotic and dizygotic twins.

If the covariance factor $Cov[G, E]$ is assumed to be zero, we can define heritability as $H^2 = \frac{Var[G] }{ Var[P]}$. This is called ``broad sense heritability" because $Var[G]$ takes into account all possible forms of genetic variance: $Var[G] = Var[G_A] + Var[G_D] + Var[G_I]$, where $Var[G_A]$ is the additive variance, $Var[G_D]$ is the variance form dominant alleles, and $Var[G_I]$ is the variance form interacting alleles (epistasis). Non-additive terms are difficult to estimate, so a simpler form of heritability called ``narrow sense heritability" that only takes into account additive variance is defined as $h^2 = \frac{ Var[G_A] }{ Var[P] }$ \cite{zuk2012mystery}.

Focusing on narrow sense heritability, the concept of ``explained heritability" is defined as the part of heritability due to known variants with respect to phenotypic variation ($\pi_{explained} = h^2_{known} / h^2_{all}$). Similarly, missing heritability is defined as $\pi_{missing} = 1 - \pi_{explained} = 1 - h^2_{known} / h^2_{all}$. When all variants associated with traits are known, then $\pi_{missing} = 0$.

Until recently, it was widely assumed by the research community that the problem of missing heritability lied in finding the appropriate genetic variants to account for the numerator of the equation ($h^2_{known}$) \cite{zuk2012mystery}. However, in a series of theorems published recently, it has been proposed that there is a problem in the way the denominator is estimated \cite{zuk2012mystery}. The authors created a limiting pathway model ($LP(k)$) that accounts for epistasis (gene-gene interactions) in $k$ biological pathways. They showed that a severe inflation of $h^2_{all}$ estimators occurs even for small values of $k$ (e.g. $k \in [2,10]$). As a result, genetic variants estimated to account only for $20\%$ of heritability, could actually account for as much as $80\%$ using an appropriate model \cite{zuk2012mystery}.

Even though this result is encouraging, the problem is now shifted to detecting epistatic interactions, a problem that we discuss in section \ref{sec:epi} and Chapter \ref{ch:gwas}. In the same work \cite{zuk2012mystery}, the authors show an example of power calculation assuming relatively large genetic effect that would require sequencing roughly $5,000$ individuals to detect links to genetic variants, which is a large but nowadays not uncommon, sample size. Nevertheless other estimates place the sample size requirements as high as  $500,000$ individuals \cite{zuk2012mystery}. Even though this sounds as an extremely large number of samples, it is quickly becoming possible thanks to large technological advances and cost reductions in sequencing and genotyping technologies.

\subsection{Conclusions}

Although some genetic causes of complex traits, such as type II diabetes, have been found, only a small portion of the phenotypic variance can be explained. This might indicate that many risk variants are yet to be discovered. Recent studies on the topic of missing heritability report that these ``difficult to find genetic variants" might be in epistatic interaction (analyzed in section \ref{sec:epigwas}) or rare variants (see section \ref{sec:comonrare}). Analysis of either them requires more complex statistical models and larger sample sizes with the corresponding increase in computational requirements. In Chapter \ref{ch:gwas} of this thesis, we focus on methods for finding epistatic interactions related to complex disease and develop computationally tractable algorithms that can process data from sequencing experiments involving large number of samples in a reasonable amount of time.

%---
\section{Identification of genetic variants}
%---

Two of the main milestones in genetics were the discovery of the DNA structure in 1953 \cite{watson1953molecular}, followed by the first draft of the human genome in 2004 \cite{collins2004finishing}. The cost of sequencing the first human reference genome was around \$3 billion (unadjusted US dollars) and it was an endeavor that took around 10 years. Since that time, sequencing technology has evolved substantially so that a human genome can now be sequenced in three days for a price of less than \$1,000, according to prices estimated by Illumina, one of the main genome sequencer manufacturers.

The amount of information delivered by sequencing devices is growing faster than computer speed (Moore's law) and data storage capacity. Just as a crude example, a leading edge sequencing system is advertized to be capable of delivering 18,000 human genomes at $30 \times$ coverage per year, yielding over 3.2 PB of information. Having to process huge amounts of sequencing information poses several challenges, a problem informally known as ``data deluge''. In this section, we explain how sequencing data is generated and how the huge amount of information delivered by a sequencer can be handled in order to make the problem tractable. We want to transform this raw data into knowledge of genomic variants that contribute to disease risk with the ultimate goal to translate these risk variants into biological knowledge. As expected, processing huge datasets consisting of thousands of sample is a complex problem. In Chapter \ref{ch:bds} we show how mitigate or solve some of these issues, by designing a computer language specially tailored to tackle what are know as ``Big data" problems.

\subsection{Sequencing data}

DNA sequencing machines (or sequencers) are based on different technologies. In a nutshell, a sequencer detects a set of polymers (or chains) of DNA nucleotides and outputs a set of strings of A, C, G, and Ts. Unfortunately, current technological limitations make it impossible to ``read" a full chromosome as one long DNA sequence. Instead, modern sequencers produce a large number of ``short reads", which range from 100 bases to 20 Kilo-bases (Kb) in length, depending on the technology. Since sequencers are unable to read long DNA chains, preparing the DNA for sequencing involves fragmenting it into small pieces. These DNA fragments are a random sub-samples of the original chromosomes. Reading each part of the genome several times allows to increase accuracy and ensure that the sequencer reads as much as possible of the original chromosomes. The coverage of a sequencing experiment is defined as the number of times each base of the genome is read on average. For instance, if the sequencing experiment is designed to produce one billion reads, and each read is 150 bases long, then the total number of bases read is 150Gb. Since the human genome is 3Gb, the coverage is said to be $50 \times$.

After sequencing a sample, we have millions of reads but we do not know where these reads originate from in the genome. This is solved by aligning (also called mapping) reads to the reference genome, which is assumed to be very similar to the genome being sequenced. Once the reads are mapped, we can infer if the sample's DNA has any differences with respect to the reference genome, a problem is known as ``variant calling''. 

Although sequencing costs are dropping fast, it is still expensive to sequence thousands of samples and in some cases it makes sense to focus on specific areas of the genome. A popular experimental setup is to focus on coding regions (exons). A technique called ``exome sequencing" consists of capturing exons using a DNA chip and then sequencing the captured DNA fragments only. Exons are roughly 1.2\% of the genome, thus this technique reduces sequencing costs significantly, for which it has been widely used by many research groups although it has the disadvantage of only analysing coding genomic variation.

\subsection{Read mapping}

Once the samples have been sequenced, we have a set of reads from the sequencer. The first step in the analysis is finding the location in the reference genome where each read is supposed to originate from, a process that is complicated by a several factors: i) there are differences between the reference genome and the sample genome, ii) sequencing reads may contain errors, iii) several parts of the reference genome are quite similar making reads from those regions indistinguishable, and iv) a typical sequencing experiment generates millions of reads.

\paragraph{Local sequence alignment} We introduce a problem known as \textit{local sequence alignment}: Given two sequences $s_1$ and $s_2$ from an alphabet (e.g. $\Sigma = \{A,C,G,T\}$), the alignment problem is to add gap characters (`-') to both sequences, so that a distance, such as Levenshtein distance, $d(s_1,s_2)$ is minimized. This problem has a well known solution, the Smith-Waterman algorithm \cite{smith1981identification}, which is a variation of the global sequence alignment solution from Needleman-Wunsch \cite{needleman1970general}, having an algorithm complexity $O(l_1 . l_2)$ where $l_1$ and $l_2$ are the length of the sequences. So, Smith-Waterman algorithm is slow since in this case one of the sequences is the entire genome.

In order to speed up sequence alignments, several heuristic approaches emerged. Most notably, BLAST \cite{altschul1990basic}, which could be for mapping sequences to a reference genome. BLAST uses an index of the genome to map parts of the query sequence, called seeds, to the reference genome. Once these seeds have been positioned against the reference, BLAST joins the seeds performing an alignment only using a small part of the reference.

\paragraph{Read mapping} Sequence alignment has an exact algorithm solution and several faster heuristic solutions. But even the fastest solutions are too slow to be used with the millions of reads generated in a typical sequencing experiment. Faster algorithms can be used if we relax our requirements in two ways: i) we allow for sub-optimal results, and ii) instead of requiring the output to be a complete local alignment between a read and the genome, we just want to know the region in the reference genome where the read sequence is from. This relaxed version of the alignment algorithm is called ``read mapping'' and the reduced complexity is enough to speed up the computations significantly. In a nutshell, a read mapping is regarded as correct if it overlaps the true reference genome region where the read originated. Once the mapping is performed, the read is locally aligned, a strategy similar to BLAST algorithm \cite{li2010fast, langmead2009ultrafast}.

Reformulating this as a \textit{mapping} problem allows us to use data structures such as suffix trees to index the reference genome. Using suffix trees we can query for a substring (read) \cite{durbin1998biological} of the indexed string in $O(m)$ time, where $m$ is the length of the query. Alternatively, we can use suffix arrays which are a space optimized alternative to suffix trees \cite{durbin1998biological}. An implicit assumption in this solution, is that the read will be very similar to the reference and that there will be no big gaps. Suffix arrays algorithms are fast but, even though they are memory optimized versions of suffix trees, memory requirements are still high ($O[ n \; log(n) ]$, where $n$ is the length of the indexed sequence -the genome-) and this becomes the limiting factor. In order to reduce memory footprint of suffix arrays, Ferragina and Manzini \cite{ferragina2000opportunistic} created a data structure based on the Burrows-Wheeler transform.  This structure, known as an FM-Index, is memory efficient yet fast enough to allow mapping high number of reads.  An FM-index for the human genome can be built in only 1Gb of memory, compared to 12Gb required for an equivalent suffix array \cite{li2010fast}.  Given a genome $G$ and a read $R$, an FM-index search can find the $N_{occ}$ exact occurrences of $R$ in $G$ in $O(|R| + N_{occ} )$ time, where $|R|$ is the length of $R$ \cite{li2010fast}. 

We should keep in mind that suffix trees, suffix arrays and FM-indexes are guaranteed to find all matching substring occurrences, nevertheless a sequencing read may not be an exact substring of the reference genome (due to sample's genome differences with the reference genome, read errors, etc.). So, even if efficient indexing and heuristic algorithms can decrease mapping time considerably, these algorithms are not guaranteed to find an optimal mapping.  Several parameters, such as read length, sequencing error profile, and genome complexity profile can affect performance.  The most commonly used implementation of the FM-index mapping algorithms are BWA \cite{li2010fast, li2010fastlong} and Bowtie \cite{langmead2009ultrafast, langmead2012fast}.  Each of them provide optimized versions for the two most common sequencing types: i) short reads with high accuracy \cite{li2010fast,langmead2009ultrafast} or ii) longer reads with lower accuracy \cite{li2010fastlong, langmead2012fast}.

\paragraph{Mapping quality\label{sec:mapq}} Sequencers not only provide sequence information, but also provide an error estimate for each base \cite{li2011statistical}.  This is often referred as a quality ($Q$) value, which is the probability of an error, measured in negative decibels $Q = -10 \; log_{10}(\epsilon)$, where $\epsilon$ is the error probability. Mapping quality is an estimation of the probability that a read is incorrectly mapped to the reference genome. Mapping algorithms provide estimates of mapping errors. In the MAQ model \cite{li2008mapping}, which is one of the earliest models for calculating mapping quality, three main sources of error are explored: i) the probability that a read does not originate from the reference genome (e.g. sample contamination); ii) the probability that the true position is missed by the algorithm (e.g. mapping error); and iii) the probability that the mapping position is not the true one (e.g. if we have several possible mapping positions). It is assumed that the total error probability can be approximated as $\epsilon \approx max(\epsilon_1,\epsilon_2, \epsilon_3)$.

\subsection{Variant calling}

Genome-wide variant calling has until recently largely been done using genotyping arrays (for SNVs) or Comparative Genomic Hybridization arrays (for CNVs). The inherent limitations of these technologies, particularly their ability to only assay genotypes at sites that are known in advance to be polymorphic, combined with the declining cost of sequencing, have now made approaches based on high-throughput resequencing the tool of choice for variant calling in clinical studies. 

Once the sequencing reads have been mapped to the reference genome, we can try to find the differences between a sequenced sample and the reference genome. This process is called ``variant calling" \cite{nielsen2011genotype}.  Several factors complicate this task, the two main ones being sequencing errors and mapping errors, described in \ref{sec:mapq}. Based on sequencing data and mapping error estimates, tools such as GATK \cite{mckenna2010genome} and SamTools/BcfTools \cite{li2008mapping} use maximum likelihood models can infer when there is a mismatch between a sample and the reference genome and whether the sample is homozygous or heterozygous for the variant. This method works best for differences of a single base (SNV), but it can also work with different degrees of success for short insertions or deletions (InDels) usually consisting of less than 10 bases. 

Aligning sequences that contain InDels (gaps) is more difficult than ungapped alignments since finding optimal gap boundary depends on the scoring method being used. This biases variant calling algorithms towards detecting false SNVs near InDels \cite{depristo2011framework}.  An approach to reduce this problem is to look for candidate InDels and perform a local realignment in those regions.  This local re-alignment process reduces significantly the number of false positive SNVs \cite{depristo2011framework}. Another approach to reduce the number of false positive SNVs calls near InDels involves the ``Base Alignment Quality" (BAQ) \cite{li2011improving}, which is the probability of misalignment for each base.  It can be shown that replacing the original base quality with the minimum between base quality and BAQ produces an improvement in SNV calling accuracy.  The BAQ can be calculated using a special type of ``Hidden Markov Model" (HMM) designed for sequence alignment \cite{li2011improving, durbin1998biological}. A more sophisticated option for reducing errors consist of performing a local genome re-assembly on each polymorphic region (e.g. HaplotypeCaller algorithm \cite{GATK}).

Finally, one should note that the error probabilities inferred by the sequencers are far from perfect.  Once the variants have been called, empirical error probabilities can be easily calculated \cite{mckenna2010genome} by comparing sequenced variants to a set of ``gold standard variants" (i.e. variants that have been extensively validated).  This allows to re-calibrate or re-estimate the error profile of the reads.  This is know as a re-calibration step, and usually improves the number of false positives calls \cite{depristo2011framework}.

Due to the nature of short reads, this family of methods does not work for structural genomic variants, such as large insertions, deletions, copy number variations, inversions, or translocations.  A different family of algorithms are used to identify structural variants generally making use of pair end reads or split reads, but their accuracy so far has been low compared to SNV calling algorithm \cite{o2013low}.

\textbf{Caveats}
	\begin{enumerate}[label=\roman*]
		\item Using current technologies and computational methods for variant calling, detection accuracy varies significantly for different variant types. SNV are by far the most accurately detected. Insertions and deletions, collectively referred as InDels, can be detected less efficiently depending on their sizes. Small InDels consisting of ten bases or less are easier to detect than large InDels consisting of 200 bases or more. The reason being that the most commonly used sequencers reads DNA in stretches roughly 200 bases long. Due to this technological limitations, detection is less reliable for more complex variant types.
	\end{enumerate}

%---
\section{Functional annotations of genomic variants \label{sec:funann}}
%---

The development of cost-effective, high-throughput next generation sequencing (NGS) technologies is poised to have a profound impact on our ability to study the effects of individual genetic variants on the pathogenesis and progression of both monogenic and common polygenic diseases. As sequencing costs decrease and throughput increases, it has now become possible to quickly identify a large number of sequence polymorphisms (SNVs, indels, structural) using samples from affected and unaffected subjects and investigate these in epidemiologic studies to identify genomic regions where mutations increase disease risk. However, translating this information into biological or clinical insights is challenging as it is often difficult to determine which specific polymorphisms are the main pathogenetic drivers of disease across a population; and more importantly, how they affect the activity of disease-related molecular pathways in tissues and organism a specific patient. In part, this difficulty results from the large number of genetic variants that are observed in individual genomes (the human population is believed to contain approximately 3.5 million polymorphic sites with minor allele frequency above 5\%) combined with the limited ability of computational approaches to distinguish variants with no impact on genome function (the vast majority) from variants affecting gene function or expression that may be associated with disease risk or drug response (the minority). The development of algorithms for automated variant annotation,which link each variant with information that may help predict its molecular and phenotypic impact, is a critical step towards prioritizing variants that may have a functional impact from those that are harmless or have irrelevant functional effects. In this section we review the key concepts and existing approaches in this important field. In Chapter \ref{ch:snpeff} we introduce an approach to collect relevant information that will help answer questions about genetic variants discovered in next-generation sequencing studies, including: (i) will a given coding variant affect the ability of a protein to carry its functions; (ii) will a given non-coding variant affect the expression or processing of a given gene; and ultimately (iii) will a given coding or non-coding variant have any impact on phenotypes of interest?

Answering these questions is essential for many types of analyses that use large-scale genomics datasets to study quantitative traits and diseases, particularly when only a small number of individuals is studied comprehensively at a genome-wide level. For example, most genome-wide association studies (GWAS) or exome sequencing studies lack the statistical power to identify rare variants or variants with small effects associated with a disease, in part due to the large number of variants assayed. This limitation can be addressed by directing both statistical analysis and subsequent experimental steps to focus on smaller sets of genetic variants that have been prioritized based on external evidence of their putative impact. The common impairment of DNA repair mechanisms and chromatin stability in malignant cells leads to a similar challenge in cancer genomics, where the hundreds or thousands of mutations that distinguish an individual's tumor and germline genomes need to be classified on the basis of their putative phenotypic effects and potential roles in carcinogenesis.

The large number of databases containing potentially helpful information about a given variant make the process of gathering and presenting relevant data challenging, despite excellent tools that already exist to analyze large genomics datasets (including GATK \cite{mckenna2010genome} and Galaxy \cite{goecks2010galaxy}) and visualize the results (such as the UCSC \cite{karolchik2014ucsc} or Ensembl \cite{flicek2012ensembl} genome browsers). Each of these databases uses its own format and is updated asynchronously, which makes it difficult for any analysis to remain up to date. In addition, the lack of comprehensive and computationally efficient models that allow integrative analyses using these resources, makes the task of comprehensive variant annotation overwhelming. By efficiently combining information from tens or hundreds of genome-wide databases, the tools described here are designed to greatly facilitate the process of variant annotation, and make it accessible to groups with limited bioinformatics expertise or resources.

%---
\subsection{Variant types}
%---

Although variant calling is a challenging task and remains an important area of research, many high-quality tools exist for calling SNVs and indels.
We discuss here the problem of annotating the variants identified by some of these tools.
The most common type of variant identified by current technologies and analysis approaches is a single base difference with respect to the reference genome (SNV) followed by multiple base differences (MNP), as well as small insertions and deletions (InDels). Here, we focus on annotating those variants (or combinations of them, called "Mixed" variants), which comprise most of the variants in a typical sequencing experiment. We do not address the annotation or large rearrangements due to the challenges involved in their identification and functional characterization and their relative rarity in the germ line.

\subsection{Types of genetic annotations}

The process of genetic variant annotation consists of the collection, integration, and presentation of experimental and computational evidence that may shed light on the impact of each variant on gene or protein activity and ultimately on disease risk or other phenotypes. Variant annotation has traditionally been divided in two apparently independent but actually interrelated tasks based on the variant's location with respect to known protein-coding genes (see Table 1 for a list of commonly used variant annotations).Coding variant annotation focuses on variants that are located within coding regions of annotated protein-coding genes and attempts to assess their impact on the function of the encoded protein. In contrast,non-coding variant annotation focuses on variants located outside the coding portion of genes (i.e. in intergenic regions, UTRs, introns, or non-protein-coding genes) and aims to assess their potential impact on transcriptional and post-transcriptional gene regulation. These two categories of variant annotations are not mutually exclusive, as variants located within exons can often have an impact on the gene transcript's processing (splicing). In addition, some transcripts can have both protein-coding and non-coding functions. Despite the intermingling of the notion of coding and non-coding variants, we will consider each type of annotation separately as assessing their impact requires different sources of data and algorithms.

The ultimate goal of variant annotation is to predict the impact of a sequence variant, although this is an ill-defined term. One the one hand, one may be interested in the molecular impact of a variant on the activity of a protein. On the other, others may be interested in a variant's impact on much higher-level phenotypes such as disease risk. Mutations that are predicted to completely abrogate a gene's activity are called loss-of-function (LOF) mutations. Those that are predicted to have less severe consequences are called moderate or low impact mutations. In practice, a variant will be predicted to cause LOF if it has two properties: (i) its molecular impact is reliably predictable by existing computational approaches (e.g. gain of stop-codon); and (ii) its functional impact, reflected by altered protein activity or expression levels, is expected to be large. Many types of variants, including most non-coding variants, may have a large functional impact but lack predictability, and as a consequence are typically not predicted to be LOF variants.

\subsection{Coding variant annotation}

Coding variants occur in translated exons. When a reliable gene annotation is available, their main impact can be classified by determining their effect on the translated amino acid sequence (if any). A synonymous coding variant (also called silent) does not change the sequence of amino acids encoded by the gene, although it may impact aspects of post-transcriptional regulation such as splicing and translation efficiency and can affect the total protein activity through changes in the amount of translated protein that is made in the cell. In contrast, a non-synonymous coding variant changes one or more amino acids encoded by the gene and can directly alter the protein's activity, localization or stability. Non-synonymous variants include missense substitutions that change a single amino acid, nonsense substitutions that lead to the gain of a stop codon, frame-preserving indels that insert or delete one or more amino acids, and frame-shifting indels that may completely alter the protein's amino acid sequence. Primary annotation and assessment of impact, determines whether a variant falls in any of these categories.

\textbf{Caveats}
	\begin{enumerate}[label=\roman*]
	
	\item \textit{Gene misannotation.} Genomic variants that have a significant effect on a protein's expression or function represent a very small fraction of all variants. Assembly and gene annotation errors or genomic oddities that break classical computational models are also rare, but lead to false positives. This implies that one is likely to find a non-negligible fraction of false-positive high-impact variants among the list of what appear to be the strongest candidates for variants with severe effects. Tools such as SnpEff can anticipate some of the most common causes of misannotation, but the number and diversity of the type of events that can lead to false-positives makes the task very challenging. As a consequence, one should always manually inspect the top candidates to ensure that they have been assigned to the correct genes and transcripts.
	
	\item \textit{Gene isoforms.} In higher eukaryotes, most genes have more than one transcript (or isoform), due to alternative promoters, splicing, or polyadenylation sites. For example, a human gene has an average of 8.8 annotated messenger RNA (mRNA) isoforms and some genes are believed to have over 4,000 isoforms resulting from complex splicing programs. For these genes, a variant may be coding with respect to one mRNA isoform and non-coding with respect to another. There are two frequent approaches to address this situation: (i) annotate a variant using the most severe functional effect predicted for at least one mRNA isoform; or (ii) use only a single canonical transcript per gene to perform primary annotation. 
	
	\item \textit{Variant calling for indels.} Variant annotation relies on knowing the exact genomic coordinates of the variant: this is rarely a problem for isolated SNVs; however, insertions and deletions often cannot be located unambiguously. Consider for example the variant $AA \rightarrow A$. This mutation results in the loss of a single base, but was it the first or second A that was deleted? From the standpoint of the cell, this question is irrelevant and deletion of any A will have the same effect. In contrast, from the standpoint of most variant annotation software, deleting the first A is different from deleting the second. Consider the scenario of a previously annotated transcript where the first A is part of the 5' UTR and the second is the first base of a start codon. If the missing base is assigned to the leftmost position in the motif (as is the current convention), the deletion would be annotated as a low impact 5'UTR variant. However, assigning it to the rightmost A would make it appear (incorrectly) to be a high-impact start-codon deletion. Similar issues may arise when considering conservation scores or transcription factor binding site (TFBS) predictions.
	
		\end{enumerate}

\subsection{Loss of function variants}

True LOF variants are difficult to predict computationally, but specific types of genetic changes will frequently lead to severely impaired protein activity. These include (i) stop-gains (nonsense mutations) and start-loss; (ii) indels causing frameshifts; (iii) large deletions that remove either the first exon or at least 50\% of the protein coding sequence; and (iv) loss of splice acceptor or donor sites that alter the protein-coding sequence. Variants that introduce premature in-frame stop codons (nonsense mutations and most frameshift indels) are expected to abolish protein function, unless the variant is very near the C-terminus of the coding region \cite{yamaguchi2008distribution} (effectively, downstream of the last functional domain in the protein). Such mutations may have severe consequences in affected cells, tissues or organism, as is seen for mutations that cause monogenic diseases \cite{scheper2007translation}. In addition, a new stop codon that lies upstream of the last exon will likely trigger nonsense mediated decay (NMD), a process that degrades mRNA before protein synthesis occurs \cite{nagy1998rule}. NMD predictions are not exact and many factors can affect mRNA degradation, including the variant's distance from the last exon-exon junction or poly-A tail, and the possibility that transcription may re-initiate downstream of the LOF variant \cite{brogna2009nonsense}.

A variant that leads to the loss of a stop codon, sometimes called read-through mutation, will result in an elongated protein-coding transcript that terminates at the next in-frame stop codon. While there are no general models that predict how deleterious this may be, such variants can also result in aberrant folding and degradation of the nascent proteins, leading to activation of cellular stress response pathways, in addition to their direct effects on protein activity and expression levels \cite{scheper2007translation}.

The effect of the loss of a start codon depends on the location of a replacement start codon with respect to the translation start site and reading frame of the native protein. If the new start codon maintains the reading frame, the only consequence may be the loss of a few amino acids in the protein transcript; however, in many cases, the new start codon will not be in-frame, thus producing a frame-shifted protein that is later degraded. In addition, the new start codon may lack an appropriate regulatory context (for example, if there is no Kozak sequence nearby or if it disrupts 5' UTR folding) leading to reduced expression of an N-terminally truncated protein. Consequently, losing a start codon is thought to be highly deleterious in most cases, due to the potential that it may reduce both protein production and activity.

\textbf{Caveats}
	\begin{enumerate}[label=\roman*]
	
	\item \textit{Rare amino acids.} Through a process called translational recoding, a UGA ``Stop" codon located in the appropriate mRNA context (determined by both primary mRNA sequence and secondary structure) may be translated to incorporate a selenocysteine amino acid (Sec / U) \cite{alberts1995molecular}. In humans, it is known to occur 100 codons located in mRNAs whose 3' UTR contains a Selenocysteine insertion sequence element (SECIS). Since the translation machinery goes so far to encode these special rare amino acids, the expectation is that mutations at those sites would be highly deleterious. This is supported by evidence that reduced efficiency of selenocysteine incorporation is linked to severe clinical outcomes, such as early onset myopathy  \cite{maiti2009mutation} and progressive cerebral atrophy  \cite{agamy2010mutations}.
	
	\item \textit{False-positives in LOF predictions.} Variants predicted to result in a LOF sometimes actually produce proteins that are partially functional  \cite{macarthur2012systematic}. In fact, an apparently healthy individual is typically heterozygous for around 100 predicted LOF variants, and homozygous for roughly 10, but many of those are unlikely to completely abolish the protein function. Indeed, these variants are enriched toward the 3' end of the gene, where they are likely to be less deleterious. 
	
	\end{enumerate}

\subsection{Variants with low or moderate impact}

Compared to the high impact variants discussed above, where extensive prior biological evidence strongly suggests that a specific type of variant will severely impair protein activity, there are few guidelines that can reliably predict how the majority of nonsynonymous (missense) variants will alter protein function or expression. As a result, the primary annotation performed by SnpEff and most related software packages will broadly categorize missense substitutions and their accompanying amino acid changes (e.g. $K154 \rightarrow L154$) as moderate impact variants. Short indels whose length is a multiple of three are treated similarly, unless they introduce a stop codon, as their effect will usually be localized.

Once missense and frame-preserving indel variants are identified, a more detailed estimation of their impact on protein function can be performed using heuristic and statistical models. The most common approaches are based on conservation, either amongst orthologous or homologous proteins, or protein domains, sometimes adding information of the physio-chemical properties of the reference and variant amino acids (e.g. differences in side chain charge, hydrophobicity, or size). The SIFT algorithm \cite{kumar2009predicting} assesses the degree of selection against specific amino acid changes at a given position of a protein sequence by analyzing the substitution process at that site throughout a collection of predicted homologous proteins identified by PSI-BLAST \cite{altschul1997gapped}. Based on this multiple sequence alignment and the highly conserved regions it contains, SIFT calculates a normalized probability of amino acid replacement (called the SIFT score), which estimates the mutation's effect on protein function. Polyphen \cite{adzhubei2010method}, another commonly used tool, takes the process one step further by searching UniProtKB/Swiss-Prot \cite{uniprot2013update} and the DSSP database of secondary structure assignments \cite{joosten2011series} to determine if the variant is located in a known active site in the protein. In contrast to other methods that categorize each variant individually, VAAST \cite{rope2011using}, a commercially available package, computes scores for groups of variants located within a given gene and ``collapses" them into a single category, a concept similar to burden testing performed for rare variants identified in exome sequencing studies. For human proteins, SnpEff makes use of the Database for Nonsynonymous SNVs' Functional Predictions \cite{liu2011dbnsfp} (dbNSFP), which collects scores produced by several impact assessment algorithms in a single database. Individually, impact assessment methods usually have an estimated accuracy of 60\% to 80\% when compared to manually curated databases of human variants, but predictions from several algorithms can be combined to provide a stringent, but more accurate estimate of impact \cite{choi2012predicting}.

In most cases these algorithms apply best to SNVs since these are common in populations and there is more genomic sequence and experimental data available to refine the statistical methods. However, some recently developed algorithms are capable of assessing variants other than SNVs, including PROVEAN \cite{choi2012predicting}, which extends SIFT to assess the functional impact of indels.

\textbf{Caveats}
	\begin{enumerate}[label=\roman*]
	
	\item \textit{Imprecise models of protein function.} Accurate impact assessment of coding variants remains an open problem and most computational predictions are riddled with both false positives and false negatives. While both missense variants and frame-preserving indels are broadly cataloged as having moderate effects, this is mostly due to lack of a comprehensive model and the extremely complex computations that would be required for an in-depth analysis (such as protein structure predictions). In these cases, proteomic information can be revealing. SnpEff adds annotations from curated proteomic databases, such as NextProt  \cite{lane2012nextprot}, which can help to elucidate if a mutation alters a critical protein amino acid or domain (such as amino acids that are post-translationally modified as part of a signaling cascade or that are form the active site of an enzyme) resulting in a protein may no longer function.
	
	\item \textit{Gain of deleterious function.} Computational variant annotation may eventually be able to fairly accurately predict the molecular impact of a variant in terms of the degree to which it translates in a loss of function for the encoded protein. However, gains of function, including the acquired ability to interact with new partners and disrupt their function, remain vastly more difficult to tackle, although several such variants have been linked to disease \cite{whitcomb1996hereditary}.
	
	\item \textit{Unanticipated effects of synonymous variants.} In most cases, synonymous variants are regarded as non-deleterious (or low impact); however, one needs to seriously consider the possibility that they may have greater functional effects by altering mRNA splicing  \cite{coulombe2009fine} or secondary structure  \cite{sabarinathan2013rnasnp}. Synonymous SNVs may also alter translation efficiency, by changing a frequently used to a rarely used codon and have been linked to changes in protein expression  \cite{sauna2011understanding}.
	
	\end{enumerate}

\subsection{Non-coding variant annotation}

Although coding variants represent less than 2\% of variants in the human genome, they make up the vast majority of confirmed disease-related variants that have been validated at a functional level. This may result from ascertainment bias (since variants in coding regions are straightforward to discover and characterize at a basic level and many studies have largely ignored non-coding variants); or may be explained by the increased complexity of computational approaches and lab assays required to predict and validate the impact of non-coding variants; or by their potentially more subtle impact on gene expression or cell function. Nonetheless, in a compendium of current GWAS studies, roughly 40\% of the variants are intergenic and 30\% intronic. Functional studies of these variants are increasingly emphasizing the importance of non-coding genetic variation at risk loci for complex genetic diseases and traits \cite{hindorff2009potential}.

Functional non-coding regions of the genome encompass a wide variety of regulatory elements contained in DNA and RNA molecules that are involved in transcriptional and post-transcriptional regulation. Cis-regulatory elements include (i) binding sites for DNA-binding proteins such as transcription factors and chromatin remodelers; (ii) binding sites for RNA-binding proteins involved in splicing, mRNA localization, or translational regulation; (iii) micro RNA (miRNA) target sites; and (iv) long non-coding RNA (lncRNA) targets on DNA, RNA and proteins. Non-coding transcripts include well-characterized regulatory RNAs (e.g. miRNA, snoRNA, snRNA, piRNA and lncRNAs) as well as RNAs involved directly in protein synthesis (e.g. tRNA and rRNA).  The annotation and impact assessment of non-coding variants presents a significant challenge for several reasons: (i) reliable technologies to study transcriptional regulatory regions on a genome-wide basis are only just reaching maturity and provide limited resolution of binding sites for individual transcription factors and regulatory RNA molecules; (ii) non-coding functional regions of most genomes remain incompletely mapped as they vary widely among different cell types and cell states (for example, in diseased and healthy tissues); (iii) non-coding regulatory elements often are part of complex transcriptional programs that are time-dependent, contain many redundant linkages or reciprocal connections between genes and respond to a wide range of intra- and extracellular signals; and (iv) genomic regulatory elements rarely have a strict consensus sequence (for example, compare the position weight matrices used to identify transcription factor or miRNA binding sites with the amino acid triplet code) making the effect of a mutation on gene regulatory programs difficult to predict. As a result, high-quality annotation of non-coding variants relies more heavily on experimental data than is the case for coding variants: since many of these experimental techniques did not study the effects of SNVs on gene regulatory programs, they can only be used to annotate variants and not to predict their effects on gene transcription. In the few cases where the effects of SNVs have been studied (for example, the effects of SNVs that are common in a population and located in genetic loci associated with complex diseases), experimental approaches provide highly accurate functional assessment at a cost of reduced coverage compared to computational approaches.

Large-scale projects such as ENCODE \cite{encode2012integrated} and modENCODE \cite{celniker2009unlocking} have made major steps toward mapping gene transcription and transcriptional regulatory regions in many tissues and cell types, but similar studies in diseased tissues remain at an early stage (for example, the growing collection of disease-related epigenomes from the Epigenome Roadmap \cite{bernstein2010nih}). The base-by-base resolution and number of cell states studied for different types of regulatory elements and non-coding transcripts varies widely among datasets; in part due to the lack of sensitive, comprehensive and high-resolution technologies to study the different molecular species and modes of interaction that can be altered by non-coding variants. Efficient technologies for genome-wide, high-throughput mapping of binding sites for RNA-binding proteins (PAR-CLiP \cite{ascano2012identification}), miRNAs (PAR-CLiP \cite{hafner2012genome} and CLASH \cite{helwak2013mapping}) are starting to be applied on a broad scale as are protocols to map transcription factor binding sites (TFBS) which can improve resolution to a single base (Chip-exo \cite{rhee2012chip}). However, in most cases, DNA and RNA binding sites are only imprecisely located within Chip-Seq peaks that span genomic regions hundreds of base pairs in length, with computational approaches being used to pinpoint the bases most likely mediating the interaction. In the absence of more precise localization data, de novo computational prediction of binding sites for DNA and RNA binding proteins remains insufficiently accurate to be of much use in annotating single noncoding variants.

This limitation is particularly critical for functional predictions of putative target sites for microRNAs and other regulatory RNA species. MicroRNAs are short RNA molecules that regulate gene expression post-transcriptionally by binding the messenger RNA of a gene through complementary, usually in the 3' region of the transcript, which leads to mRNA degradation or inhibits translation. Sequence variants that cause the loss or gain of a miRNA target site would lead to dysregulation of the gene, with likely deleterious effects. Although miRNAs are relatively well documented in most model organisms including human, their binding sites are only starting to be mapped experimentally, and computational predictions has very low specificity. Meaningful information regarding the possible role of a variant in disrupting a miRNA target site is starting to emerge \cite{liu2012mirsnp}, although variants that create new miRNA binding sites remain under the radar.

Even if the position of a functional element could be perfectly determined, predicting a variant's impact on chromatin conformation, promoter activity, gene expression, or transcript processing remains challenging. For transcription factors, this involves predicting whether the protein will still be able to recognize its mutated site (and with what affinity), as well as predicting the impact of these changes on gene expression levels. The latter is particularly hard to predict as a result of interactions, competition, and redundancy contained in regulatory networks of transcription factors or RNA binding proteins. As a consequence, computational prediction of the functional impact of non-coding variants remains a very active area of research and there is no broad consensus on the best methodology to use \cite{ward2012interpreting}. One significant exception is the identification of variants affecting canonical splice sites, defined as two bases on the 3' end on the intron (splice site acceptor) and 5' end of the intron (splice site donor). Variants that affect canonical splice sites are easily detected and typically lead to abnormal mRNA processing, involving exon loss or extension that leads to loss of function of the encoded protein.

\subsection{Impact assessment of non-coding variants}

Two broad classes of publicly available genome-wide datasets are commonly combined to assess the functional impact of non-coding genetic variants: (i) computational predictions of sequence conservation and sites involved in molecular interactions such as transcription factor and RBP binding, as well as miRNA-mRNA target interactions; and (ii) experimental genome-wide localization assays for DNA binding proteins, histone modifications, and chromatin accessibility.

\paragraph{Computational sources of evidence:} Interspecies sequence conservation plays a key role in scoring and prioritizing non-coding variants. This is based on the assumption is that sites or regions that have been more conserved across species than expected under a neutral model of evolution are likely to be functional; suggesting that mutations contained in them are likely to be deleterious. In the absence of strong experimental data, sequence conservation measures calculated from whole genome multiple alignments, (for example using PhastCons  \cite{siepel2005evolutionarily}, SciPhy  \cite{garber2009identifying}, PhyloP  \cite{pollard2010detection} , and GERP  \cite{davydov2010identifying}), have been developed to provide a generic indicator of function for non-coding variants. Although high conservation scores generally mean that a genomic region may be functional, the converse is not true and many experimentally proven functional noncoding regions show only modest sequence conservation (for example due to binding site turnover events). Finally, some regulatory regions (e.g. specific elements regulating immune response  \cite{raj2013common}) are under positive selection and may thus show less conservation than surrounding neutral regions. 

In human, genome-wide computational predictions of transcription factor binding sites based on matching to publicly available position weight matrices are available from variety of sources, including Ensembl \cite{flicek2012ensembl} and Jaspar  \cite{bryne2008jaspar}.  Because of the low information content of most binding affinity profiles, the specificity of the predictions is generally very low. Related approaches exist to predict splicing regulatory regions  \cite{fairbrother2002predictive} and miRNA target sites \cite{ziebarth2011polymirts}, some of which are precomputed for whole genomes and available from the UCSC or Ensembl genome browsers. Recent efforts to determine RNA-binding protein sequence affinities can also be used to identify putative binding sites for these proteins in mRNA  \cite{ray2013compendium}.

\paragraph{Experimental sources of evidence:} To investigate the potential impact of variants on transcriptional regulation, many published experimental data sets produced by large-scale projects such as ENCODE \cite{encode2012integrated}, modENCODE \cite{celniker2009unlocking} and Roadmap Epigenomics \cite{bernstein2010nih}, can be used directly by annotation packages. These include: (i) ChIP-seq or ChIP-exo experiments that identify TFBSs on a genome-wide basis; (ii) DNAseI hypersensitivity or Formaldehyde-Assisted Isolation of Regulatory Elements (FAIRE) assays that identify regions with open chromatin; and (iii) ChIP-seq studies to identify the presence of specific promoter or enhancer-associated histone post-translational modifications, which can be combined to identify active, poised, and inactive enhancers and promoters \cite{ray2013compendium}. Most of these data sets are easily available through Galaxy \cite{goecks2010galaxy} (as tracks from the UCSC Genome Browser) or through SnpEff (as downloadable pre-computed datasets). In parallel with the types of studies described above, expression quantitative trait loci (eQTLs) represent an agnostic way to map putative regulatory regions. An increasing number of such loci are available through the GTEX database  \cite{lonsdale2013genotype}. Experimental data that may support assessment of the impact of variants on post-transcriptional regulation remain sparser, although databases such as doRiNa  \cite{anders2011dorina} or starBase  \cite{yang2011starbase} contain genome-wide datasets obtained by CLIP-Seq and degradome sequencing. To our knowledge, these data have yet to be used in the context of variant annotation studies.

\paragraph{Combining sources of evidence:} Despite the variety of computational and experimental sources of evidence available, impact assessment for non-coding variants remains relatively crude, due to the fact that biological models of gene regulation remain fairly simple. Nonetheless, significant steps forward have been made recently, and two web-based tools, HaploReg  \cite{ward2012haploreg} and RegulomeDb  \cite{boyle2012annotation}, perform SNV and indel impact assessment for variants from dbSNV on the basis of a broad body of computational and experimental evidence. Both use pre-computed scores for variants from dbSnp and therefore cannot be used for rare variants, but they are extremely valuable for exploration by associating the variant of interest with a variant in dbSnp via linkage disequilibrium. 

\textbf{Caveats}
	\begin{enumerate}[label=\roman*]
	
	\item \textit{Sparseness of functional sites within ChIP-seq peaks.} Even if a noncoding variant is located in a region that contains a ChIP-seq peak for a given TF and has all the hallmark signatures of regulatory chromatin, the likelihood that it is deleterious remains low, because most DNA bases contained within a peak are non-functional. 
	
	\item \textit{Gain of function mutations.} While this section has focused on variants causing the loss of a functional regulatory element, genetic variants may also create new or more effective transcription factor binding sites. These are substantially harder to detect as they can occur in regions that show no evidence of function in individuals possessing the reference allele, and show little conservation across species. Furthermore, computational methods to predict gain of affinity for a given TF caused by a variant have insufficient specificity to be of much use on their own. 
	
	\end{enumerate}

%---
\subsection{Clinical effect of variants}
%---

One of the most revealing types of annotation of both coding and noncoding variants reports whether the variant has previously been implicated in a phenotype or disease. Although such information is available for only a small minority of all deleterious variants, their number is growing and should be the first type of annotation one seeks out. Clinical annotations, until recently, have been scattered in a large number of specialized databases of medical conditions with a genetic basis, including the comprehensive, manually curated collection of genetic loci, variants and phenotypes in the Online Mendelian Inheritance in Man database (OMIM, www.omim.org); web pages containing detailed clinical and genetic information about uncommon disorders in the Swedish National Board of Health and Welfare Database for Rare Diseases (www.socialstyrelsen.se/rarediseases) and the peer-reviewed NIH GeneReviews collection \cite{bryne2008jaspar} (www.ncbi.nlm.nih.gov/books/NBK1116); and a curated collection of over 140,000 mutations associated with common and rare genetic disorders in the commercial Human Gene Mutation Database (HGMD, www.hgmd.org/). In most cases, these datasets do not use standardized data collection or reporting formats; are designed to primarily provide information to patients and health professionals through a web interface; and rely on heterogeneous criteria to describe disease phenotypes and clinical outcomes; pathological and other clinical laboratory data; as well as the genetic and biologic experiments that have been used to demonstrate disease mechanisms at a molecular or cellular level. These shortcomings are being addressed by initiatives that provide centralized, evidence-based, comprehensive collections of known relationships between human genetic variants and their phenotype that are suitable for computational analysis, such as the NIH effort to aggregate records from OMIM, GeneReviews and locus-specific databases in ClinVar (www.ncbi.nlm.nih.gov/clinvar). 

Another important application of variant detection and annotation is in the study of cancer genomes, which is occurring increasingly in clinical settings to support treatment decisions for advanced tumors. Annotation of variants detected in tumor sequences can be analyzed for clinical cohorts, using similar techniques as other complex traits, as well as for individual patients, using techniques to identify differences between somatic (tumor) and germline (healthy) tissues. In the latter case, one looks for cancer-associated mutations that distinguish the somatic genome of cancer cells of an individual from the germline genome in order to find the driving mutations that pinpoint the specific mechanisms underlying tumorigenesis or metastasis. Ideally, these mutations can be used to select a treatment for the patient, establish prognosis, or to identify causative mutations that have led to the cancer's progression. In such a setting, given that sequence differences between the cancer and germline genomes are of greater interest than the background genetic changes between the germline and a reference genome, variant calling is performed using specialized algorithms, such as MuTect  \cite{cibulskis2013sensitive} and SomaticSniper  \cite{larson2012somaticsniper}.

\textbf{Caveats}
	\begin{enumerate}[label=\roman*]

	\item \textit{Annotation accuracy.} Biological knowledge, as well as molecular and phenotypic evidence supports the identification of certain groups of high impact variants based on simple criteria (such as premature stops, frameshifts, start lost and rare amino acid mutations); however, it is often hard to predict whether non-synonymous variants will have equally large effects on an organism's health. Even when the accepted ``rules of thumb" used in the primary annotation indicate that protein function is impaired, we should consider that these predictions may be based on a small number of model genes and will require appropriate wet-lab validation or confirmatory studies in cohorts. In addition, as more human genomes are sequenced, it is likely that some genetic variants that have been linked to Mendelian diseases will be found in healthy individuals  \cite{riggs2013towards}; and in many cases, may not actually be disease-causing mutations  \cite{bell2011carrier}.
	
	\end{enumerate}

\subsection{Data structures and computational efficiency}

Most computational pipelines for genomic variant annotation and primary impact assessment are relatively efficient and can annotate variants obtained from large resequencing projects involving thousands of samples within a few minutes or hours even using a moderately powered laptop. This is typically achieved through two key optimizations: (i) creation of reference annotation databases and (ii) implementation of efficient search algorithms. Reference database creation refers to the process of creating and storing precomputed genomic data from the reference genome, which can be searched quickly to extract information relevant to each variant. This process needs to be performed only once per reference genome and most annotation tools have pre-computed databases for many organisms available for users to download.

Since these databases are typically quite large, efficient search algorithms are used together with appropriate data structures to optimize the search process. In ANNOVAR \cite{wang2010annovar}, each chromosome is subdivided in a set of intervals of size $k$ and genomic features for a given chromosome are stored in a hash table of size $L/k$, where $L$ is the length of the chromosome. Another approach, used by SnpEff, is to use an ``interval forest", which is a hash of interval trees  \cite{cormen2001introduction} indexed by chromosome. Querying an interval tree requires $O[log(n) + m]$ time, where $n$ is the number of features in the tree and m is the number of features in the result. 

\subsection{Conclusions}

In Chapter \ref{ch:snpeff} we introduce SnpEff \& SnpSift, two approaches we designed for efficiently performing functional annotations of sequencing variants. These packages allow annotating, prioritizing, filtering and manipulating variant annotations as well as combining several public or custom-created databases. It should be noted SnpEff was one of the first annotation packages and has become one of the most widely used annotation software in both research and clinical environments. 

%---
\section{Genome wide association studies}
%---

A genome wide association study aims at identifying genetic variants associated to a particular phenotype. First, the genomes (or exome, depending on the study design) of affected individuals (cases) and healthy individuals (controls) need to be sequenced, variants called, annotated and filtered. Then, the goal is to find variants that exhibit some statistical association with the trait or phenotype of interest, which could be a disease status (e.g. diabetic vs healthy), a biomedical measurement (e.g. cholesterol level), or any measurable characteristic (e.g. height). Since the genome is so large, patterns of mutations that suggest correlation may be encountered by chance, so we need to establish statistical significance in order to distinguish true association from spurious ones. Like most studies, we will focus our discussion on SNVs, but most methods can be extended to other genomic variants.

\subsection{Single variant tests and models \label{sec:single}}

Consider a simple situation where there is only one variant in the whole genome for the cohort we are analysing. Since each individual has two sets of chromosomes, the variant can be present in one, both, or neither chromosomes, so the number of times a non-reference allele is present in an individual, is $ N_{nr} = \{0, 1,2\}$.

When the trait of interest is binary (e.g healthy vs disease), a cohort can be divided into cases and controls and we can build a 3 by 2 contingency table:

\[
\begin{array}{l|c|c|c|}
	& Homozygous Reference & Heterozygous & Homozygous non-reference\\
	& (N_{variant} = 0) & (N_{nr} = 1) & (N_{nr} = 2) \\
    \hline 
    Cases & N_{ca,ref} & N_{ca,het} & N_{ca,hom} \\ 
    \hline 
    Controls & N_{co,ref} & N_{co,het} & N_{co,hom} \\
    \hline 
\end{array} 
\]

Further assumptions about how many variants are required to increase disease risk can reduce this $3 \times 2$ table to a $2 \times 2$ table. In the ``dominant model'', the effect of a mutated gene dominates over the healthy one, so one variant is enough to increase risk. The opposite, called ``recessive model", is when both chromosomes have to be mutated in order to increase risk \cite{balding2006tutorial, clarke2011basic}. In these models, we can count how many cases and controls have at least one variant (dominant model) or two variants (recessive model). This simplifies the previous table, yielding a $2 \times 2$ contingency table, than can be tested using either a $\chi^2$ test or a Fisher exact test \cite{balding2006tutorial}.

Two other commonly used models, are the ``multiplicative" and the ``additive" models \cite{balding2006tutorial,clarke2011basic}. In these models, a disease risk is assumed to be multiplied (or increased) by a factor $\gamma$ with every variant present. We cannot simplify the contingency table, so we assess significance using a Cochran-Armitrage test \cite{clarke2011basic}.

\subsection{Multiple variant tests}

In a real case scenario there are thousands or millions of variants. We can extend the concept shown in the previous section by performing individual tests for each variant present in the cohort. Multiple testing can be addressed either by performing a correction, such as False Discovery Rate \cite{balding2006tutorial, clarke2011basic}, or using a stricter genome wide significance level. There are $3 \times 10^9$ bases in the genome, but taking into account the correlation between nearby variants (linkage disequilibrium), the genome wide significance level is generally accepted to be $p_{value} \leq 10^{-8}$.

In order to check if the null hypothesis of a significance tests is adequate, a QQ-plot is used (i.e. plotting the $y = -log(p_{value})$ vs $x = -log[ rank(p_{value}) / (N+1) ]$, where $N$ is the total number of variants). Adherence of the p-values to a 45 degree line on most of the range implies few systematic sources of association \cite{balding2006tutorial, clarke2011basic}. If the p-values have a higher slope than the $y=x$ line, there might be ``inflation", possibly due to co-factors, such as population structure (see section \ref{sec:popStruct}). If the inflation is not too high (e.g. less than $5\%$), this bias can be corrected by shifting the p-values towards the 45 degree slope. More sophisticated methods are explained in section \ref{sec:popStruct}.

\subsection{Continuous traits and correcting for co-factors \label{sec:cofactors}}

Methods described so far are suitable for binary ``traits" or ``phenotypes". Statistical methods that link genetic information to traits can also be used on continuous or ``quantitative" traits (e.g. weight, height, cholesterol level, etc.). A linear regression can be used assuming the traits are approximately normally distributed \cite{balding2006tutorial, clarke2011basic}. A significance test ($p_{value}$) for linear models can be calculated using an $F$ statistic, but more sophisticated methods are also available \cite{balding2006tutorial, clarke2011basic}.

Using linear models, it is easy to include known co-factors to correct for biases or inflation. For instance, if it is known that a risk increases with age or that males are more susceptible than females, age and sex can be added to the linear equation in order to correct for these effects \cite{balding2006tutorial,clarke2011basic}. In a similar manner, we can add co-factors to binary traits using logistic regression.

\subsection{Population structure \label{sec:popStruct}}

It is widely accepted that humans started in Africa and migrated to Europe, then to Asia and later to America \cite{hartl1997principles}. Out of an initial population, a few individuals migrate and colonize a new territory. This implies that the genetic variety of the new colony is significantly reduced, compared to the previous population, since the genetic pool is only a small ``founder population". The ``Out of Africa" hypothesis implies that each new migration produced a reduction in genetic variety, also known as a ``population bottleneck'' \cite{hartl1997principles}.

As we previously mentioned, each individual inherits two chromosome sets, a maternal and a paternal one. Through recombination a chromosome is formed by a crossover combining maternal and paternal chromosomes and then passed down, thus the offspring has two sets of chromosomes (one from each parent) that on average have half chromosome from each grandparent. This breaking and shuffling of chromosomes every generation, increases genetic diversity. Nevertheless if variants are located nearby in the chromosome, the chances that they are broken apart by recombination event are smaller than if they are further away from each other. This produces a correlation of close variants or ``linkage disequilibrium" (LD). Nearby highly correlated variants are said to be in the same ``LD-block" \cite{hartl1997principles}. If a population has low genetic variety, the LD-blocks are large. So African population has more variety (smaller LD-blocks) and conversely, European, Asian and Amerindian populations have less variety (larger LD-blocks) \cite{hartl1997principles}.

\subsection{Population as confounding variable }

Imagine that we have a cohort of individuals drawn from two populations ($P_A$ and $P_B$) and that individuals in $P_A$ have much higher risk of diabetes than individuals from $P_B$. Now imagine that individuals from $P_A$ have a variant $v_A$ more often, but $v_A$ is actually neutral and has no health effects whatsoever. If we do not take into account population factors, our study would conclude that $v_A$ is the cause of diabetes, just because we see $v_A$ more often in affected individuals. In this case it is clear that population structure is a confounding variable. We could avoid this problem by analyzing each population separately \cite{patterson2006population}, but this would cause a loss of statistical power since we have fewer samples.

A population that is a mixture of two or more population is known as an ``admixed population''. For instance the ``African-American'' population is a mixture of, roughly, $80\%$ African and $20\%$ European genomes \cite{hartl1997principles,balding2006tutorial}. This means that analyzing a cohort of African-American individuals, we would get population structure as a confounding variable because of population admixture \cite{hartl1997principles}. Obviously, in this case we cannot analyze each population separately, because each individual in the sample is a mixture of two populations.

The admixed population problem can be studied by performing a correction using the eigen-structure of the sample covariance matrix \cite{patterson2006population}. Samples can be arranged as a matrix $C$ where each row is a sample and each column represents a position in the genome where there is a variant. The numbers $C_{i,j}$ in the matrix indicate the number of non-reference alleles in a sample (row) at a genomic position (column $j$). Since the allele can be present in zero, one, or two chromosomes in each individual, the possible values for $C_{i,j}$ are $\{0, 1, 2\}$. The covariance matrix is calculated as $M= \hat{C}^T . \hat{C}$, where $\hat{C}$ is the matrix $C$ corrected to have zero mean columns. Usually, the first two to ten principal components of $M$ are used as factors in linear models (see section \ref{sec:cofactors}) to correct for population structure \cite{patterson2006population}.

Whether a cohort has any population structure and needs correction or not can be tested using two methods: a) plotting the projections of the first two principal components and empirically observing the number of clusters in the chart, or b) using a statistic of the eigenvalues of $M$ based on Tracy-Widom's distribution \cite{patterson2006population}.

\subsection{Common and Rare variants\label{sec:comonrare}}

The ``allele frequency" (AF) is defined as the frequency a variant appears in a population. Variants are usually categorized according to AF into three groups: i) Common variants ($AF \geq 5\%$), ``low frequency" ($1\% < AF < 5\%$), and iii) ``rare variants" ($AF < 1\%$). Common variants originated earlier in the population while rare variants are either relatively recent or selected against.

There are three main models for disease susceptibility  \cite{hartl1997principles, gibson2012rare}:i) the Common-Disease-Common-Variant hypothesis (CDCV) assumes that if disease is common, it must be caused by a common variant; ii) the ``infinitesimal hypothesis" proposes that there are many common variants each having small risk effects; and iii) the Common-Disease-Rare-Variant hypothesis proposes that there exists many rare variants, each one having large risk effects.

\subsection{Rare variants test}

The ``rare variant model'' assumes that multiple rare variants have large effects on a trait. The problem is that, since these variants are rare, huge sample sizes are required for tests to identify statistically significant associations. To overcome this problem, methods known as ``burden tests" collapse groups of rare variants that are hypothesised to have  similar effect and perform statistical significance tests on the group \cite{li2008methods}. An example of collapsing technique is to count the number or rare variant in a given window and apply a Fisher exact test, as shown in section \ref{sec:single}. A limitation of some burden tests is that they implicitly assume that all rare variants have the same direction of effect, although in reality they might have no effect, be deleterious, or protective \cite{li2008methods,wu2011rare}.

Several techniques allow weighting rare variants by collapsing them using a kernel matrix. This allows to incorporate other information, such as allele frequency and functional annotations. It can be shown that the statistic induced by kernel weighting functions follows a mixture of $\chi^2$ distributions and there is an efficient way to approximate it \cite{li2008methods,wu2011rare}, avoiding computationally expensive permutations tests.

%---
\section{Epistasis \label{sec:epi}}
%---

In this section we introduced the basic concepts and methodologies used in GWAS. Although fairly mature, there is still heavy research and continuous improvement on GWAS statistical methods. Not only it is well known that traditional (i.e. single marker) GWAS methods fail under non-additive models \cite{culverhouse2002perspective}, but also variants so far discovered using these methods do not account for all the expected phenotypic variance attributed to genetic causes (i.e. missing heritability). As other authors pointed out \cite{cordell2009detecting, zuk2012mystery, zuk2014searching}, this might be because we need to look for epistatic variants which are not taken into account using these methods. In the next section, and in Chapter \ref{ch:gwas}, we cover the topic of epistatic GWAS analysis.

\subsection{What is epistasis and why it is important}

%------------------------------------------------------------
% From: Epistasis: too often neglected in complex trait studies? (Nature 2004)
% \cite{carlborg2004epistasis}
%
%		- In the case of QUANTITATIVE TRAITS, epistasis describes the gen- eral situation in which the phenotype of a given genotype cannot be predicted by the sum of its component single-locus effects1 \cite{carlborg2004epistasis}
%		- Extensive work on the control of qualitative genetic variation has highlighted the biological importance of epistasis at a ‘locus-by-locus’ level. On the basis of this work, several classic genotype–phenotype patterns that are caused by epistasis — such as comb type in chickens, coat colour in various animals, the BOMBAY PHENOTYPE in the ABO blood-group system in humans and kernel colour in wheat \cite{carlborg2004epistasis}
%		- HUMAN EPI EXAMPLE: D-allele of the angiotensin I converting enzyme (ACE) gene and the C-allele of the angiotensin II type 1 receptor (AGTR1) gene3. The risk of myocardial infarction is signifi- cantly increased by the ACE D-allele in patients who carry that particular AGTR1 allele. \cite{carlborg2004epistasis}
%		- In the case of quantitative genetic variation, several or many genes of largely unknown function combine with environmental influ- ences to control trait variation. This is the case for many complex traits that are of medical rel- evance in humans or of economic importance in plants and livestock. \cite{carlborg2004epistasis}
%		- The extent to which epistasis is involved in regulating complex traits is not known, and so we cannot assume that epistasis will be found for every trait in every population. \cite{carlborg2004epistasis}
%		- However, we argue that epistasis has been overlooked for too long and that it now needs to be routinely explored in complex trait studies.  \cite{carlborg2004epistasis}
%		- The term ‘epistasis’ was initially used in the context of Mendelian inheritance; environmental effects are relatively unimportant for Mendelian traits, so Ii individuals can be clearly assigned to one of a limited number of classes according to their phenotype. Here, epistasis was used to describe the situation in which the actions of one locus mask the allelic effects of another locus, in the same way that completely dominant alleles mask the effects of the recessive allele at the same locus. \cite{carlborg2004epistasis}
%		- EXAMPL EPI PIG: A clear example of this can be seen  [in Fig A] ￼￼￼￼which the dominant allele (I) at the KIT locus, which confers white-coat colour in the pig, is dominant over all alleles at the MC1R locus (E), which confer a darker coat colour. The effects of the various alleles at the E locus can only be determined in individuals with the recessive genotype ii at the I locus. This example was classically termed ‘dominant epistasis’, which gives a segregation ratio of 12:3:1 for white:black:brown, respectively \cite{carlborg2004epistasis}
%		- Epistatic QTL-mapping studies in model organisms have detected many new interac- tions and have therefore concluded that epis- tasis makes a large contribution to the genetic regulation of complex traits.  \cite{carlborg2004epistasis}

%------------------------------------------------------------
% From: Epistasis dominates the genetic architecture of Drosophila quantitative traits (2012)
% \cite{huang2012epistasis}
%
%		- Epistasis—nonlinear genetic interactions between polymorphic loci—is the genetic basis of canalization and speciation, and epi- static interactions can be used to infer genetic networks affecting quantitative traits. \cite{huang2012epistasis}
%		- DATASET: Here, we compared the genetic architecture of three Drosophila life history traits in the sequenced inbred lines of the Drosophila melanogaster Genetic Reference Panel (DGRP) and a large outbred, advanced intercross population derived from 40 DGRP lines (Flyland)\cite{huang2012epistasis}
%		- Surprisingly, none of the SNPs associated with the traits in Flyland replicated in the DGRP and vice versa. However, the majority of these SNPs partic- ipated in at least one epistatic interaction in the DGRP.\cite{huang2012epistasis}
%		- Our analysis underscores the importance of epistasis as a principal factor that determines variation for quantitative traits and provides a means to uncover genetic networks affecting these traits. \cite{huang2012epistasis}

%------------------------------------------------------------
% From: Epistasis: what it means, what it doesn’t mean, and statistical methods to detect it in humans (2002)
% \cite{cordell2002epistasis}
%
%		- In this review, we provide a historical background to the study of epistatic interaction effects and point out the differences between a number of commonly used definitions of epistasis \cite{cordell2002epistasis}
%		- For complex traits such as diabetes, asthma, hypertension and multiple sclerosis, the search for suscept- ibility loci has, to date, been less successful than for simple Mendelian disorders. This is probably due to complicating factors such as an increased number of contributing loci and susceptibility alleles, incomplete penetrance, and contributing environmental effects \cite{cordell2002epistasis}
%		- The presence of epistasis is a particular cause for concern, since, if the effect of one locus is altered or masked by effects at another locus, power to detect the first locus is likely to be reduced and elucidation of the joint effects at the two loci will be hindered by their interaction.  \cite{cordell2002epistasis}
%
%		- DEFINITION:
%			- The term ‘epistatic’ was first used in 1909 by Bateson (1) to describe a masking effect whereby a variant or allele at one locus (denoted at that time as an ‘allelomorphic pair’) prevents the variant at another locus from manifesting its effect.  \cite{cordell2002epistasis}
%			- This was seen as an extension of the concept of dominance. There are, however, some problems with this definition, particularly when applied to binary traits. In human genetics, the phenotype of interest is often qualitative and usually dichotomous, indicating presence or absence of disease. \cite{cordell2002epistasis}
%			- Mathematical models for the joint action of two or more loci usually focus on the penetrance, the probability of developing disease given genotype. \cite{cordell2002epistasis}
%			- Suppose that a predisposing allele is required at both loci in order to exhibit the trait, i.e. one or more copies of both allele A and allele B are required. Then, when the effects of both loci are considered, we obtain the penetrance table shown in Table 2. In this table, the effect of allele A can only be observed when allele B is also present: without the presence of B, the effect of A is not observable. The effect at locus A would appear to be ‘masked’ by that at locus B. \cite{cordell2002epistasis}
%			-  This leads to a situation that is not precisely analogous to that described by Bateson (1). In Bateson’s (1) definition, it is clear that if factor B is epistatic to factor A, we do not expect factor A to also be epistatic to factor B.  \cite{cordell2002epistasis}
%			- Table 3 is usually assumed to correspond to a situation in which the biological pathways involved in disease influenced by the two loci are at some level separate or independent (5). \cite{cordell2002epistasis}
%
%		- COPY TABLE 1: Table 1. Example of phenotypes (e.g. hair colour) obtained from different genotypes at two loci interacting epistatically, under Bateson’s (1909) definition of epistasis \cite{cordell2002epistasis}
%		- Mathematically, the quantitative genetic concept of epistasis may be represented (10) for two loci by the linear model (COPY EQUATION!). If there is no epistasis then the resulting model (COPY NON-EPI EQUATION) \cite{cordell2002epistasis}

%------------------------------------------------------------
% From: Epistasis - the essential role of gene interactions in the structure and evolution of genetic systems (Nature 2008)
% \cite{phillips2008epistasis}
%
%		- Epistasis, or interactions between genes, has long been recognized as fundamentally important to understanding the structure and function of genetic pathways and the evolutionary dynamics of complex genetic systems. \cite{phillips2008epistasis}
%		- It has been approximately 100 years since William Bateson invented the term ‘epistasis’ to describe the discrepancy between the prediction of segregation ratios based on the action of individual genes and the actual outcome of a dihybrid cross1 \cite{phillips2008epistasis}
%		- The use of the term epistasis has since expanded to describe nearly any set of complex interactions among genetic loci  \cite{phillips2008epistasis}
%		- Over the years geneticists have used epistasis to describe three distinct things: the functional relationship between genes, the genetic ordering of regulatory pathways and the quantitative differences of allele-specific effects \cite{phillips2008epistasis}
%		- Over the years the disparate needs of geneticists have led to a plethora of differently nuanced meanings for the term epistasis, all of which involve gene interactions at various levels  \cite{phillips2008epistasis}
%		- ‘Functional epistasis’ addresses the molecular inter- actions that proteins (and other genetic elements) have with one another, whether these interactions consist of proteins that operate within the same pathway or of proteins that directly complex with one another18 \cite{phillips2008epistasis}
%		- ‘Compositional epistasis’ is a new term that is intended to describe the traditional usage of epistasis as the blocking of one allelic effect by an allele at another locus.  \cite{phillips2008epistasis}
%		- ‘statistical epistasis’ is the usage of epistasis that is attributed to Fisher (BOX 1), in which the average deviation of combinations of alleles at different loci is estimated over all other genotypes present within a population.  \cite{phillips2008epistasis}
%		- EXAMPLE EPI: Coat colour variation in mammals has long been is one of the most fruitful examples in the study of the relationship between genotype and phenotype. ... epistasis arises when the effects of alleles at one locus are blocked by the presence of a specific allele at another locus. For example, a cross between agouti and extension (now called the melanocortin 1 receptor or Mc1r) double heterozygotes (AaEa) yields the non-Mendelian segregation ratio of 9:4:3 (instead of 9:3:3:1) \cite{phillips2008epistasis}
%		- Complex synthetic interactions. : There is no reason to expect all forms of epistasis to be revealed simply by the absence of a gene, which is certainly an extreme approach to perturbing complex systems. For example, Kroll et al.35 devised a method for looking for interactions that are induced after systematically overexpressing genes. Using this approach, sopko et al.36 found that, when overex- pressed in Saccharomyces cerevisiae, about 15% of a set of 5,280 yeast genes induced a growth defect, with most of the overexpression effects not matching the pheno- types of their corresponding deletions.  \cite{phillips2008epistasis}
%		- From mutational studies we know that epistasis in the classical sense is ubiquitous because genes interact in hierarchical sys- tems to generate biological function.  \cite{phillips2008epistasis}
%		- It should be apparent that the global analysis of gene- interaction patterns bears a striking resemblance to what is now called systems biology \cite{phillips2008epistasis}
%		- HUMAN HEALTH EPI EXAMPLES: 
%			- There are numerous cases of epistasis appearing as a statistical feature of association studies of human disease. A few recent examples include coronary artery disease63, diabetes64, bipolar effective disorder65 and autism66. Unfortunately, in only a few cases has the functional basis of these potential interactions been revealed.  \cite{phillips2008epistasis}
%			- One of these cases involves the genetic interactions underly- ing the autoimmune disease multiple sclerosis. Here, Gregersen et al.67 found evidence that natural selection might be maintaining linkage disequilibrium between the histocompatibility loci HLA-DRB5*0101 (DR2a) and HLA-DRB1*1501 (DR2b) (FIG. 3), which are known to be associated with multiple sclerosis; linkage disequilibrium can be generated by strong epistasis among adjacent loci \cite{phillips2008epistasis}
%		- EVOLUTION: 
%			- epistasis can have an important influence on a number of evo- lutionary phenomena, including the genetic divergence between species79, ... the evolution of the structure of genetic systems8 \cite{phillips2008epistasis}
%			- Thus far, these studies81–85 have shown that epistasis can have a strong role in limiting the possible paths that evolution can take, but not in limiting its eventual outcome. \cite{phillips2008epistasis}
%			- linkage can facilitate the maintenance of epistatic interactions (and vice versa)86 and could help to explain how molecular complexity evolves \cite{phillips2008epistasis}
%			- recent analysis of patterns of gene regu- lation suggest that there can be complex patterns of gene regulation in localized genomic regions8 \cite{phillips2008epistasis}

%------------------------------------------------------------
% From: \cite{zuk2012mystery}
% TOPIC: MISSING HERITABILITY
%
%		- missing heritability: overestimation of the denominator happens when epistasis is ignored (phantom) \cite{zuk2012mystery}
%		- phantom heritability could be 62.8\% in Cohn's disease, thus accounting for 80\% of the current missing heritability \cite{zuk2012mystery}
%		- Until recently "The pre- vailing view among human geneticists appears to be that inter- actions play at most a minor  part in explaining missing heritability."	 \cite{zuk2012mystery}
%		- But "[they] show that simple and plausible models can give rise to substantial phantom heritability."	 \cite{zuk2012mystery}
%		- Epistasis is common [SEE MARKED PARAGRAPHS IN LANDER'S PAPER, ADD ORIGINAL REFERENCES] \cite{zuk2012mystery}
%		- From a biological standpoint, there is no a priori reason to expect that traits should be additive. Biology is filled with non- linearity: The saturation of enzymes with substrate concentration and receptors with ligand concentration yields sigmoid response curves; cooperative binding of proteins gives rise to sharp tran- sitions; the outputs of pathways are constrained by rate-limiting inputs; and genetic networks exhibit bistable states. \cite{zuk2012mystery}
%		- Genetic studies in model organisms have long identified specific instances of interacting genes (17). Important examples include synthetic traits (e.g., 18), which occur only when multiple loci or pathways are all disrupted. \cite{zuk2012mystery}
%		- Studies have begun to reveal that epistasis is pervasive.  \cite{zuk2012mystery}
%			- In the yeast Saccharomyces cerevisiae, Brem et al. (19) analyzed as quantitative traits the levels of gene transcripts in segregants of a cross between two strains. For each transcript, they found the strongest quanti- tative trait locus (QTL) in the cross and then, conditional on the genotype at this locus, identified the strongest remaining QTL. In 67% of cases, these two QTLs demonstrated epistatic interactions. In bacteria, Khan et al. (20) and Chou et al. (21) have recently demonstrated clear epistasis among collections of five mutations that increase growth rate.  \cite{zuk2012mystery}
%			- In mouse and rat, Shao et al. (22) ana- lyzed a panel of chromosome substitution strains, with each strain carrying a different chromosome from a donor strain on a common recipient genetic background. For dozens of quantitative traits, the sum of the effect attributable to the individual donor chromosomes far exceeds (median eightfold) the total effect of the donor ge- nome, indicating strong epistasis.  \cite{zuk2012mystery}
%			- Although genetic interactions are hard to detect in humans (see below), several cases involving var- iants with large marginal effects have been recently reported in Hirschsprung’s disease, ankylosing spondylitis, psoriasis, and type I diabetes  \cite{zuk2012mystery}
%		- ...geneticists have tested for pairwise epis- tasis between loci, but have found few significant signals. \cite{zuk2012mystery}
%		- ...The reason is that individual interaction effects are expected to be much smaller than linear effects, and the sample size re- quired to detect an effect scales inversely with the square of the effect size. If n loci had equivalent effects, the sample size to detect the n loci would thus scale with n^2, whereas the sample size to detect their n^2 interactions scales with n^4. \cite{zuk2012mystery}
%		- Suppose that we consider two var- iants with frequency 20% that contribute to different pathways and increase risk by 1.3-fold (which is a large effect relative to those typically seen in GWAS). The sample size required to detect the variants is ∼4,900 (with 50% power and genome-wide significance level of α = 5 × 10−8 in a genome-wide association study with an equal number of cases and controls), whereas the sample size re- quired to detect their pairwise interaction is roughly 450,000 (at 50% power and an appropriate significance level to account for multiple hypothesis testing). A researcher who studied 100,000 samples would likely discover all of the loci but would find little evidence of epistatic interactions. \cite{zuk2012mystery}
%		- In short, the failure to detect epistasis does not rule out the presence of genetic interactions sufficient to cause substantial phantom heritability \cite{zuk2012mystery}
%		-...although the pervasiveness of epistasis in experimental organ- isms suggests that the true heritability h2 of traits may be much lower than current estimates \cite{zuk2012mystery}
%		- Despite considerable efforts, few well-replicated instances of epistasis in common human disease and trait genetics have been discovered thus far. \cite{zuk2012mystery}
%		- The only examples to date involve interactions featuring at least one locus with a large marginal e↵ect, such as HLA.  \cite{zuk2012mystery}
%		- GWAS, in ankylosing spondylitis21 and psoriasis,22 discovered interactions between two di↵erent HLA alleles and ERAP1. (In ankylosing spondylitis, the HLA-B27 allele has an odds ratio of 40.8, and in psoriasis the HLA-C allele has an odds ratio of 4.66.) HLA also plays a role in an interaction e↵ect described in a GWAS of Type 1 diabetes. (In Type 1 diabetes, HLA has a main e↵ect of 5.5, but acts non-additively with the risk of all other alleles considered cumulatively.23). Finally, interaction between RET and EDNRB in Hirschsprung’s disease was discovered in a genome-wide linkage study,24 in which RET was strongly associated with disease (log-odds score of 5.6). \cite{zuk2012mystery}

%------------------------------------------------------------
% From: A Perspective on Epistasis: Limits of Models Displaying No Main Effect
% \cite{culverhouse2002perspective}
% TOPIC: HISTORICAL PERSPECTIVE (PRE-SEQUENCING / GENOTIPYNG) 
% IMPORTANT: Context was year 2002!!!
%
%		- for the abandonment of linkage studies in favor of genome scans for association. However, there exists a large class of genetic models for which this approach will fail: purely epistatic models with no additive or dominance variation at any of the susceptibility loci. \cite{culverhouse2002perspective}
%		- Is it reasonable to suppose that an approach that must succeed in identi- fying fully penetrant Mendelian genes will also succeed for complex diseases?  \cite{culverhouse2002perspective}
%		- The complex relationship between genotype and phe- notype, however, may ultimately prove to be inade- quately described by simply summing the modest effects from several contributing loci \cite{culverhouse2002perspective}
%		- EXAMPLES OF EPISTASIS
%			- Indeed, it has been argued that epistatic interactions are a nearly universal component of the architecture of most com- mon traits. Templeton (2000), for instance, has listed a number of phenotypes in which epistasis plays a large role.  \cite{culverhouse2002perspective}
%			- An example in insects is the abnormal-abdomen phenotype in Drosophila mercatorum (DeSalle and Templeton 1986; Hollocher et al. 1992; Hollocher and Templeton 1994). \cite{culverhouse2002perspective}
%			- In humans, variation in triglyceride levels can be explained, in part, by two sets of inter- actions: between ApoB and ApoE in females and be- tween the ApoAI/CIII/AIV complex and low-density li- poprotein receptor in males (Nelson et al. 2001) \cite{culverhouse2002perspective}
%			- Even the seemingly “simple” Mendelian trait of sickle-cell anemia is revealed to be greatly modified by epistatic interactions. Individuals with sickle-cell anemia who are homozygous for two polymorphisms near the Gg locus (leading to the persistence of fetal hemoglobin) have only mild clinical symptoms \cite{culverhouse2002perspective}
%		- The main reason that most studies of complex human phenotypes fail to find evidence for epistatic interactions may simply be that commonly used designs and analytic methods inherently minimize or exclude the possibility of epistasis (Frankel and Schork 1996) \cite{culverhouse2002perspective}
%		- The complex relationship between genotype and phe- notype, however, may ultimately prove to be inade- quately described by simply summing the modest effects from several contributing loci. \cite{culverhouse2002perspective}
%		- HERITABILITY: 
%			- Thus, for fixed K, p , and p , maximizing the broad AB heritability (h 2 p V /V ) under the constraint repre- IT sented by formula (2) is equivalent to the maximizing of VI. \cite{culverhouse2002perspective}
%			- TABLE 2 and 3: Maxima of heritability using epistasis. \cite{culverhouse2002perspective}
%			-  Three-locus models can also give rise to higher relative risks than are possible in corresponding two-locus models. Three-locus penetrance models maximizing heritability at the low end of disease prevalence \cite{culverhouse2002perspective}
%		- NUMBER OF TESTS:
%			- We note that the number of tests necessary to evaluate all two-, three-, and four-way interactions, for 30–60 candidate loci, has a range similar to the number of tests suggested for a single genomewide association scan using SNPs (Collins et al. 1999; Kruglyak 1999) \cite{culverhouse2002perspective}
%			- Thus, al- though searching for two-, three-, four-, or n-way in- teractions among all the markers in a genome scan would not be practicable, a candidate-locus approach based on a genome scan for linkage may be. \cite{culverhouse2002perspective}
%		- ANALYSIS METHODS:
%			- Cases only.—The most straightforward multilocus analysis of cases-only data is a x2 test of independent segregation for the loci.  \cite{culverhouse2002perspective}
%			- Case-control.—A second approach is a multilocus case-control analysis. One method for doing this would be to compare the distribution of cases among the 3L genotypes, where L is the number of biallelic loci being simultaneously examined, versus the distribution of controls. In this analysis, a sample of N cases and N unrelated controls drawn from a population modeled by table 3 will, again, yield an expected x2 statistic 􏰃2N. However, the degrees of freedom under the null hypothesis are now 8. \cite{culverhouse2002perspective}
%		- POWER: We have seen that, if the true genetic model underlying a disease is purely epistatic, with no additive or domi- nance variation at any of the susceptibility loci, then association methods analyzing one locus at a time will have no power to detect the loci.  \cite{culverhouse2002perspective}
%		- First, we expect that, with a sufficient number of contributing loci, purely epistatic interac- tions could account for virtually all the variation in affection status for diseases with any prevalence \cite{culverhouse2002perspective}
%		- Of course, there are subclasses of purely epistatic models (providing no marginal evidence for the involve- ment of any single locus) for which, in addition, no two, three, or L􏰂1 loci jointly give evidence of in- volvement in the disorder. This leads to the concern that even assessment of all two-, three-, and (L􏰂1)-way interactions among candidate loci may be insufficient for detection of the contributing loci. \cite{culverhouse2002perspective}
%		- The restriction on maximum heritabilities in these models is most easily seen by examining L-locus models for which no collection of L 􏰂 1 loci shows mar- ginal deviations.  \cite{culverhouse2002perspective}
%		- MISSING HERITABILITY: Researchers of many complex diseases (including non–insulin-dependent diabetes mellitus, prostate can- cer, and schizophrenia) face the conundrum of moder- ately heritable diseases for which locus-by-locus anal- yses have not accounted for the predicted genetic variance. The models discussed in the present article provide one possible explanation for this. \cite{culverhouse2002perspective}
%		- These considerations lead us to believe that, in situ- ations in which heritability is moderate to high but in which locus-by-locus analyses do not account for the predicted genetic variance, it is worth pursuing a hy- pothesis of interacting loci [near the linkage peaks] \cite{culverhouse2002perspective}


%------------------------------------------------------------
% From : Defining genetic interaction (2007)
% \cite{mani2008defining}
%
%		- Sometimes mutations in two genes produce a phenotype that is surprising in light of each mutation’s individual effects. This phenom- enon, which defines genetic interaction, can reveal functional rela- tionships between genes and pathways. \cite{mani2008defining}
%		- Recent studies have used four mathematically distinct definitions of genetic interaction (here termed Product, Additive, Log, and Min). Whether this choice holds practical consequences has not been clear, because the definitions yield identical results under some condition \cite{mani2008defining}
%		- Here, we show that the choice among alternative definitions can have profound consequences. \cite{mani2008defining}
%		- The study of genetic inter- action has become increasingly systematic and large-scale, espe- cially in the yeast Saccharomyces cerevisiae (6, 8–21). \cite{mani2008defining}
%		- A quantitative genetic interaction definition has two compo- nents: a quantitative phenotypic measure and a neutrality function that predicts the phenotype of an organism carrying two noninter- acting mutations. Interaction is then defined by deviation of a double-mutant organism’s phenotype from the expected neutral phenotype \cite{mani2008defining}
%		- A double mutant with a more extreme phenotype than expected defines a synergistic (or synthetic) interaction between the corresponding mutations (synthetic lethality, in the extreme case). \cite{mani2008defining}
%		- Alleviating or “diminishing returns” interactions, in which the double-mutant phenotype is less severe than expected, often result when gene products operate in concert or in series within the same pathway. Alleviating interactions arise, for example, when a muta- tion in one gene impairs the function of a whole pathway, thereby masking the consequence of mutations in additional members of that pathway. \cite{mani2008defining}
%		- One class of phenotype, fitness, has been central to many large-scale genetic interaction studies. Although fitness was origi- nally measured in terms of population allele frequencies (1, 22, 23), it can also be measured by using growth rates of isogenic microbial cultures. \cite{mani2008defining}
%		- Genetic interaction studies have used different measures of fitness, including: (i) the exponential growth rate of the mutant strain relative to that of wild type (4, 9, 15, 19) (the relative-growth- rate measure); (ii) the increase in mutant population relative to wild type in one wild-type generation (the relative-population measure) (6); and (iii) the number of progeny per mutant organism relative to the number of progeny for wild type in one wild-type generation (the relative-progeny measure) (24) \cite{mani2008defining}
%		- Genetic interaction studies have also differed in their choice of neutrality functions, generally using either a multiplicative or a minimum mathematical function. \cite{mani2008defining}
%		- The multiplicative function, which was originally applied to fitness measures defined in terms of allele frequencies, predicts double-mutant fitness to be the product of the corresponding single-mutant fitness values. The multiplica- tive function can be combined with each of the three fitness measures above to yield three distinct definitions of genetic inter- action (4, 6, 15, 19, 24). \cite{mani2008defining}
%		- A fourth (Min) definition of genetic interaction results from the minimum neutrality function, under which noninteracting muta- tions are expected to yield the fitness of the less-fit single mutant. Each fitness measure above yields an identical set of genetic interactions under this function. A hypothetical example illustrates one rationale for the Min definition: Two single mutations each disrupt a distinct cellular pathway that limits cell growth, such that one of these mutations is substantially more limiting than the other. The double mutant might then be expected to exhibit the phenotype of the most-limiting single mutant.  \cite{mani2008defining}
%		- It has not been clear whether the choice of genetic interaction definition has any practical consequences. To evaluate the impact of definition choice, we applied each of the four definitions in turn to two reference studies. \cite{mani2008defining}
%		- Here, we show that the choice of definition can dramatically alter the resulting set of genetic interactions and the extent to which they correspond to shared gene function.  \cite{mani2008defining}
%		- For a gene pair (x, y), we refer to the fitness of the two single mutants and the double mutant, respectively, as Wx, Wy, and Wxy. \cite{mani2008defining}
%		- The neutrality function E(Wxy), predicting double-mutant fitness for a strain with mutations in noninteracting genes x and y, is defined differently under the Min, Product, Log, and Additive  \cite{mani2008defining}
%
%		- DATASET: To evaluate the impact of definition choice, we applied each of the four definitions in turn to two reference studies, St. Onge et al. (19) (Study S) and Jasnos and Korona (6) (Study J), both providing quantitative growth-rate measurements of isogenic wild-type and single- and double-mutant cell populations. \cite{mani2008defining}
%
%		- RESULTS: The Choice of Genetic Interaction Definition Matters: \cite{mani2008defining}
%			- Additive and Log Definitions Demonstrate Different Biases: However, we had observed that interaction strength had a significant positive bias (under all defi- nitions) for pairs involving mutations with extreme fitness effects. \cite{mani2008defining}
%			- Product and Log Definitions Are Equivalent for Deleterious Mutations:  \cite{mani2008defining}
%			- The Product Definition Reveals Functional Relationships Missed by the Min Definition. \cite{mani2008defining}
%			- Genetic Interaction Networks from Min and Product Definitions Differ Greatly. \cite{mani2008defining}
%
%		- WHICH DEFINITION TO USE?: We examined the distribution of 􏰍, the deviation of the expected double-mutant phenotype from the observed double mutant phenotype, and found the Product and Log definitions to be closest to this ideal in general. Additionally, we showed that the Log and Product definitions are practically equivalent when both single mutants are deleterious. \cite{mani2008defining}

%------------------------------------------------------------
% From: Shadows of complexity: what biological networks reveal about epistasis and pleiotropy
% \cite{tyler2009shadows}
% TOPIC: EPISTASIS
%
%		- We assert that epistasis and pleiotropy are not isolated occurrences, but ubiquitous and inherent properties of biomolecular networks. \cite{tyler2009shadows}
%		- EPIS EXAMPLE: 
%			- HUMAN: For example, in humans the E4 allele of apolipoprotein epsilon (ApoE) is associated with elevated blood serum cholesterol levels, but only in individuals with the A2A2 genotype at the low density lipoprotein receptor (LDLR) locus.(3) In other words, the contribution of the ApoE allele to cholesterol levels depends on the genotype at the LDLR locus. \cite{tyler2009shadows}
%			- FLY: Eye color determination in Drosophila provides a classic example. The genes scarlet, brown, and white, play major roles in a simplified model of Drosophila eye pigmentation. Eye pigmentation in Drosophila requires the synthesis and deposition of both drosopterins, red pigments synthesized from GTP, and ommochromes, brown pigments synthesized from tryptophan. A mutation in brown prevents production of the bright red pigment resulting in a fly with brown eyes, and a mutation in scarlet prevents production of the brown pigment resulting in a fly with bright red eyes. In a fly with a mutation in the white gene, neither pigment can be produced, and the fly will have white eyes regardless of the genotype at the brown or scarlet loci. In this example the white gene is epistatic to brown and scarlet. A mutant genotype at the white locus masks the genotypes at the other loci. \cite{tyler2009shadows}
%		- HISTORICAL: 
%			- William Bateson first described epistasis in 1907.(2) Like pleiotropy, this concept was developed to explain deviations from Mendelian inheritance \cite{tyler2009shadows}
%			- The term literally means “standing upon”, and Bateson used it to describe characters that were layered on top of other characters thereby masking their expression.  \cite{tyler2009shadows}
%			- The commonly used definition of epistasis--an allele at one locus masks the expression of an allele at another locus--reflects this original definition. \cite{tyler2009shadows}

\subsection{Epistasis in complex traits}


\subsection{Detecting Epistasis / interactions}


%------------------------------------------------------------
% From: Systematic Detection of Epistatic Interactions Based on Allele Pair Frequencies (2012)
% \cite{ackermann2012systematic}
%
%		- Whereas most existing epistasis screens explicitly test for a trait, it is also possible to implicitly test for fitness traits by searching for the over- or under-representation of allele pairs in a given population.  \cite{ackermann2012systematic}
%		- Such analysis of imbalanced allele pair frequencies of distant loci has not been exploited yet on a genome-wide scale, mostly due to statistical difficulties such as the multiple testing problem. We propose a new approach called Imbalanced Allele Pair frequencies (ImAP) for inferring epistatic interactions that is exclusively based on DNA sequence information. \cite{ackermann2012systematic}
%		- Most gene interaction studies explicitly measure a phenotype such as growth rate or viability [ \cite{ackermann2012systematic}
%		- However, one can also study implicit phenotypes by searching for the over- or under-representation of certain allele pairs in a given population. \cite{ackermann2012systematic}
%		- Such allele pairs are examples of Dobzhansky-Mu ̈ller incompat- ibilities: they establish a fitness bias in favor of individuals inheriting the over-represented allele combination [15]. In their most extreme form such incompatibilities are embryonic lethal. \cite{ackermann2012systematic}
%		- In this context, an implicit phenotype is a trait that is not explicitly measured in the sample but whose regulators can still be inferred from the genotype data. \cite{ackermann2012systematic}
%		- A small number of recent studies have explored this idea for the genome-level identification of epistatic interactions: if a large number of individuals is genotyped at a large number of genomic positions, it becomes possible to test all allele pairs for over- and under- representation in that population [18–20]. \cite{ackermann2012systematic}
%		- However, even though some methodological progress has been made [18], previous studies could hardly identify a significant number of interactions. The main obstacle is the humongous number of statistical hypotheses tested when comparing all markers in a genome against all markers. \cite{ackermann2012systematic}
%		- Here, we propose to address this problem by exploiting the additional information gained from studying family trios. We show that by analyzing a sufficiently large number of individuals with known family structure it becomes possible to detect substantially more interactions than what is expected if all markers were independent. \cite{ackermann2012systematic}
%		- Our method, called ‘‘Imbalanced Allele Pair frequencies (ImAP)’’ is based on inspecting 3|3 contingency tables that track the frequencies of all possible two-locus allele combinations in heterozygous individuals (assum- ing a diploid genome). The test that we propose is similar to a x2 test in that it compares the observed frequencies in this table to expected frequencies assuming independence. However, our version corrects the expected frequencies for confounding factors such as family structure or allelic drift [21]. \cite{ackermann2012systematic}
%		- In a population of 2,002 heterozygous mice with known family structure genotyped at 10,168 markers we identify 168 LD block pairs with imbalanced alleles \cite{ackermann2012systematic}


%------------------------------------------------------------
% From: \cite{rohlfs2010detecting}		Cited by: 16
%	Topic: DETECTING EPISTASIS (not GWAS)
%
%		- Distinct combinations of alleles in coevolving genes interact differently, conferring varying degrees of fitness. If this fitness differential is adequately large, the resulting selection for allele matching could maintain allelic association, even between physically unlinked loci. \cite{rohlfs2010detecting}
%		- However, because the coevolving genes are not necessarily in physical linkage, this is not an appropriate measure of coevolution-induced allelic association \cite{rohlfs2010detecting}
%		- Coevolving genes are expected to undergo compensatory mutations to maintain their interaction. \cite{rohlfs2010detecting}
% 	- Methods have been developed for detecting coevolution by testing for high correlation of phylogenetic distance matrices between gene families, genes, or gene domains.1–6 \cite{rohlfs2010detecting}
%		- HLA and KIR are well established as interacting immune-response loci under intense diversi- fying selection. Although these genes are on different chromosomes, their allele frequencies are significantly cor- related within human populations, as one would expect under intense selection for allele matching.15 \cite{rohlfs2010detecting}
%		- Most cases of selec- tive advantage for specific allele pairing would be resolved with fixation of the optimal allele pair.7 \cite{rohlfs2010detecting}
%		- In this paper, we explore the ramifications of coevolu- tion between the genes mediating sperm-ZP binding in humans. Specifically, the ZP-located protein ZP3 (MIM 182889) has been shown to mediate sperm binding to the ZP19  \cite{rohlfs2010detecting}
%		- Because ZP3 and ZP3R are putative interactors mediating gamete recognition, are polymorphic among humans, and are located on different chromosomes, they are excel- lent candidates for coevolution-induced allelic association \cite{rohlfs2010detecting}
%		- WTCCC, Affymetrix 500K SNP genotyping platform for 1504 individuals in the 1958 Birth. \cite{rohlfs2010detecting}
%		- General allelic association between a pair of SNPs is quantified by CLD. An estimate of CLD has been previously given35.. \cite{rohlfs2010detecting}
%		- To address this possibility, we use a standard con- tingency table for independence between the two genotypes (Table 1), resulting in the chi-square distributed test statistic with four degrees of freedom: \cite{rohlfs2010detecting}
%		- The CLD and GA test statistics measure allelic association, but they are also dependent on marginal one-locus genotype counts \cite{rohlfs2010detecting}
%		- To control for the one-locus genotype counts, X12 and X42 are used as test statistics in permutation tests.  \cite{rohlfs2010detecting}
%		- Permutation p values approximate exact p values, which are the probabilities of an allelic association at least as strong as that observed, given the marginal genotypes at each locus.  \cite{rohlfs2010detecting}
%		- Calculating the power of these exact tests can be prohibitively slow with a large sample size. As an alternative, we quickly esti- mate power by using theoretical test statistic distributions under the alternative hypothesis. Under the alternative hypothesis with genotype frequency matrix F, X 2 is approximately chi-square 1 distributed with one degree of freedom and noncentrality param- eter \cite{rohlfs2010detecting}
%		- We ran a similar analysis on a secondary candidate gene pair implicated in maternal-fetal interactions: GHR (MIM 600946) and GH2 (MIM 139240).37 \cite{rohlfs2010detecting}
%		- Power: because of computational limitations, we were unable to perform the exact test for larger value of n;  \cite{rohlfs2010detecting}
%		- Asymptotic Analysis: For a high but biologically reasonable s of 0.1,38 with a sample size of n 1⁄4 1480, the asymptotic CLD test has a power of 0.525 and the asymptotic GA test has a power of 0.327 \cite{rohlfs2010detecting}
%		- Population: Population structure could also cause allelic association between physically unlinked loci.  \cite{rohlfs2010detecting}
%		- Allelic association would be observed if the alleles at each locus have different frequencies in different populations and those populations are pooled together. In this analysis, ZP3 and ZP3R are associated as compared to other genes in the same individuals. It is not likely that population structure would cause allelic association in our candidate gene pair but not in other gene pairs in the same population. It is possible that ZP3 and ZP3R are statistical outliers that we expect under no selection and are associated simply by chance. However, given our limited single-hypothesis candi- date gene approach, we find that unlikely. \cite{rohlfs2010detecting}
%		- The field has yet to identify a gene pair that is certainly coevolving in which both genes are polymorphic. In the absence of a clear positive control, \cite{rohlfs2010detecting}


%------------------------------------------------------------
% From: Exploiting the co-evolution of interacting proteins to discover interaction specificity		(2003 cited 205)
% \cite{ramani2003exploiting}
%
%		- Predicting interaction specificity, such as matching members of a ligand family to specific members of a receptor family, is largely an unsolved problem. Here we show that by using evolutionary relationships within such families, it is possible to predict their physical interaction specificities.  \cite{ramani2003exploiting}
%		- We introduce the computational method of matrix alignment for finding the optimal alignment between protein family similarity matrices \cite{ramani2003exploiting}
%		- Binding speci- ficities of duplicate genes (paralogs) often diverge, such that new binding specificities are evolved \cite{ramani2003exploiting}
%		- the use of phylogenetic trees to account for the co-evolution of interacting proteins \cite{ramani2003exploiting}
%		- the hypothesis underlying these approaches is that interacting proteins often exhibit coordi- nated evolution, and therefore tend to have similar phylogenetic trees. Goh et al.17 demonstrated this by showing that chemokines and their receptors have very similar phylogenetic trees \cite{ramani2003exploiting}
%		- METHOD: 
%			- In order to exploit the evolutionary information contained in such interacting protein families, we developed an algorithm that is conceptually equivalent to superimposing the phylogenetic trees of the two protein families. \cite{ramani2003exploiting}
%			- The matrix alignment method for predicting protein inter- action specificity. Proteins in family A interact with those in family B. In each family, a similarity matrix summarizes the proteins’ evolution- ary relationships. The algorithm uses the similarity matrices to pair up the genes in the two families. Columns of matrix B are re-ordered (along with their corresponding rows in the matrix) such that the B matrix agrees maximally with matrix A, judged by minimizing the root mean square difference (r.m.s.d.) between elements in the two matrices. Interactions are then predicted between proteins heading equivalent columns of the two matrices. \cite{ramani2003exploiting}
%			- One matrix is shuffled, maintaining the correct relationships between proteins but simply re-ordering them in the matrix, until the two matrices maximally agree, minimizing the root mean square difference between elements of the two matrices. Interactions are then predicted between proteins heading equivalent columns of the two matrices.  \cite{ramani2003exploiting}
%			- For matrix alignment, MATRIX currently applies a stochastic simulated annealing-based algorithm. \cite{ramani2003exploiting}


%------------------------------------------------------------
% From: Human and Helicobacter pylori coevolution shapes the risk of gastric disease
% \cite{kodaman2014human}
% TOPIC: COEVOLUTION AND CANCER
%
%		- Helicobacter pylori is the principal cause of gastric cancer, the sec- ond leading cause of cancer mortality worldwide. However, H. pylori prevalence generally does not predict cancer incidence.			 \cite{kodaman2014human}
%		- DATA: To determine whether coevolution between host and pathogen influences disease risk, we examined the association between the severity of gastric lesions and patterns of genomic variation in matched human and H. pylori samples. Patients were recruited from two geographically distinct Colombian populations with sig- nificantly different incidences of gastric cancer, but virtually iden- tical prevalence of H. pylori infection.  \cite{kodaman2014human}
%		- All H. pylori isolates contained the genetic signatures of multiple ancestries, with an ancestral African cluster predominating in a low-risk, coastal pop- ulation and a European cluster in a high-risk, mountain popula- tion. The human ancestry of the biopsied individuals also varied with geography, with mostly African ancestry in the coastal region (58%), and mostly Amerindian ancestry in the mountain region (67%).  \cite{kodaman2014human}
%		- The interaction between the host and pathogen ancestries completely accounted for the difference in the severity of gastric lesions in the two regions of Colombia. In particular, African H. pylori ancestry was relatively benign in humans of African an- cestry but was deleterious in individuals with substantial Amerin- dian ancestry. \cite{kodaman2014human}
%		-  Thus, coevolution likely modulated disease risk, and the disruption of coevolved human and H. pylori genomes can explain the high incidence of gastric disease in the mountain population. \cite{kodaman2014human}


%------------------------------------------------------------
% From: Identification of direct residue contacts in protein--protein interaction by message passing (2009, cited 274)
% \cite{weigt2009identification}
%
%		- Applied to a set of >2,500 representatives of the bacterial two-component signal transduction system, the combination of covariance with global inference success- fully and robustly identified residue pairs that are proximal in space without resorting to ad hoc tuning parameters, both for heteroint- eractions between sensor kinase (SK) and response regulator (RR) proteins and for homointeractions between RR proteins. \cite{weigt2009identification}
%		- The spec- tacular success of this approach illustrates the effectiveness of the global inference approach in identifying direct interaction based on sequence information alone. \cite{weigt2009identification}
%		- Experimental approaches to identify surfaces of interaction between proteins such as surface-scanning mutagenesis and co- crystal structure generation are arduous and/or serendipitous. \cite{weigt2009identification}
%		- Covariance methods rely on the premise that amino acid substitu- tion patterns between interacting residues are constrained and hence correlated. To maintain protein function, the acceptance of a deleterious substitution at 1 position must be compensated for by substitution(s) in the residue(s) interacting with it (14) \cite{weigt2009identification}
%		- However, the covariance approach has a number of shortcomings that may significantly affect its predictive power (15). One impor- tant problem stems from the fact that correlation in amino acid substitution may arise from direct as well as indirect interactions \cite{weigt2009identification}
%		- A formidable technical challenge with this approach is to work out the expected statistical correlation generated by a given set of trial direct interactions, because this itself is a very difficult global optimization problem [as exemplified by the notorious ‘‘spin- glass’’ problem (16)]. This challenge is dealt with here by applying a message-passing approach (17, 18). In recent years, insights from spin-glass physics have led to the development of generalized message-passing techniques, which have been applied successfully to a number of hard combinatorial problems such as K-SAT (19 –21). \cite{weigt2009identification}
%		- The statistically corre- lated pairs are candidates for positions in contact at the protein– protein interface. However, statistical correlation does not auto- matically imply strong direct interaction. Imagine that position i is coupled directly to j, and j to k. Then i and k will also show correlation, without being directly coupled.  \cite{weigt2009identification}
%		- To circumvent this problem, we infer a global statistical model  \cite{weigt2009identification}
%		- Note that in principle higher correlations of 3 or more positions can be included in a similar way. However, the size of the available dataset does not allow for going beyond 2-residue correlations. The 21 􏰐 21 elements of fij(Ai, Aj) have to be estimated from the M 􏰈 2,546 sequences in the database; frequency counts for 􏰆2 positions would be very imprecise because of insufficient sample size. \cite{weigt2009identification}
%		-  Application of the maximum- entropy principle yields the simplest possible [Boltzman distribution] \cite{weigt2009identification}
%		- Determining these parameters to meet Eq. 1 is an algorithmically hard task, and can be achieved by using a 2-step procedure. \cite{weigt2009identification}
%			- Givenacandidatesetofmodelparameters,single-and2-residue distributions Pi(Ai) and Pij(Ai, Aj) are estimated from Eq. 2. This is computationally expensive, the exact summation over all possible protein sequences would require O(21^(N-2) N^2) steps. Approximations can be achieved by MCMC sampling—which is expected to be very slow for 21-state variables—or more effi- ciently by a semiheuristic message-passing approach (31). We use the latter approach; it reduces the computational complexity to O(21^2 N^4). \cite{weigt2009identification}
%			- Once all Pij(Ai, Aj) are estimated, we can use gradient descent to adjust the coupling strengths eij(Ai, Aj) \cite{weigt2009identification}
%			- This equation can be derived variationally within a Bayesian approach, it maximizes the joint probability of the data under model 2 (compare SI Text). Because this probability is convex, it is guaranteed to converge to a single global maximum. \cite{weigt2009identification}
%			- a quantity called direct information (DI) is introduced. It measures the part of the mutual information of a position pair, which is induced by the direct coupling. Intuitively, it can be understood as the mutual information in a 2-variable model for positions i and j only, which has the correct statistics of the amino acid occupancy of single positions, and coupling eij(Ai, Aj) in between.  \cite{weigt2009identification}
%			- Because of the scaling of the algorithmic complexity, the method cannot be applied simultaneously to all 212 positions of the protein alignment. Therefore, the 60 positions of the protein alignment being involved in the 140 highest MI-ranking pairs (containing the 32 candidates for contact pairs identified before) are selected. \cite{weigt2009identification}


%------------------------------------------------------------
% From: Identifying and seeing beyond multiple sequence alignment errors using intra-molecular protein covariation	(2010, cited 22)
% \cite{dickson2010identifying}
%
%		- Different alignments of the same protein family give different results demonstrating that covariation depends on the quality of the sequence alignment. \cite{dickson2010identifying}
%		- We show that current criteria are insufficient to build alignments for use with covariation analyses as systematic sequence alignment errors are present even in hand-curated structure-based alignment datasets like those from the Conserved Domain Database.  \cite{dickson2010identifying}
%		- We demonstrate that removing alignment errors due to 1) improper structure alignment, 2) the presence of paralogous sequences, and 3) partial or otherwise erroneous sequences, improves contact prediction by covariation analysis \cite{dickson2010identifying}
%		- Standard benchmarks for covariation accuracy measure the fraction of covarying amino acid pairs that are in contact.  \cite{dickson2010identifying}
%		- First, the sequence alignments must contain sufficient sequences with enough variation for the signal to exceed the noise. Estimates of the required number of sequences needed in the alignments for this to be true vary from *30 [6] to w125 [4,8,15,16].  \cite{dickson2010identifying}
%		- Secondly, all positions in a protein appear to covary because of their shared ancestry, and this signal is the only systematic source of covariation for the vast majority of position pairs [6,14,17].  \cite{dickson2010identifying}
%		- As one example, structure-based alignment algorithms are susceptible to shift error [18], meaning that positions in the structure alignment are not orthologous despite the fact that much of the secondary structural elements seem to overlap between aligned structures. \cite{dickson2010identifying}
%		- We observed that the same protein family often gave different numbers of covarying positions when alignments were from different sources even if the alignments contained comparable numbers of sequences.  \cite{dickson2010identifying}
%		- We also found that alignments generated without structural information identified fewer pairs in contact in the folded protein compared to alignments generated with structural information. \cite{dickson2010identifying}
%		- Here we show that a strong covariation signal can be caused by alignment error, potentially leading to false positive predictions.  \cite{dickson2010identifying}


%------------------------------------------------------------
% From \cite{petkov2005evidence}		Cited by: 91
%	Topic: DETECTING EPISTASIS (not GWAS)
%
%		- Evidence from inbred strains of mice indicates that a quarter or more of the mammalian genome consists of chromosome regions containing clusters of functionally related genes \cite{petkov2005evidence}
%		- 60 genetically diverse inbred strains. \cite{petkov2005evidence}
%		- forming networks with scale-free architecture. Combining LD data with pathway and genome annotation databases, we have been able to identify the biological functions underlying several domains and networks. \cite{petkov2005evidence}
%		- As typified by the a and b globin gene clusters, tandem duplications can give rise to gene families whose members develop divergent, but still related, functions over time.  \cite{petkov2005evidence}
%		- Gene clusters may arise as a means of promoting their coregulation through regional controls of chromatin structure and expression, and there is now considerable evidence, well summarized by Hurst et al. [1], that for variety of eukaryotes, including yeast, Caenorhabditis, Drosophila, high- er plants, and mammals, genes sharing expression patterns are more likely to be in proximity than would be expected by chance.  \cite{petkov2005evidence}
%		- ...And finally, Fisher [2] and later Nei [3,4] have argued on theoretical grounds that when genes interact epistatically, evolutionary selection will promote their genetic linkage as a means of enhancing the coinheritance of favorable allelic combinations.  \cite{petkov2005evidence}
%		- The process of inbreeding to homozygosity imposes intense selective pressures; all efforts among some species have failed, and with mice, only a fraction of initial attempts succeeded.  \cite{petkov2005evidence}
%		- Accordingly, we can expect that if clustering of functionally related genes is a common feature of mammalian genomes, there is likely to be selection for coadapted allelic combinations among the genes encoding functions that influence fitness and survival during inbreeding. This would result in regions of linkage disequilibrium (LD) among inbred strain genomes; i.e., some allelic combinations should occur more often than expected by chance. \cite{petkov2005evidence}
%		- Data: 1,456 SNPs, chosen for their high information content, among a set of 60 common and wild- derived inbred mouse strains chosen for their genetic diversity.  \cite{petkov2005evidence}
%		-  The identity of these strains and the phylogenetic relationships among them are indicated in Figure 1, which was constructed using neighbor-joining \cite{petkov2005evidence}
%		- LD calculation: estimated LD using D9, the difference between the observed frequency of an allelic combination and its random expectation, relative to the maximum deviation possible given the allele frequencies of the two markers [14,15]. D9 corrects for differences in allele frequencies and describes LD equally well when there is selection for or against the combination of majority alleles. A cumulative Fisher’s exact test (FET) was used to compute the probability (pFET) of obtaining an equally or more extreme distribution under the null hypothesis of random allelic association between pairs of SNPs.  \cite{petkov2005evidence}
%		- Permutaation test: In one set, marker locations were randomized while maintaining the assignments of alleles to strains (Figure 2, red triangles), and in the other set the assignments of alleles to strains were randomized while preserving allele ratios and marker locations (Figure 2, solid circles) \cite{petkov2005evidence}
%		- It is difficult to escape the conclusion that the selective factors acting to generate LD domains and networks during inbreeding reflect clustering and/or interaction of function- ally related elements along chromosomes \cite{petkov2005evidence}


%------------------------------------------------------------
% From: \cite{wang2012ldgidb}		Cited by: 1
%	Topic: DETECTING EPISTASIS (not GWAS)
%
%		- non-physical linkages between different mutations (or single nucleotide polymorphisms, SNPs)  \cite{wang2012ldgidb}
%		-  These interactions can be physical protein interactions, regulatory interactions, functional compensation/antagonization or many other forms of interactions.  \cite{wang2012ldgidb}
%		- non-physical SNP linkages, coupled with knowledge of SNP-disease associations may shed more light on the role of gene interactions in human disorders. \cite{wang2012ldgidb}
%		- exonic regions of protein-coding genes from the HapMap database to construct a database named the Linkage-Disequilibrium-based Gene Interaction database (LDGIdb). The database stores 646,203 potential human gene interactions, which are potential interactions inferred from SNP pairs that are subject to long-range strong linkage disequilibrium (LD), or non-physical linkages. To minimize the possibility of hitchhiking, SNP pairs inferred to be non-physically linked were required to be located in different chromosomes or in different LD blocks  \cite{wang2012ldgidb}
%		- Here we consider only the subpopulations that con- tain at least 20 individuals. \cite{wang2012ldgidb}
%		- strong LD (r2 ≥ 0.8); \cite{wang2012ldgidb}

%------------------------------------------------------------
% From: Long Range Linkage Disequilibrium across the Human Genome		(2013, cited 3)
% \cite{koch2013long}
%	Topic: DETECTING EPISTASIS (not GWAS)
%
%		- Long-range linkage disequilibria (LRLD) between sites that are widely separated on chromosomes may suggest that population admixture, epistatic selection, or other evolutionary forces are at work.  \cite{koch2013long}
%		- We quantified patterns of LRLD on a chromosome-wide level in the YRI population of the HapMap dataset of single nucleotide polymorphisms (SNPs). \cite{koch2013long}
%		- We calculated the disequilibrium between all pairs of SNPs on each chromosome (a total of .261011 values) and evaluated significance of overall disequilibrium using randomization.  \cite{koch2013long}
%		- The results show an excess of associations between pairs of distant sites (separated by .0.25 cM) on all of the 22 autosomes.  \cite{koch2013long}
%		- Disequilibria between closely-linked sites result largely from random genetic drift or (equivalently) the common ancestry of unrecombined chromo- some blocks.  \cite{koch2013long}
%		- While these ‘‘long range haplotypes’’ can extend over a few hundred kb in unrelated humans [5], they still span only a very small fraction of an entire chromosome. \cite{koch2013long}
%		- Considerably less attention has been paid to patterns of LD between pairs of sites that are separated by much greater genetic distances (say, 1 cM or more). \cite{koch2013long}
%		- finding substantial long range linkage disequilibrium (LRLD) suggests that counter- vailing forces are at work. \cite{koch2013long}
%		- 1) One possibility is population admixture [6], which has been proposed to explain unusual patterns of LRLD in some human populations \cite{koch2013long}
%		- 2) A second contrib- uting force is drift. Even in a population at demographic equilibrium, recombination between distant chromosome blocks will largely but not completely erase LD caused by drift. Recurrent bottlenecks are particularly effective at generating LD [9], and may have contributed importantly to disequilibria in non- African populations of humans \cite{koch2013long}
%		- 3) Third, epistatic selection can maintain linkage disequilibrium indefinitely [11]. Epistasis has been implicated in the LD observed between two pairs of genes in humans [12,13].  \cite{koch2013long}
%		- 4) Fourth, the hitchhiking of linked sites with a positively-selected mutation can generate large haplotype blocks that result in disequilibria over the region that they span [3,4]. \cite{koch2013long}
%		- 5) Fifth, structural variation in chromosomes, such as inversions, can alter patterns of recombination and consequently cause LD to extend over unusually large regions of a chromosome [14–16]. \cite{koch2013long}
%		-  to our knowledge there has been only one previous survey of associations between chromosomal regions across the entire human genome using high-density data. Sved [17] studied correlations in heterozygosity between chromosome blocks. His analysis of the HapMap phase 3 data found evidence of associations between blocks at distances of up to 10 cM and weak correlations between blocks on different chromosomes, but he did not attempt to assess their statistical significance. Lawrence et al. [18] provided a web-based tool for exploring long distance linkage disequilibria in the HapMap data, but did not go on to study patterns in the data. \cite{koch2013long}
%		- This paper investigates patterns of LRLD in the YRI population (the Yoruba in Ibadan, Nigeria) from the HapMap Phase 2 dataset of single nucleotide polymorphisms [23]. YRI also has weaker short-range disequilibria that might otherwise obscure the patterns of LRLD \cite{koch2013long}
%		- We calculated the disequilibria between all pairs of SNPs on the same chromosome, then analyze these data with new statistical methods.  \cite{koch2013long}
%		- Using null distributions generated by randomization, we find significant excess of disequilibria on all 22 autosomes in the Yoruba population.  \cite{koch2013long}
%
%		- Data: 120 YRI haplotypes that were genotyped at over 2.86106 SNPs in HapMap Phase 2 (data build 22) \cite{koch2013long}
%
%		- LD issues: Most commonly used measures of linkage disequilibria are not well suited for that purpose [8]. For example, a large value of D9 is likely to result from sampling if allele frequencies are near 0 or 1, while even a small value is unlikely to appear by chance if allele frequencies are intermediate and the sample size is large. \cite{koch2013long}
%		- We therefore use the probability that a value of the disequilibrium D as large or larger than that in the sample would be observed if there is no association in the population from which the sample is drawn, conditioned on the sampled allele frequencies at the two loci. This probability, which we denote pD, is given by the tail of Fisher’s exact test [8,28,29] \cite{koch2013long}
%		- As the distance between a pair of sites on a chromosome grows large (specifically, the product of the recombination rate and the effective population size becomes much greater than 1), the sampling distribution for two-locus haplotypes converges on that of Fisher’s exact test [30,31].  \cite{koch2013long}
%
%		- Patches: When a pair of distant sites are in disequilibrium, it is likely that other sites near to them will also be associated as a result of short- range associations [17,32]. In effect, the underlying structure in the data is disequilibrium between pairs of chromosomal blocks rather than between pairs of individuals sites \cite{koch2013long}
%		- To control for this we used a simple and efficient ad hoc strategy that identifies ‘‘patches’’ of disequilibria. \cite{koch2013long}
%
%		- Results: We take two approaches to search for nonrandom patterns of LRLD. We first ask whether observed values of pD are more extreme than expected. For this purpose we determined the most extreme (that is, smallest) value of pD in each patch, then calculated the mean of these extreme values across all patches on a chromosome. We refer to this statistic as pDmax.  \cite{koch2013long}
%		-	Second, we ask whether the number of LRLD patches observed for a given chromosome is greater than expected by chance. We denote this statistic as nP. \cite{koch2013long}
%		- To test for the statistical significance of pDmax and nP, we generate their null distributions using a randomization method \cite{koch2013long}
%		- There are two motivations behind this method. First, it preserves the allele frequencies at each site. Second, it maintains the structure of short range disequilibria in the sample.  \cite{koch2013long}
%		- Computational time: Constructing these null distributions is the most computationally intensive part of our method. For the analyses reported below, over 4.861014 values of pD were computed, and the project consumed about 34,000 hours of CPU time. \cite{koch2013long}
%		- All of the 22 chromosomes show significant values for pDmax at the p,0.05 level, and all remain significant after a Bonferroni correction for multiple tests. For the second test statistic, n , 19 chromosomes show significant P values, 18 of which remain significant after the Bonferroni correction. These results suggest there is long-range linkage disequilibrium in the YRI population. \cite{koch2013long}
%		- [LRLD] have been little studied, they may be indicators of important evolutionary processes \cite{koch2013long}

\subsection{Epistatic Evolution and CoEvolution}

%------------------------------------------------------------
% From: A Novel Method for Detecting Intramolecular Coevolution: Adding a Further Dimension to Selective Constraints Analyses (2006, Cited 120)
% \cite{fares2006novel}
%	TOPIC: CoEvolution Method [CAPS]
%
%		- COEVOLUTION FIRST PAPER: Coevolution of any type has its origin in the covarion hypothesis proposed first by Fitch and Markowitz (1970). This hypothesis states that, at any given time, some sites are invariable due to their functional or structural constraints but, as mutations are fixed else- where in the sequence, these constraints may change. \cite{fares2006novel}
%		- The reason is that, while interaction would necessarily involve co- evolution, coevolution does not imply physical interac- tion. vi th/mc	 \cite{fares2006novel}
%		- Coevolution analysis and functional data for heat-shock proteins, Hsp90 and GroEL, highlight that almost all detected coevolving sites are functionally or structurally important. \cite{fares2006novel}
%		- The identifi- cation of genes showing particular amino acid residues that have undergone adaptive evolution is key in deter- mining functionally or structurally important protein regions. \cite{fares2006novel}
%		- Methods designed to detect adaptive evolution can be based on Bayesian approaches (Yang et al. 2000) or maximum parsimony (Suzuki and Gojobori 1999; Fares et al. 2002a). None of these methods takes into account the evolutionary interdependence between protein residues \cite{fares2006novel}
%		- Sites constraints are hence dependent on the interactions with other residues of the molecule \cite{fares2006novel}
%		- Mutations at either nearby sites or functionally related distant sites in the structure will change the selective constraints. \cite{fares2006novel}
%		- For instance, linear sliding- window methods are one-dimensional based and as- sume independence between different window regions irrespective of their three-dimensional proximity \cite{fares2006novel}
%		- Con- versely, classification of amino acids in the same group of evolution based on their three-dimensional prox- imity (three-dimensional sliding window) will ignore the coevolution between functional regions that are spatially distant
%		- Various reports state that residues can form a physically connected network that links distant functional sites in the tertiary protein structure (Su ̈el et al. 2003) \cite{fares2006novel}
%		- Coevolution between clusters of sites, which are not in contact, has also been shown (Pritchard and Dufton 2000) \cite{fares2006novel}
%		- Coevolution between distant sites has been observed in sites proximal to regions with critical functions, where coevolution occurs to maintain the structural characteristics around these regions and consequently to maintain the protein con- formational and functional stability (Gloor et al. 2005). \cite{fares2006novel}
%		- [various methods coevolution exist...] => The main limitation of many of these methods has been their inability to separate phylogenetic linkage from functional and structural coevolution.  \cite{fares2006novel}
% 	- Gloor et al. (2005) partially corrected these effects although their method requires alignments of at least 125 sequences to remove stochas- tic covariation. \cite{fares2006novel}
%		- METHOD: \cite{fares2006novel}
%			- The method instead compares the transition probability scores between two sequences at these particular sites, using the blocks substitution matrix (BLOSUM) \cite{fares2006novel}
%			- For each protein alignment the correspon- dent BLOSUM matrix is applied, depending on the average sequence identity. \cite{fares2006novel}
%			- an alignment including two highly divergent sequence groups (for example, gene duplication predating speciation) could show an unrealistic pairwise average identity level. In this respect, sequences that diverged a long time ago are more likely to fix correlated mutations at two sites by chance \cite{fares2006novel}
%			- BLOSUM values should be hence normalized by the time of divergence be- tween sequences. BLOSUM values (Bek) are thus weighted for the transition between amino acids e and k using the time (t) since the divergence between sequences i and j: \cite{fares2006novel}
%			- The mean variability for the corrected BLOSUM transition is... \cite{fares2006novel}
%			- The coevolution between amino acid sites (A and B) is es- timated thereafter by measuring the correlation in the pair- wise amino acid variability, relative to the mean pairwise variability per site, between them. \cite{fares2006novel}
%		- LIMITATIONS:  \cite{fares2006novel}
%			- For example, saturation of synonymous sites can lead to underestimates of the divergence times, although data sets used in this study did not show such effects. \cite{fares2006novel}
%			- The number of sequences in the alignment also poses a problem when sequences are too divergent, although the sensitivity is improved compared to that of previous methods.  \cite{fares2006novel}
%			- Further, constant amino acid sites that are very likely to be functionally important cannot be tested for coevolution using CAPS, although this limitation affects all the methods so far. \cite{fares2006novel}


%------------------------------------------------------------
% From: Co-evolution of proteins with their interaction partners	(2000, cited 395)
% \cite{goh2000co}
%
%		- The divergent evolution of proteins in cellular signaling pathways requires ligands and their receptors to co-evolve, creating new pathways when a new receptor is activated by a new ligand. \cite{goh2000co}
%		- We have used phosphoglycerate kinase (PGK), an enzyme that forms its active site between its two domains, to develop a standard for measuring the co-evolution of inter- acting proteins. \cite{goh2000co}
%		- The N-terminal and C-terminal domains of PGK form the active site at their interface and are covalently linked. Therefore, they must have co-evolved to preserve enzyme function. \cite{goh2000co}
%		- The analysis is extended to ligands and their receptors, using the chemokines as a model.  \cite{goh2000co}
%		- The chemokine family of protein ligands and their G-protein coupled receptors have co- evolved so that each subgroup of chemokine ligands has a matching sub- group of chemokine receptors. \cite{goh2000co}
%		- Protein-protein binding is a subset of these interactions which is of primary importance in metabolic and signaling pathways. \cite{goh2000co}
%		- Proteins and their interaction partners must co- evolve so that any divergent changes in one part- ner's binding surface are complemented at the interface by their interaction partner (Atwell et al., 1997; Jespers et al., 1999; Moyle et al., 1994; Pazos et al., 1997). \cite{goh2000co}


%------------------------------------------------------------
% From: Coevolution analyses illuminate the dependencies between amino acid sites in the chaperonin system GroES-L		(2013, cited 2)
% \cite{ruiz2013coevolution}
%
%		- GroESL is a heat-shock protein ubiquitous in bacteria and eukaryotic organelles. This evolutionarily conserved protein is involved in the folding of a wide variety of other proteins in the cytosol, being essential to the cell.  \cite{ruiz2013coevolution}
%		- The folding activity proceeds through strong conformational changes mediated by the co-chaperonin GroES and ATP: \cite{ruiz2013coevolution}
%		- We hypothesize that different overlapping sets of amino acids coevolve within GroEL, GroES and between both these proteins \cite{ruiz2013coevolution}
%		- METHOD: CAPS \cite{ruiz2013coevolution}


%------------------------------------------------------------
% From: Computational Protein Design Quantifies Structural Constraints on Amino Acid Covariation		(2013, cited 5)
% \cite{ollikainen2013computational}
% TOPIC: Computational protein design methods and coevolution!!!
%
%		- Amino acid covariation, where the identities of amino acids at different sequence positions are correlated, is a hallmark of naturally occurring proteins. \cite{ollikainen2013computational}
%		- This covariation can arise from multiple factors, including selective pressures for maintaining protein structure, requirements imposed by a specific function, or from phylogenetic sampling bias. \cite{ollikainen2013computational}
%		- Here we employed flexible backbone computational protein design to quantify the extent to which protein structure has constrained amino acid covariation for 40 diverse protein domains. \cite{ollikainen2013computational}
%		- We find significant similarities between the amino acid covariation in alignments of natural protein sequences and sequences optimized for their structures by computational protein design methods. \cite{ollikainen2013computational}
%		- These results indicate that the structural constraints imposed by protein architecture play a dominant role in shaping amino acid covariation and that computational protein design methods can capture these effects.  \cite{ollikainen2013computational}
%		- Evolutionary selective pressures on protein structure and function have shaped the sequences of today’s naturally occurring proteins [1–3]. As a result of these pressures, sequences of natural proteins are close to optimal for their structures \cite{ollikainen2013computational}
%		- Natural protein sequences therefore provide an excellent test for compu- tational protein design methods, where the goal is to predict protein sequences that are optimal for a desired protein structure and function [5]. \cite{ollikainen2013computational}
%		- Beyond simply recovering the native sequence, a further challenge in computational protein design is to predict the set of tolerated sequences that are compatible with a given protein fold and function [9–13]. Predicting sequence tolerance is important for applications such as characterizing mutational robustness [14,15], predicting the specificity of molecular interactions [16– 20], and designing libraries of proteins with altered functions [21,22].  \cite{ollikainen2013computational}
%		- Previous work has indicated that networks of covarying amino acids play a role in allosterically linking distant functional sites, suggesting that amino acid covariation is driven by protein functional constraints [30,31]. \cite{ollikainen2013computational}
%		- In this paper, we use computational protein design to measure the extent to which protein structure has shaped amino acid covariation in a diverse set of 40 protein domains. \cite{ollikainen2013computational}
%		- Since computational protein design predicts sequences that are energet- ically optimal based on protein structure alone, we expect that pairs of amino acids that highly covary in both designed and natural sequences to have likely covaried to maintain protein structure \cite{ollikainen2013computational}
%		- We find significant overlap in the sets of highly covarying amino acid pairs between designed and natural sequences for all 40 domains examined, suggesting that mainte- nance of protein structure is a dominant selective pressure that constrains the evolution of amino acid interactions in proteins. \cite{ollikainen2013computational}
%		- METHOD: The mutual information (MI) between each pair of columns , Z-score resepct to the mean MI. This normalization of MIp was demonstrated to reduce the sensitivity to misaligned regions in multiple sequence alignments, which otherwise result in artificially high mutual information scores [28]. \cite{ollikainen2013computational}


%------------------------------------------------------------
% From: Correlated Mutations Contain Information About Protein±protein Interaction		(1997, cited 489)
% \cite{pazos1997correlated}
%	TOPIC: CoEvolution one of the earliest papers
%
%		- Many proteins have evolved to form speci®c molecular complexes and the speci®city of this interaction is essential for their function.  \cite{pazos1997correlated}
%		- The network of the necessary inter-residue contacts must consequently constrain the protein sequences to some extent.  \cite{pazos1997correlated}
%		- In other words, the sequence of an interacting protein must re ̄ect the consequence of this process of adap- tation. It is reasonable to assume that the sequence changes accumulated during the evolution of one of the interacting proteins must be compen- sated by changes in the other. \cite{pazos1997correlated}
%		- Here we apply a method for detecting correlated changes in multiple sequence alignments to a set of interacting protein domains and show that positions where changes occur in a correlated fashion in the two interacting molecules tend to be close to the protein±protein interfaces.  \cite{pazos1997correlated}
%		- This leads to the possibility of developing a method for predicting con- tacting pairs of residues from the sequence alone. Such a method would not need the knowledge of the structure of the interacting proteins, and hence would be both radically different and more widely applicable than traditional docking methods. \cite{pazos1997correlated}
%		- We indeed demonstrate here that the information about correlated sequence changes is suf®cient to single out the right inter-domain dock- ing solution amongst many wrong alternatives of two-domain proteins. \cite{pazos1997correlated}
%		- We propose here a new and completely different approach to the study and prediction of protein± protein interaction. Instead of considering the structural nature of the interactions, we try to de- tect the sequence traces that evolution may have left on the interacting sequences during the process of preserving the protein±protein interaction sites. \cite{pazos1997correlated}
%		- Therefore, our approach is not restricted to the cases in which the structures of the proteins to be docked are known and is applicable to any family of interacting proteins for which a large enough se- quence family is available. \cite{pazos1997correlated}
%		- Over time, amino acid substitution may stabilise an interface that does not exist in the closed monomer ... stabilising mutations in these interfaces would be favoured in natural selection'' (Bennett et al., 1995); \cite{pazos1997correlated}
%		- We propose that it is possible to detect this signal by studying com- pensatory mutations. In order to do so, we have appropriately modi®ed our previously published method for the calculation of correlated mutations in multiple-sequence alignments (GoÈbel et al., 1994; Pazos et al., 1997). \cite{pazos1997correlated}


%------------------------------------------------------------
% From: Correlated mutations and residue contacts in proteins		(1994, cited 594)
% \cite{gobel1994correlated}
% TOPIC: SEMINAL PAPER!!
%
% Could not download the paper, all I got was the abstract
%
% Abstract
%The maintenance of protein function and structure constrains the evolution of amino acid sequences. This fact can be exploited to interpret correlated mutations observed in a sequence family as an indication of probable physical contact in three dimensions. Here we present a simple and general method to analyze correlations in mutational behavior between different positions in a multiple sequence alignment. We then use these correlations to predict contact maps for each of 11 protein families and compare the result with the contacts determined by crystallography. For the most strongly correlated residue pairs predicted to be in contact, the prediction accuracy ranges from 37 to 68% and the improvement ratio relative to a random prediction from 1.4 to 5.1. Predicted contact maps can be used as input for the calculation of protein tertiary structure, either from sequence information alone or in combination with experimental information. © 1994 John Wiley & Sons, Inc. \cite{gobel1994correlated}
%		- METHOD (from another paper): Correlated mutations were calculated as de- scribed (GoÈbel et al., 1994). Each position in the alignment is coded by a distance matrix. This pos- ition-speci®c matrix contains the distances between all pairs of sequences at that position. Distances are de®ned by the scoring matrix of McLachlan (1971). The association between each pair of pos- itions is calculated as the average of the correlation for each corresponding bin of the position-speci®c matrices. Positions with more than 10% gaps or completely conserved were not included in the cal- culation. \cite{gobel1994correlated}
%


%------------------------------------------------------------
% From: Disentangling Direct from Indirect Co-Evolution of Residues in Protein Alignments
% \cite{burger2010disentangling}
%
%		- Predicting protein structure from primary sequence is one of the ultimate challenges in computational biology. \cite{burger2010disentangling}
%		- Given the large amount of available sequence data, the analysis of co-evolution, i.e., statistical dependency, between columns in multiple alignments of protein domain sequences remains one of the most promising avenues for predicting residues that are contacting in the structure.  \cite{burger2010disentangling} \cite{burger2010disentangling}
%		- A key impediment to this approach is that strong statistical dependencies are also observed for many residue pairs that are distal in the structure. \cite{burger2010disentangling}
%		- Using a comprehensive analysis of protein domains with available three-dimensional structures we show that co-evolving contacts very commonly form chains that percolate through the protein structure, inducing indirect statistical dependencies between many distal pairs of residues \cite{burger2010disentangling}
%		- The identification of functionally and structurally important elements in DNA, RNA and proteins from their sequences has been a major focus of computational biology for several decades. A common approach is to create a multiple alignment of homologous sequences, which places ‘equivalent’ residues into the same column and as such gives a hint of the evolutionary constraints that are acting on related sequences. \cite{burger2010disentangling}
%		- Markov models [1] of protein families and domains have been highly successful in identifying sequences that have similar function and fold into a common structure, \cite{burger2010disentangling}
%		- These hidden Markov models typically assume that the residues occurring at a given position are probabilistically independent of the residues occurring at other positions. At the time at which these models were developed, it was entirely reasonable to ignore dependencies between residues at different positions, since the amount of available sequence data was generally insufficient to estimate joint probabilities of multiple residues. \cite{burger2010disentangling}
%		- As the functionality of biomolecules crucially depends on their three-dimensional structures, whose stabilities depend on interac- tions between residues that are near to each other in space, it is of course to be expected that significant dependencies between residues at different positions will exist. \cite{burger2010disentangling}
%		- CAPS and MI SUCK:  \cite{burger2010disentangling}
%			- We collected a comprehensive set of 2009 multiple alignments of protein domains from the Pfam database [19] for which a three dimensional structure was available (see Materials and Methods) and calculated, for each pair (ij) of columns in each alignment, the statistical dependency using a measure, log (Rij ), which is a finite-size corrected version of mutual information (see Materials and Methods). Since the distribution of log (R) values for an alignment depends strongly on the number of sequences in the alignment, their phylogenetic relationship, and the length of the alignment, log (R) values cannot be directly compared across different alignments. Therefore, we calculated the mean and variance of log (R) values for each alignment and transformed the log (R) values to Z-values (number of standard deviations from the mean). Finally, for each alignment, we divided all pairs of residues into those that are contacting in the three-dimensional structure, and those that are distant in the structure, and calculated the distribution of Z-values for these two sets of residue pairs. As in previous work (e.g. [10,20]) and as defined for CASP [21], two residues were considered in [....]  \cite{burger2010disentangling}
%			- [...] indeed, a higher fraction of contacting residues shows strong statistical dependen- cies than distal residues. However, we also see that the difference in the Z-distribution of close and distal pairs is only moderate. \cite{burger2010disentangling}
%			- Since there are generally many more distal pairs than close pairs, this implies that, even at high Z-values, the majority of residue- pairs are in fact distal in the structure \cite{burger2010disentangling}
%			- This result shows that simple measures of statistical dependency, such as mutual information, are poor at predicting which pairs of residues are directly contacting in the structure. \cite{burger2010disentangling}
%		- WHY DO MI AND CAPS SUCK?  \cite{burger2010disentangling}
%			- The main question is why so many structurally distal pairs show statistical dependencies in their amino-acid distributions that are stronger than those between directly contacting residues. \cite{burger2010disentangling}
%			- 1) First, whereas measures such as mutual information treat the sequences in the multiple alignments as statistically independent, in reality many of the sequences are phylogenetically closely related \cite{burger2010disentangling}
%			- 2) Some of these distant dependencies have been suggested to be caused by homo- oligomeric interactions [14,22]. Thus, in this interpretation, some of the ‘distal’ pairs with strong statistical dependencies are in fact contacting in the homo-oligomer.  \cite{burger2010disentangling}
%			- 3) dependencies are induced by indirect interactions that are mediated either by intermediate molecules [15,23] or by chains of directly interacting residue pairs that run through the protein and connect distal pairs [23–25] \cite{burger2010disentangling}
%		- METHOD: \cite{burger2010disentangling}
%			- We show that a Bayesian network model which we recently developed to predict protein-protein interac- tions [27] can be adapted to rigorously disentangle direct from indirect statistical dependencies between residues \cite{burger2010disentangling}
%			- Briefly, our model assumes that the sequences in a multiple alignment D (the data) are drawn from an (unknown) underlying joint probability distribution P(x1,x2,...,xl) with l the width of the alignment and xi the amino acid at position i. Profile hidden Markov models typically assume that the amino acids at different positions are independent  \cite{burger2010disentangling}
%			- Any model that considers only pairwise conditional dependen- cies factorizes the joint probability...where \pi(i) is the single other position which the residue at position i depends on \cite{burger2010disentangling}
%			- In particular, we do not attempt to estimate the conditional probabilities P(xi jxj ) but rather treat these conditional probabilities as nuisance parameters that we integrate out in calculating the likelihood of the alignment. \cite{burger2010disentangling}
%			- In addition, and importantly, we do not consider only a single ‘best’ way of choosing which other position p(i) each position i depends on, but rather we sum over all ways in which the dependencies can be chosen.  \cite{burger2010disentangling}
%			- The sum over spanning trees in (9) can be calculated using a generalization of Kirchhoff’s matrix-tree theorem. For this we need to calculate the Laplacian of the matrix... \cite{burger2010disentangling}


%------------------------------------------------------------
% From: Disentangling evolutionary signals: conservation, specificity determining positions and coevolution. Implication for catalytic residue prediction (2012, cited: 8)
% \cite{teppa2012disentangling}
%
%		- However, it is not clear to what extent these different methods overlap, and if any of the methods have higher predictive potential compared to others when it comes to, in particular, the identification of catalytic residues (CR) in proteins. \cite{teppa2012disentangling}
%		- The importance of a particular residue in a protein can be due to many different factors, including structural stability, protein- protein interaction, protein-DNA/RNA interaction, lig- and binding site and maintenance of protein functions. \cite{teppa2012disentangling}
%		- In most cases, it is difficult to assign a particular func- tion to a particular residue or group of residues, as func- tion is determined by a subtle interplay between multiple residues and mutation to any of them might impact the protein function and/or structure \cite{teppa2012disentangling}
%		- Three clear signals of evolution are: con- servation, conservation within specific groups of sequences sharing a common function, and coevolution between resi- dues (see Figure 1) \cite{teppa2012disentangling}
%			-1) Conservation is straightforward to cal- culate and interpret. A change in a conserved position (even when proteins are highly diverse) should have a deleterious effect on the protein function. \cite{teppa2012disentangling}
%			-2) Specificity de- termining positions (SDPs) are those positions within multiple sequence alignments (MSAs) that are conserved within groups of proteins that perform the same function (specificity groups) and varying between groups with dif- ferent functions/specificities. These sites generally de- termine protein specificity either by binding specific substrate/inhibitor or through interaction with other protein [2-4]. \cite{teppa2012disentangling}
%			-3) The degree of co-evolution between pairs of residues is commonly estimated using a measure of mutual informa- tion (MI) \cite{teppa2012disentangling}
%		- Several methods to predict specificity-determining positions have been developed. Many of these require a previous classification of the proteins into functional groups [3,5,6], which is a problematic limitation since the specificity of a given protein is unavailable in the great majority of cases and is non-trivial to calculate and validate. \cite{teppa2012disentangling}
%		- Here, we aim at addressing this question by comparing the ability to identify catalytic residues (CR) in enzymatic proteins of different information-based methods \cite{teppa2012disentangling}
%		- DATA: The analysis is based on a set of 424 enzymatic Pfam families earlier described by Marino Buslje (2010)  \cite{teppa2012disentangling}
%		- Given this data set, we calculated measures related to evolution for the different methods included in the benchmark, and next analyzed the overlap/correlation between these measures and their predictive potential for identifica- tion of CR in proteins. \cite{teppa2012disentangling}
%		- RESULTS:
%			- Methods for prediction of SDPs aim at estimating a score that correlates with the functional importance of a given residue in terms of protein specificity. \cite{teppa2012disentangling}
%			- From Figure 2, it is clear that the methods for SDP identification (ivET, SDPfox and XDET) show limited mutual overlap. The correlations values are low for all comparisons, with the highest value of 0.34 being between SDPfox and XDET. \cite{teppa2012disentangling}
%			-  We next analyzed the correlation between methods aimed to rank the residues by functional importance \cite{teppa2012disentangling}
%			- From our results, we find that the methods included in the benchmark can be divided in three groups with limited mutual overlap.  \cite{teppa2012disentangling}
%				-1) One group consists of methods which predictive signal is strongly correlated to sequence conservation (rvET, and sequence conserva- tion itself),  \cite{teppa2012disentangling}
%				-2) one group consists of the methods whose predictive signal is derived from mutual information (cMI),  \cite{teppa2012disentangling}
%				-3) and the last group consists of the methods devel- oped for prediction of specificity determining positions (SDPfox, XDET and ivET). \cite{teppa2012disentangling}
%			- CONCLUSION:  we find that only methods from the first two of the above three groups displayed a reliable predictive performance (mean AUC value above 0.8), indicating that the methods from the SDP group has limited value for the identification of residues critical for protein func- tion. \cite{teppa2012disentangling}


%------------------------------------------------------------
% From: Direct-coupling analysis of residue coevolution captures native contacts across many protein families	(2011 cited 165)
% \cite{morcos2011direct}
% TOPIC: Mean Field approximation to DCA Algorithm
%
%		- It has long been suggested that the resulting correlations among amino acid compositions at different sequence positions can be exploited to infer spatial contacts within the tertiary protein struc- ture. \cite{morcos2011direct}
%		- Crucial to this inference is the ability to disentangle direct and indirect correlations, as accomplished by the recently introduced direct-coupling analysis (DCA). Here we develop a computationally efficient implementation of DCA, which allows us to evaluate the accuracy of contact prediction by DCA for a large number of protein domains, based purely on sequence information. \cite{morcos2011direct}
%		- DCA is shown to yield a large number of correctly predicted contacts, recapitulating the global structure of the contact map for the majority of the pro- tein domains examined.  \cite{morcos2011direct}
%		- Furthermore, our analysis captures clear signals beyond intradomain residue contacts, arising, e.g., from alternative protein conformations, ligand-mediated residue cou- plings, and interdomain interactions in protein oligomers. \cite{morcos2011direct}
%		- Correlated substitution patterns between residues of a protein family have been exploited to reveal information on the struc- tures of proteins (1–10).  \cite{morcos2011direct}
%		- However, such studies require a large number (e.g., the order of 1,000) of homologous yet variable pro- tein sequences.  \cite{morcos2011direct}
%		- If two residues of a protein or a pair of interacting proteins form a contact, a destabilizing amino acid substitution at one position is expected to be compensated by a substitution of the other position over the evolutionary timescale, in order for the residue pair to maintain attractive interaction.  \cite{morcos2011direct}
%		- A major shortcoming of covariance analysis is that correlations between substitution patterns of interacting residues induce secondary correlations between noninteracting residues \cite{morcos2011direct}
%		- This problem was subsequently overcome by the direct-coupling ana- lysis (DCA) (16, 17), which aims at disentangling direct from indirect correlations.  \cite{morcos2011direct}
%		- The top 10 residue pairs identified by DCA were all shown to be true contacts between the TCS proteins, and they were used to guide the accurate prediction (3-Å rmsd) of the interacting TCS protein complex (18, 19) \cite{morcos2011direct}
%		- Previously, a message-passing algorithm was used to imple- ment DCA (16). This approach, here referred to as mpDCA, was rather costly computationally because it is based on a slowly converging iterative scheme. This cost makes it unfeasible to ap- ply mpDCA to large-scale analysis across many protein families. \cite{morcos2011direct}
%		- Here we will introduce mfDCA, an algorithm based on the mean- field approximation of DCA. The mfDCA is 10^3 to 10^4 times fas- ter than mpDCA \cite{morcos2011direct}
%		- Starting with a multiple-sequence alignment (MSA) of a large number of sequences of a given protein domain, extracted using Pfam’s hidden Markov models (HMMs) (21, 22), the basic quantities in this context are the frequency count f i ðAÞ for a single MSA column i, characterizing the relative frequency of finding amino acid A in this column, and the frequency count f ij ðA;BÞ for pairs of MSA columns i and j, characterizing the frequency that amino acids A and B coappear in the same protein sequence in MSA columns i and j. Alignment gaps are considered as the 21st amino acid. Mathematical definitions of these counts are provided in Methods. \cite{morcos2011direct}
%		- The raw statistical correlation obtained above suffers from a sampling bias, resulting from phylogeny, multiple-strain sequen- cing, and a biased selection of sequenced species. The problem has been discussed extensively in the literature (10, 23–26). \cite{morcos2011direct}
%		- In this study, we implemented a simple sampling correction, by counting sequences with more than 80% identity and reweighting them in the frequency counts.  \cite{morcos2011direct}
%		- A simple measure of correlation between these two columns is the mutual information (MI), defined by Eq. 3 in Methods. As we will show, the MI turns out to be an unreliable predictor of spatial proximity. \cite{morcos2011direct}
%		- Central to our approach is the disentanglement of direct and indirect correlations, which is attempted via DCA, \cite{morcos2011direct}
%		- This algorithm, termed mfDCA, is able to perform DCA for alignments of up to about 500 amino acids per row, as compared to 60–70 amino acids in the message-passing approach.  \cite{morcos2011direct}
%		- METHOD [mean field calculation] \cite{morcos2011direct}
%			- To disentangle direct and indirect couplings, we aim at inferring a statistical model P(A1;...;AL) for entire protein sequences (A1 ;...;AL). \cite{morcos2011direct}
%			- Besides this constraint, we aim at the most general, least-constrained model PðA1;:::;ALÞ. This model can be achieved by applying the maximum-entropy principle (45, 46), and it leads to an explicit mathematical form of PðA1;:::;ALÞ as a Boltzmann distribution with pairwise couplings eij ðA;BÞ and local biases \cite{morcos2011direct}
%			-  The exponential of [the partition function] is expanded into a Taylor series. Keeping only the linear order of this expansion, we obtain the well-known mean-field equations \cite{morcos2011direct}
%			- For later convenience, we also introduce the Hamiltonian [exponential of negative Hamiltonian is the partition function] \cite{morcos2011direct}
%			- It is important to note that the partition function itself contains all necessary information on the marginals, in particular we have.... \cite{morcos2011direct}
%			- The algorithmic approach is based on a systematic small-coupling expansion, i.e., on a Taylor expansion around zero coupling. This expansion was introduced in [12] by Plefka for disordered Ising models (Ising spin- glasses, corresponding to binary variables with q = 2). \cite{morcos2011direct}
%			- Furthermore we introduce the so-called Gibbs potential ... as the Legendre transform of the free energy F = − ln Z . \cite{morcos2011direct}
%			- The first derivative of the Gibbs potential with respect to α equals thus the average of the coupling term in the Hamiltonian. At α = 0, this average can be done eas- ily, since the joint distribution of all variables becomes factorized over the single sites [.......] we find the first- order approximation of the Gibbs potential \cite{morcos2011direct}

%------------------------------------------------------------
% From: Emerging methods in protein co‐evolution (2013, Nature)
% \cite{de2013emerging}
%
%		- DEFINITION: 
%			- In its simplest definition, co‐evolution refers to the coordinated changes that occur in pairs of organisms or biomolecules, typically to main‐ tain or to refine functional interactions between those pairs.  \cite{de2013emerging}
%			- Darwin himself initiated the study of co‐evolution, and his observation on the relationship between the size of orchids’ corollae and the length of the proboscis of pol‐ linators led him to predict successfully the existence of a new species that was able to suck from the large spur of Darwin’s orchid.  \cite{de2013emerging}
%			- The studies of Dobzhansky1 and others2 contributed to the establishment of this concept in genetic terms, although the term co‐evolution is usually attrib‐ uted to Ehrlich3, and it is commonly defined as ‘reciprocal evolutionary change in interacting species’4. \cite{de2013emerging}
%			- For the past 20 years, much effort has been dedicated to investigating co‐evolution at the molecular level. In a classical study, coordinated sequence changes among genes (and their protein products) were proposed to be essential to optimize physiological performance and reproductive success5, thus indicating that molecular co‐evolution could be an important and widespread determinant of fitness. \cite{de2013emerging}
%			- Although co‐evolution can potentially occur between various biomolecules, most recent tools focus on protein co‐evolution.  \cite{de2013emerging}
%		- Tools at the residue level were inspired by the existence of interdependent changes in groups of variable amino acids, as formulated for the first time by the covarion model6, and they typically use the multiple sequence alignment (MSA) for a protein family of homologues to search for correlated mutations.  \cite{de2013emerging}
%		- Such correlated muta‐ tions are suggestive of compensatory changes that occur between entangled residues (for example, those in prox‐ imity, direct contact or acting together in catalytic or binding sites) to maintain protein stability, function or folding7–10. Furthermore, extending these methods to search for correlated mutations between pairs of interacting proteins can identify sites of inter‐protein interaction11–17. \cite{de2013emerging}
%		- In parallel, related methods have been developed to search for larger groups of residues that are specifically co‐conserved within particular protein subfamilies.  \cite{de2013emerging}
%		- The co‐evolution between interacting species, such as parasites–hosts, predators–prey and symbionts–hosts, is in many cases manifested as a similarity of the phy‐ logenetic trees of these co‐evolving species \cite{de2013emerging}
%		- Likewise, molecular co‐evolution caused by physical or functional protein interactions frequently results in similarities of the corresponding protein family trees. Consequently, approaches based on protein family tree similarity can successfully identify interaction partners for a given protein, such as ligand–receptor pairs \cite{de2013emerging}
%		- Co‐evolution at the residue level: Substantial effort has been invested in studying the co‐evolution of pairs of positions in MSAs of protein families (that is, residue co‐evolution). These pairs of co‐evolving positions were often found to correspond to spatially proximal residues in the protein structure, and such putative inter‐residue contacts have aided protein structure prediction  \cite{de2013emerging}
%		- Furthermore, co‐evolution between residues in different proteins has been used as predic‐ tors of the interacting surfaces (protein interfaces) in pro‐ tein complexes as well as in the search for interacting partners of a given protein, as discussed later in ‘Hybrid residue–protein methods’. \cite{de2013emerging}
%		- Detecting correlated amino acid changes in pairs of positions. Residue co‐evolution was originally assessed through detecting pairs of positions (two columns of the MSA) that have interdependent amino acid frequencies23 or similar patterns of amino acid substitutions7,9,10 \cite{de2013emerging}
%		- ...can be assessed by a linear correlation. This method has been extensively tested and compared with newer methods and shows a small but significant capability to recover pairs of positions in physical contact24 and still serves as a baseline to benchmark the performance of new methods25.  \cite{de2013emerging}
%		- CAPS dampens the influence of background phylo‐ genetic divergence by requiring the detected correlations to still be detected after particular clades are removed from the MSA. It also corrects the amino acid substi‐ tution matrix so as to consider the actual divergence among the sequences \cite{de2013emerging}
%		- MI: Mutual information has been also used to detect co‐varying positions. Whereas correlation‐based meth‐ ods explore inter‐sequence amino acid substitutions, mutual information considers the distribution of each amino acid in the different sequences for a position. In fact, mutual information quantifies whether the pres‐ ence of an amino acid in a given sequence for a posi‐ tion is a ‘good prediction’ of the presence of any given amino acid in the same sequence for a second position. In this sense, mutual information does not account for which particular amino acids are present in the same sequences in both positions but relies on the statistical significance of the observed co‐variations. Therefore, the different amino acids are treated as different sym‐ bols that are not related by similarity relationships, and the magnitude of the biochemical changes is not taken into account when assessing the similarity of mutational patterns. \cite{de2013emerging}
%		- MARKOV MODELS: In this case, the use of an enhanced continuous-time Markov process model for sequence co‐evolution represented an important step forwards13. These approaches are suit‐ able for small‐scale studies of co‐evolution in small pro‐ tein families, but the evaluation of their performance in large‐scale studies remains excessively demanding in computational terms. \cite{de2013emerging}
%		- Disentangling directly coupled residues from the net‐ work of indirectly correlated positions.  \cite{de2013emerging}
%			- An important obstacle in the detection of co‐evolving positions is the apparent co‐variation or indirect coupling that can occur when more than two positions show coordinated substitution patterns. In these cases, the apparent co‐ variation between two positions is the consequence of the evolutionary interdependence of both positions with one or more additional positions. The aggrega‐ tion of these indirect couplings can make it difficult to recognize the directly interdependent positions.
%			- As the direct couplings are more reliable for predict‐ ing physically proximal residues in protein structures, approaches are needed to distinguish direct from indi‐ rect couplings \cite{de2013emerging}
%			- A first basic model was proposed by Lapedes et al.37, who assumed that indirect couplings do not represent evolutionary interdependence and can be considered to be uninformative pairwise co‐variations. This first approach used a Monte Carlo algorithm to infer the sim‐ plest probabilistic model that was able to account for the whole network of co‐variations in a simulated sce‐ nario.  \cite{de2013emerging}
%			- Direct coupling analysis (DCA)15–17,38 and protein sparse inverse covariance (PSICOV)39 establish a global statistical model of the MSA in terms of position‐specific variability and inter‐position coupling \cite{de2013emerging}
%			- Alternatively, Burger and van Nimwegen’s41 method uses a Bayesian network model that includes pair‐ wise conditional dependencies, and the regularized multinomial regression‐based correlated mutations (RMRCM) approach42 takes into account the whole network of dependencies and not only the individual pairwise dependencies. \cite{de2013emerging}
%			- For MSAs with more than 1,000 sequences, DCA and PSICOV seem to be superior to Burger and van Nimwegen’s method38,39. \cite{de2013emerging}
%			- In fact, some of these methods are able to predict contacts between residues far apart in the linear sequence with sufficient accuracy as to be useful for guiding in silico folding experiments (BOX 1). Nevertheless, such clear improvements are obtained only for protein families with thousands of members \cite{de2013emerging}
%			- Potential co‐evolution between functionally related pro‐ tein families was initially observed in sporadic cases. For example, remarkable similarity was detected between the phylogenetic trees of ligands (such as insulins and interleukins) and their receptors; this co‐evolution was proposed to be required for the maintenance of their specific interactions2 \cite{de2013emerging}
%		- LIMITATIONS: \cite{de2013emerging}
%			- The quality of MSAs is obviously essential as they serve as the initial input to most of the methods. Furthermore, the methods work better on large protein families for which the degree of sequence similarity has a wide but homogenously dis‐ tributed range from distant to similar sequences \cite{de2013emerging}
%			- In general, optimal performance is obtained when protein subfamilies (branches of the tree) are spaced at regular intervals, \cite{de2013emerging}
%			- For example, assembling phylogenetic trees is confounded by complex evolutionary scenarios, such as sequences acquired by horizontal gene transfer, genetic saturation or the difficulties in identifying the correct orthologous sequences when genome duplication and domain rear‐ rangements have occurred. \cite{de2013emerging}

%------------------------------------------------------------
% From: Multidimensional mutual information methods for the analysis of covariation in multiple sequence alignments
% \cite{clark2014multidimensional}
%
%		- (multidimensional) extension of traditional mutual information (MI) can be an additional tool to study covariation \cite{clark2014multidimensional}
%		- as tested with a set of 9 MSAs each containing <400 sequences, and was shown to be comparable to that of the newest methods based on maximum entropy/pseudolikelyhood statistical models of protein sequences.  \cite{clark2014multidimensional}
%		- METHOD COMPARISSON: However, while all the methods tested detected a similar number of covarying pairs among the residues separated by < 8 Å in the reference X-ray structures, there was on average less than 65% overlap between the top scoring pairs detected by methods that are based on different principles. \cite{clark2014multidimensional}
%		- Unfortunately, the reliability of covariation data can be diminished by the existence of correlations originating not just from the direct interactions (physical or functional) between two residues, but also from their shared inter- action with one or more other residues, and by the shared phylogenetic history of several homologous proteins in the MSA.  \cite{clark2014multidimensional}
%		- While the performance of these methods has been tested primarily with high quality MSAs containing a very large number of sequences (between 5L and 25L, with L=sequence length), very often investigators are interested in studying the covarying positions of proteins for which the available MSA contains less than L sequences, and whose alignment quality is not optimal due to the presence of many (or large) gaps, \cite{clark2014multidimensional}
%		- METHOD:  \cite{clark2014multidimensional}
%			- We can consider a more complicated case including a third channel (column). In this case, I(X1;X3;X2) between the three variables repre- sents the ‘interaction information’ for a channel with two discrete inputs X1 and X3 and a single discrete output X2 (a 2-way channel).  \cite{clark2014multidimensional}
%			- If we are in- terested in ‘explaining out’ the effect of X3 on the transmission between X1 and X2, we can take a sum of the mutual information I(X1;X2) for each possible value x3 of X3, weighted by the probability of occurrence (px3) of each of those values: \cite{clark2014multidimensional}
%			- Averaging over all values of X3 (a 3rd column) in an MSA we obtain for the 3-dimensional MI between any two columns (X1 and X2): \cite{clark2014multidimensional}
%		- LIMITATIONS: Due to the long execution times and large memory requirements (growing with the 4th power of the sequence length) of 4D_MI only the removal of 3rd order indirect coupling (3D_MI) is practical with desktop computers for MSAs of sequences longer than 200 residues. \cite{clark2014multidimensional}
%		- COMPARISSON: \cite{clark2014multidimensional}
%			- We have evaluated the performance of standard MI (2D_MI), 3D_MI, 4D_MI, PSICOV [14], plmDCA [17], GREMLIN [18], and Hopfield-Potts_DCA with Princi- pal Component Analysis [19] (called here hpPCA) with the MSAs of 9 protein families \cite{clark2014multidimensional}
%			- These MSAs contain less than 400 sequences with ratios of sequence number to sequence length (called here the ‘L ratio’) between 0.4 and 2.0, and thus represent a particularly sensitive test for the performance of the different methods with less than optimal size MSAs.  \cite{clark2014multidimensional}
%			- all the methods tested produced covariation maps that closely resembled the contact maps derived from the representative X-ray structures of each family \cite{clark2014multidimensional}
%			- While all the methods used in this study performed quite well in terms of percentage of close contacts recog- nized among the top covarying pairs, they did not neces- sarily recognize the same close contacts, as no more than 50% of all the pairs were shared between the MI/mdMI based methods and the other methods \cite{clark2014multidimensional}
%			- Finally, since there is < 65% overlap among the sets of covarying residues identified by algorithms based on different principles, further improvement in accuracy is likely to be obtained by selecting only the shared pairs or by averaging the results from different methods. \cite{clark2014multidimensional}


%------------------------------------------------------------
% From: Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction		(2007, cited 155)
% \cite{dunn2008mutual}
%
%		- HISTORICAL: (Fitch et al., 1970; Yanofsky et al., 1964). \cite{dunn2008mutual}
%		- How, then, do non-conserved positions change during evolution? It is believed that mutations in these positions can occur because they are either accompanied or preceded by compensatory changes in other variable positions (Fitch et al., 1970; Yanofsky et al., 1964). \cite{dunn2008mutual}
%		- In the context of multiple sequence alignments, MI is an attractive metric because it explicitly measures the dependence of one position on another, but its usefulness has been limited by three factors.  \cite{dunn2008mutual}
%			- 1) First, positions with higher variability, or entropy, will tend to have higher levels of both random and nonrandom MI than positions of lower entropy (Fodor and Aldrich, 2004a; Martin et al., 2005), even though the latter are more constrained and would seem more likely to depend on neighboring positions.  \cite{dunn2008mutual}
%			- 2) Second, random MI arises because the alignments do not contain enough sequences for background noise to be negligible; our previous modeling studies showed that alignments should contain at least 125 sequences before the random signal begins to subside relative to non-random MI (Martin et al., 2005).  \cite{dunn2008mutual}
%			- 3) A third complicating factor is that all position pairs have MI due to the phylogenetic relationships of the organisms represented in the alignment (Wollenberg and Atchley, 2000). This latter source may be limited to some degree by excluding highly similar sequences from closely related species from the alignment, but cannot be eliminated (Martin et al., 2005; Tillier and Lui, 2003). Each of these sources of MI will tend to obscure the desired signal based on the structural or functional relationships of positions. \cite{dunn2008mutual}
%		- METHOD:  \cite{dunn2008mutual}
%			- MI measures the reduction of uncertainty about one position given information about the other  \cite{dunn2008mutual}
%			- Thus the challenge is to separate the signal caused by structural and functional constraints, MIsf, from the background, MIb, which is the sum of contributions from random noise and shared ancestry. \cite{dunn2008mutual}
%			- we postulated that each position in a multiple sequence alignment may have a particular propensity toward MIb, that is related to its entropy and phylogenetic history, and that the MIb between any two positions is the product of their propensities. It then follows that MIb for positions a and b may be expressed as the product of the average MIb values of positions a and b with all other positions in the set, divided by the average MIb of all positions in the set. We call this term the average product correction, (APC), \cite{dunn2008mutual}
%		- We determined how different a given covariance value was relative to all other values in the data set. The mean and SD of the values determined by each of the algorithms were calculated for all pairs of positions. The number of SD from the mean, i.e. the Z-score, was determined for each value or for each corrected value in a given data set \cite{dunn2008mutual}
%		- A number of obstacles, including random noise, the influence of entropy, the phylogenetic history and the number of sequences required, complicate the identification of coevolving positions in multiple sequence alignments when using MI \cite{dunn2008mutual}
%		- We have taken a different approach and developed a correction that rapidly and accurately estimates the back- ground MI found in protein family multiple sequence align- ments. Our method was initially based on the assumptions that the coevolution signal between pairs of unrelated positions is derived from random noise or from shared ancestry but not from structural or functional constraints;  \cite{dunn2008mutual}
%		- We have shown that the APC accurately estimates MI in the absence of structural or functional relationships. Furthermore, in real protein alignments the subtraction of the APC from MI results in a metric, MIp, that is independent of the entropy of the positions, and that provides a significant improvement over previously published methods in identifying co-evolving posi- tions that are proximal in protein structure. \cite{dunn2008mutual}
%		- We have also mathematically demonstrated the validity of the APC correction. \cite{dunn2008mutual}


%------------------------------------------------------------
% From: Protein structure prediction from sequence variation
% \cite{marks2012protein}
%
%		- Protein folding:
%			- This information can be efficiently mined to detect evolutionary couplings between residues in proteins and address the long-standing challenge to compute protein three-dimensional structures from amino acid sequences. \cite{marks2012protein}
%			- We expect computation of covariation patterns to complement experimental structural biology in elucidating the full spectrum of protein structures, their functional interactions and evolutionary dynamics. \cite{marks2012protein}
%			- In the past 50 years, there has been tremendous progress in experimen- tal determination of protein three-dimensional structures, but this has not kept pace with the explosive growth of sequence information that results from massively parallel sequencing technology.  \cite{marks2012protein}
%			- Computational prediction of protein structures, which has been a long-standing challenge in molecular biology for more than 40 years, may be able to fill this gap, if done with sufficient accuracy.  \cite{marks2012protein}
%			- However, correct de novo predictions from sequence, when not a single structure in a protein family is known, have been hard to achieve, \cite{marks2012protein}
%			- Clearly, and unfortunately, the de novo structure prediction problem does not scale13, the conformational search space increases exponentially as the size of the protein increases, presenting a funda- mental computational challenge, even for fragment-based methods14. In this sense, the general problem of de novo three-dimensional structure prediction has remained unsolved. \cite{marks2012protein}
%			- A substantial step forward in protein-structure prediction is now on the horizon based on the power of evolutionary information found in patterns of correlated mutations in protein sequences  \cite{marks2012protein}
%			- Several groups have demonstrated that extracting covariation information from sequences is sufficient not only to estimate which pairs of residues are close in three-dimensional space15–21 but also to fold a protein to reasonable accuracy15,22–25 \cite{marks2012protein}
% \cite{marks2012protein}
%		- Indirect correlations: \cite{marks2012protein}
%			- if residues A and B contact each other, as do residues B and C, then there is in gen- eral, a transitive influence observed between residues A and C (‘chain- ing effect’17,27). \cite{marks2012protein}
%			- As residues can contact many other residues (not just one), transitive effects occur across the network, and pairs of residues that are correlated as computed using a ‘local’ statistical model, such as mutual information scores, are not necessarily functionally constrained or close in space \cite{marks2012protein}
%		- LOCAL MODELS: \cite{marks2012protein}
%			- Local statistical models (below referred to as local models or local methods) assume that pairs of residue positions are statistically independent of other pairs of residues  \cite{marks2012protein}
%			- Other confounding effects that have prevented high-accuracy prediction of residue contacts include uneven representation of family members in sequence space, statistical- noise as the result of an inadequate number of sequences in the family as well as phylogenetic effects. \cite{marks2012protein}
%		- GLOBAL MODELS \cite{marks2012protein}
%			- In contrast, a ‘global’ modeling approach treats correlated pairs of resi- dues as dependent on each other, rather than as statistically independent, thereby minimizing the effects of transitivity and spurious noise. \cite{marks2012protein}
%			- This approach also uses globally consistent single-residue marginals, which takes into account effects from conservation of single residue positions. Global approaches yield high coupling scores only for pairs or residue positions that are likely to be causative of all the observed correlations. \cite{marks2012protein}
%			- Noncausal correlation is well understood in statistical physics; it includes, for instance, long-range order observed in spin systems, where in fact the spins only have short-range direct interactions, and is called ‘chained covariation’27,34.  \cite{marks2012protein}
%			- One global statistical approach is known as entropy maximization under data constraints, a classic inference method connecting information the- ory and Boltzmann statistics 35 \cite{marks2012protein}
%			- Maximizing entropy under constraints36 has been successfully used in statistical physics and other areas of statistical inference37–39, and the conditional mutual information derived from correlations between positions in a protein sequence is a discrete, nonlinear analog of partial correlation analysis40 \cite{marks2012protein}
%			- In contrast to simple mutual information, the conditional mutual information can be thought of as the degree of covariation between residues at positions a and b that is due solely to direct effects of a on b, factoring out contri- butions to the correlation that are caused by interaction of both a and b with the rest of the network of residues. \cite{marks2012protein}
%			- CORRELATION (PSI_COV) the covariance matrix (the observed minus expected pair counts) of dimension (20L)2, where L is the length of the protein sequence, by counting how often a given pair of the 20 amino acids, say alanine and lysine, occurs in a particular pair of positions, say position 15 and 67, in any one sequence, summing over all sequences in the multiple-sequence alignment. This large matrix contains the raw data capturing all residue pair relationships across evolution up to second order (pairs, not triplets or higher). One can then compute a measure of causative correlations, the conditional mutual information, in the global statistical approaches by taking the inverse of the covariance matrix. That such a matrix inver- sion results in a measure of causative correlations is well known in the statistical theory of Gaussian multivariate distributions of continuous variables40. \cite{marks2012protein}
%			- MEAN FIELD APPROX: An analogous derivation for discrete-state biological sequence analy- sis is, for example, based on a mean-field expansion in analogy to statisti- cal physics16. The resulting explicit probability model for a sequence in the particular protein family resulting from inversion of the covariation matrix contains numerical estimates of direct pair interactions. These are directly and simply computed from the raw data in the covariation matrix, in contradistinction to machine-learning methods that rely on parameter fitting in learning sets and cross-validation in test sets. The pair interaction terms can also be interpreted as residue-residue pair energies, in analogy to pair terms in a Hamiltonian energy expression in statistical physics. The conditional mutual information between a pair of positions derived using the global statistical approach becomes a useful predictor of residue-residue contacts. \cite{marks2012protein}
%			- The maximum-entropy approach to potentially solving the problem of protein structure prediction from residue covariation patterns was first described by Lapedes and collaborators17,27. However, instead of inversion of the covariance matrix, they used a more computationally demanding Monte Carlo method (that is, iterative exploration of the best set of pair interactions values) to derive the probability terms in conditional mutual information. Although Lapedes and Jarzynski did not compute three-dimensional structures, they reached a first break- through in contact prediction in 2002 for 11 small proteins and reported 50–70% accuracy for top 20 contact predictions, in contrast to 35–45% accuracy with the previous best methods available17. \cite{marks2012protein}


%------------------------------------------------------------
% From: PSICOV: precise structural contact prediction using sparse inverse covariance estimation on large multiple sequence alignments
% \cite{jones2012psicov}
%
%		-  It has long been observed that with sufficient correct information about a protein’s residue–residue contacts, it is possible to elucidate the fold of the protein (Gobel et al., 1994
%		- The underlying rationale rests on the fact that any given contact critical for maintaining the fold of a protein will constrain the physicochemical properties of the amino acids involved. Should a given contacting residue mutate and potentially perturb the properties of the contact, then its contacting partner will be more likely to mutate to a physicochemically complementary amino acid residue, to ensure the native fold of the protein remains stabilized.
%		- Turning this observation around, pairs of residues seen to co- evolve in tandem and thus preserving their relative physiochemical properties, are likely candidates to form contacts. 
%		- The starting point of our method is to consider an alignment with m columns and n rows,
%		- where each row represents a different homologous sequence and each column a set of equivalent amino acids across the evolutionary tree, with gaps considered as an additional amino acid type. We can compute a 21m by 21m sample covariance matrix as follows:
%		- Any individual element of this matrix gives the covariance of amino acid type a at position i with amino acid type b at position j.
%		- By calculating the matrix inverse of the covariance matrix, the precision or concentration matrix (􏰩) is obtained, from which a matrix of partial correlation coefficients for all pairs of variables can be calculated as follows
%		- In the simplest case, a partial correlation coefficient can be calculated between two random variables with the controlling effect of a third random variable taken into account. The partial correlation matrix above, however, gives the correlations between all pairs of variables with the controlling effects of all other variables taken into account 
%		- Thus, assuming the sample covariance matrix can in fact be inverted, the inverse covariance matrix provides information on the degree of direct coupling between pairs of sites in the given MSA. Off-diagonal elements of the inverse covariance matrix which are significantly different from zero are indicative of pairs of sites which have strong direct coupling (and are likely to be in direct physical contact in the native structure).
%		- Unfortunately, the empirical covariance matrices produced in this application are guaranteed to be singular due to the fact that not every amino acid will be observed at every site
%		- Although different approaches have been proposed to allow inverse covariance estimation where the sample covariance matrix cannot be directly inverted, one of the most powerful techniques is that of sparse inverse covariance estimation.
%		- In general terms, where an inverse covariance estimate is constrained to be sparse, the non-zero terms tend to more accurately relate to correct positive correlations in the true inverse covariance matrix
%		- The graphical Lasso is a statistical method which estimates the inverse covariance of the data by minimizing the objective function:...
%		- For 44% of the targets, contact prediction was excellent with a precision >0.5 for the longest-range top-L/2 predicted contacts (i.e. >50% correctly predicted long-range contacts per residue). 


%------------------------------------------------------------
% From: Recent Coselection in Human Populations Revealed by Protein--Protein Interaction Network			(2014, cited 0)
% \cite{qian2015recent}
%
%		- Genome-wide scans for signals of natural selection in human populations have identified a large number of candidate loci that underlie local adaptations. This is surprising given the relatively short evolutionary time since the divergence of the human population. Approximately 50,000–100,000 years ago, the anatomically modern human migrated from Africa to the rest of the globe
%		-  One hypothesis that has not been formally examined is whether and how the recent human evolution may have been shaped by coselection in the context of complex molecular interactome
%		- In this study, genome-wide signals of selection were scanned in East Asians, Europeans, and Africans using 1000 Genome data, and subsequently mapped onto the protein–protein interaction (PPI) network
%		- We found that the candidate genes of recent positive selection localized significantly closer to each other on the PPI network than expected
%		- Furthermore, gene pairs of shorter PPI network distances showed higher similarities of their recent evolutionary paths than those further apart. 
%		- Several hundred to more than one thousand candi- date regions, which may have undergone recent positive se- lection, have been reported in these studies (Sabeti et al. 2002; Voight et al. 2006; Tang et al. 2007; Akey 2009; Pickrell et al. 2009). The abundant selection signals are in sheer contrast with the relatively short period of time since humans migrated from Africa.
%		- EXAMPLES: A number of positively selected candidate genes in the same pathways or interaction subnetworks have also been identified. Known examples are EGLN1 and EPAS1 in the hypoxia-response pathway playing key roles in genetic adaptation to high-altitude regions (Xu et al. 2011) as well as multiple genes in the NRG-ERBB4 developmental pathway (Pickrell et al. 2009)
%		- Coevolution of interacting proteins, in large time frames, has been intensively studied and is typically based on the evo- lutionary distances across different species 
%		- an alternative mechanism notes that epistatic interaction is not compulsory to explain the associated evolutionary patterns. If selection pressures act on an entire pathway or a functional subnetwork, multiple genes in the same pathway/subnetwork may change in the same fitness direction, and at a same evolutionary rate and time to achieve a common phenotypic outcome. 
%		- the association in evolutionary patterns may simply reflect parallel selection of different genes in the same pathway of shared functionality 
%		- Nonetheless, both hypothetic mechanisms would lead to a set of similar predictions: first, the genes of positive selection would cluster closer to each other in the PPI network than predicted under null hypothesis; second, the clustered genes of selection may share more similar evolutionary paths than genes unrelated on the PPI network.
%		- Numerous studies reported that proteins located closer to the center of PPI network evolved more slowly than those at the periphery of the network, consistent with the view that central proteins are more essential and receive greater evolutionary constraints
%		- DATA: 1000 Genomes Project interim phase 1 data set, including Europeans (CEU; 85 samples), Yorubans (YRI; 88 samples), and EAS (186 samples CHB+JPT). T
%		- METHOD: 
%			- A modified CMS method was applied to scan for genome- wide signals of recent positive selection, as previously de- scribed (Grossman et al. 2010).
%			- Degree centrality (DC) and betweenness centrality (BC) were used to measure both local and global topological positions of candidate genes on the human PPI network (Freeman 1977; Kim et al. 2007). Degree is defined as the number of connections a node has with its neighbors whereas betweenness quantifies the number of times a node acts as a bridge along the shortest path between any other pairwise nodes. 
%			- The Mann–Whitney U test (also called Wilcoxon rank sum test) was applied to compare the two centrality measures, BC and DC, between selection signals and nonselection signals, to determine whether network position influences recent posi- tive selection
%		- RESULTS
%			- we found a moderate pattern that recent positive selections tended to occur more on the subcentral region of the PPI network,
%			-  However, it differs from studies done on a macroevo- lutionary timescale, which have consistently reported that ac- celerated evolutionary rates tend to happen at the periphery of the protein interaction network,


%------------------------------------------------------------
% From: Using sequence alignments to predict protein structure and stability with high accuracy		(ORIGINALLY WRITTEN IN 2002, not 2012!)
% \cite{lapedes2012using}
%
%		- We present a sequence-based probabilistic formalism that directly addresses co-operative effects in networks of interacting positions in proteins, providing significantly improved contact prediction
%		- Each sequence of length L of a given family can be viewed as a different global state of an L-site, twenty-state (for twenty amino acids) spin system, with spin- spin (i.e. residue-residue) interactions determined by (1) the (unknown) structure of the associated fold, and (2) the physico-chemical characteristics of the residues
%		- Solving the inverse problem to determine the underlying physical interactions addresses “correlation at a distance”, in which correlations between locally connected sites in an interacting network such as a spin system
%		- Previous computational work on abstract models of proteins [4], as well as a statistical analysis of the frequency of ion-pairs in crystal structures of real proteins [5], provided early hints that Boltzmann-like statistics are associated with aspects of protein architecture.
%		- The Boltzmann network method presented here does not treat each individual pair of sites of interest as isolated from other residues. Instead, we construct a probability distribution describing full length sequences of length L for each protein sequence family.
%		- Any given sequence alignment typically contains enough data to estimate only single and pairwise amino acid frequencies with reasonable accuracy.
%		- The maximum entropy distribution whose moments match a given set of single and pairwise amino acid frequencies may be written in the following form [23], reminiscent of thermal Boltzmann statistics
%		- It can be shown [25] that matching the moments of the maximum entropy distribution to the given sequence data is equivalent to maximizing the loglikelihood of the given sequence data given the parametric form
%		- we use the probability distribution over all L sites, Eqns. (1,2), to resolve issues of correlation at a distance (network effects) in proteins, resulting in significantly improved contact prediction from sequence information
%		- LIMITATIONS: Limiting factors in application of the Boltzmann network algorithm include (1) the amount of naturally evolved sequence data currently available per family (size of the sequence alignment), and (2) the phylogenetic relatedness (and associated selection arti- facts) of these sequences. Modifications to the algorithm presented here, e.g. (1) consider- ation of statistical significance of the fitted λ parameters, and (2) addressing phylogenetic relationships of sequences in an alignment, have the potential to further increase accuracy using naturally evolved sequence sets.

  
\subsection{CoEvolutionary models}


\subsection{Epistatic GWAS \label{sec:epigwas}}

% GWAS & Epistasis
Genome wide association studies have traditionally focused on single variants or nearby groups of variants. An often cited reason for the lack of discovery of high impact risk factors in complex disease is that these models ignore loci interactions \cite{cordell2009detecting} which have recently been pointed out as a potential solution for the ``missing heritability" problem \cite{zuk2012mystery, zuk2014searching}. With interactions being so ubiquitous in cell function, one may wonder why they have been so neglected by GWAS. There are several reasons: i) models using interactions are much more complex \cite{gao2010classification} and by definition non-linear, ii) information on which proteins interacts with which other proteins is incomplete \cite{venkatesan2009empirical}, iii) in the cases where there protein-protein interaction information is available, precise interacting sites are rarely known \cite{venkatesan2009empirical}. Taking into account the last two items, we need to explore all possible loci combinations, thus the number of $N$ order interactions grows as $O(M^N)$ where $M$ is the number of variants \cite{de2013emerging}. This requires exponentially more computational power than single loci models. This also severely reduces statistical power, which translates into requiring larger cohort, thus increasing sample collection and sequencing costs \cite{de2013emerging}.

In Chapter \ref{ch:gwas} we develop a computationally tractable model for analysing putative interaction of pairs of variants from GWAS involving large case / control cohorts of complex disease. Our model is based on analysing cross-species multiple sequence alignments using a co-evolutionary model in order to obtain informative interaction prior probabilities that can be combined to perform GWAS analysis of pairs of non-synonymous variants that may interact.

%------------------------------------------------------------
% From: Test for Interaction between Two Unlinked Loci (2006)
% \cite{zhao2006test}
% TOPIC: GWAS EPISTASIS
%
%		- often been defined as a deviance from genetic additive effects, which is essentially treated as a residual term in genetic analysis and leads to low power in detecting the presence of interacting effects
%		- We developed a general theory for studying linkage disequilibrium (LD) patterns in disease population under two-locus disease models. 
%		- Our results showed that the P values of the LD-based statistic were smaller than those obtained by other approaches, including logistic regression models.
%		- This was further developed by Cockerham4 and Kempthorne5 into the modern representation that treats statistical gene interactions as interaction terms in a re- gression model or a generalized linear model on allelic effects.2,6–11
%		- we propose to define interaction between two unlinked loci (or genes) for a qualitative trait as the deviance of the penetrance for a haplotype at two loci from the product of the marginal penetrance of the individual alleles that span the haplo- type. 
%		- DEFINE: Deviance
%		- Interaction between two unlinked loci will result in deviation of the penetrance of the two-locus haplotype from indepen- dence of the marginal penetrance of the alleles at an in- dividual locus, which in turn will create linkage disequi- librium (LD) even if two loci are unlinked. 
%		- Therefore, it is possible to de- velop statistics for detection of interaction between two unlinked loci by use of deviations from LD
%		- we assume that two disease-susceptibility loci are in Hardy-Weinberg equilibrium (HWE) and are unlinked. 
%		- [they show that] Under this definition, in the absence of interaction, two unlinked loci in the disease population will be in linkage equilibrium
%		- Similar to linkage equilibrium, where the frequency of a haplo- type is equal to the product of the frequencies of the component alleles of the haplotype, absence of interaction between two un- linked loci implies that the proportion of individuals carrying a haplotype in the disease population is equal to the product of the proportions of individuals carrying the component alleles of the haplotype in the disease population
%		- TEST STATISTIC: 
%			- Intuitively, we can test interaction by comparing the difference in the LD levels between two unlinked loci between cases and controls
%			- We can show that test statistic TI is asymptotically distributed as a central x2 distribution under the (1) null hypothesis of no interaction between two unlinked loci 
%		- we compared the power of the LD-based statistic with that of the logistic model. 
%		- Power comparison with logistic regression analysis demonstrated that this LD-based test statistic has much higher power in detecting interaction than does the logistic regression method.
%		- To further evaluate its performance for detection of in- teraction between two loci, the proposed LD-based statis- tic was applied to two published data sets. Our results showed that, in general, P values of the test statistic TI were much smaller than those of other approaches, in- cluding logistic regression analysis.
%		- IF THIS METHOD IS "BETTER", WHY DON'T WE JUST USE IT !?!?!


%------------------------------------------------------------
% From: Detecting gene–gene interactions that underlie human diseases (Nature, 2009)
% \cite{cordell2009detecting}
%	Topic: GWAS EPISTASIS (METHODS REVIEW)
%
%		-  Following the identification of several disease-associated polymorphisms by genome-wide association (GWA) analysis, interest is now focusing on the detection of effects that, owing to their interaction with other genetic or environmental factors, might not be identified by using standard single-locus tests
%		- ...it is hoped that detecting interactions between loci will allow us to elucidate the biological and biochemical pathways that underpin disease. 
%		- In recent years, the field has been revolution- ized by the success of genome-wide association (GWA) studies1–5. Most of these studies have used a single-locus analysis strategy, in which each variant is tested individu- ally for association with a specific phenotype
%		- However, a reason that is often cited for the lack of success in genetic studies of complex disease6,7 is the existence of interac- tions between loci. 
%		- If a genetic factor functions primarily through a complex mechanism that involves multiple other genes and, possibly, environmental factors, the effect might be missed if the gene is examined in isola- tion without allowing for its potential interactions with these other unknown factors.
%		- The purpose of this Review is to provide a survey of the methods and related software packages that are cur- rently being used to detect the interactions between the genetic loci that contribute to human genetic disease.
%		- Interaction as departure from a linear model. The most common statistical definition of interaction relies on the concept of a linear model that describes the relationship between an outcome variable and a predictor variable or variables
%		- Arguably the most well-known form of this type of analysis is simple linear or least squares regression26, in which we relate an observed quantitative outcome y (for example, weight) to a predictor variable x (for example, height) using a ‘best fit’ line or regression
%		- From a statistical point of view, interaction repre- sents departure from a linear model that describes how two or more predictors predict a phenotypic outcome
%		- For a disease outcome and case–control data, rather than modelling a quantitative trait y, the usual approach is to model the expected log odds of disease as a linear function of the relevant predictor variables
%		- DEFINITION Penetrance: The probability of displaying a particular phenotype (for example, succumbing to a disease) given that one has a specific genotype.
%		- DEFINITION: Marginal effects: The average effects (for example, penetrances) of a single variable, averaged over the possible values taken by other variables. These could be calculated for one locus of a two-locus system as the average of the two-locus penetrances, averaged over the three possible genotypes at the other locus.
%		- For or simplicity, I have concentrated here on defining interaction in relation to two genetic factors (two-locus interactions). In practice, however, for complex diseases we might also expect three-locus, four-locus and even higher-level interactions. Mathematically, such higher- level interactions are simple extensions to the two-locus models described earlier. 
%
%		- CASE ONLY METHODS:
%			- A case-only test of interaction can therefore be performed by testing the null hypothesis that there is no correlation between alleles or genotypes at the two loci in a sample that is restricted to cases alone. This test can easily be performed using a simple χ2 test of inde- pendence between genotypes (a four degrees of freedom test) or alleles (a one degree of freedom test), or using logistic or multinomial regression in any statistical analysis package.
%			- The main problem with the case-only test is its requirement that the genotype variables are not cor- related in the general population. It is this assumption, rather than the design per se, that provides the increased power compared with case–control analysis
%			- The case- only test is therefore unsuitable for loci that are either closely linked or show correlation for another reason (for example, if certain genotype combinations are related to viability).
%		- Tests for association allowing for interaction: From a mathematical point of view, a test for association at a given locus C while allow- ing for interaction with another locus B (a joint test16) corresponds to comparing the fit to the observed data of a linear model in which the main effects of B, C and their interactions are included 
%		- Theoretically, if no interaction effects exist, these joint tests will be less powerful than marginal single- locus association tests. However, if interaction effects exist, then the power of joint tests can be higher than that of single-locus approaches52.
%
%		- CLASSIFICATION TREE: Recursive partitioning approaches are based on classification and regression trees111. Trees are constructed (see the figure) using rules that determine how wella split at a node (based on the values of a predictor variable such as a SNP) can differentiate observations with respect to the outcome variable (such as case–control status). A popular splitting rule is to use the variable that maximizes the reduction in a quantity known as the Gini impurity111,112 at each node. 
%
%		- RAMDOM FOREST: A random forest is constructed by drawing with replacement several bootstrap samples of the same size (for example, the same number of cases and controls) from the original sample. An unpruned classification tree is grown for each bootstrap sample, but with the restriction that at each node, rather than considering all possible predictor variables, only a random subset of the possible predictor variables is considered. This procedure results in a ‘forest’ of trees, each of which will have been trained on a particular bootstrap sample of observations.
%
%		- BAYESIAN MODEL SELECTION: Bayesian model selection techniques92 offer an alterna- tive approach for selecting predictor variables and the interactions between them that are the best predictors of phenotype. The key difference between Bayesian model selection and simple comparisons of nested regression models using frequentist (non-Bayesian) procedures is the specification of prior distributions for the unknown regression parameters as well as for a dimension param- eter in a Bayesian approach. This dimension parameter specifies how many non-zero predictors are included
%			- A posterior distribution for these parameters, given the observed data, can then be calculated using Markov chain Monte Carlo (MCMC)93 simulation techniques, in which one traverses the space of the possible models (sets of parameter values), sam- pling the outputs of the simulation run at intervals. Although MCMC is a flexible approach, it can require some care with respect to the choice of prior distribu- tions, proposal schemes (determining how one moves between models) and the number of iterations required to achieve convergence.
%			- BEAM: Bayesian Epistasis Association Mapping. A recently proposed MCMC approach that is specifically designed to detect interacting, as well as non-interacting, loci is Bayesian epistasis Association Mapping13, which is implemented in the software package BeAM. In BeAM, predictors in the form of genetic marker loci are divided into three groups: group 0 contains markers that are not associated with disease, group 1 contains markers that contribute to disease risk only by main effects and group 2 contains markers that interact to cause disease by a satu- rated model. Given prior distributions that describe the membership of each marker in each of the three groups and prior distributions for the values of the relevant regres- sion coefficients given group membership, a posterior distribution for all relevant parameters can be generated using MCMC simulation. In addition to making infer- ences in a fully Bayesian inferential framework, one can use the results from BeAM in a frequentist hypothesis- testing framework by calculating a ‘B-statistic’13 that tests each marker or set of markers for significant association with a disease phenotype.
%			- EBAM LIMITATIONS: BeAM cannot currently handle the 500,000–1,000,000 markers that are now routinely being genotyped in genome scans of 5,000 or more individuals.


%------------------------------------------------------------
% From: Detecting epistatic effects in association studies at a genomic level based on an ensemble approach (2011)
% \cite{li2011detecting}
%	Topic: A GWAS EPISTASIS METHOD
%
%		- We extend the basic AdaBoost algorithm by incorporating an intuitive importance score based on Gini impurity to select candidate SNPs. 
%		- Permutation tests are used to control the statistical significance.
%		- We have performed extensive simulation studies using three interaction models to evaluate the efficacy of our approach at realistic GWAS sizes, and have compared it with existing epistatic detection algorithms.
%		- CURRENT METHODS: Generally speaking, existing approaches for searching gene– gene or SNP–SNP interactions can be grouped into four broad categories.
%			- 1) Methods in the first category rely on exhaustive search. Classical statistics such as the Pearson’s χ2 test or the logistic regression that are commonly used as single-locus tests for GWAS can potentially be used in searching for pairwise interactions. Marchini et al. (2005) have shown that explicitly modeling of interactions between loci for GWAS with hundreds of thousands of markers is computationally feasible. They also showed that these simple methods explicitly considering interactions can actually achieve reasonably high power with realistic sample sizes under different interaction models with some marginal effects, even after adjustments of multiple testing using the Bonferroni correction.
%			- 2) The second category consists of methods relying on stochastic search, with BEAM (Zhang and Liu, 2007) as one representative of such algorithms. Later algorithms in this category [e.g. epiMODE (Tang et al., 2009)] largely adopted and extended BEAM. BEAM uses Markov chain Monte Carlo (MCMC) sampling to infer whether each locus is a disease locus, a jointly affecting disease locus, or a background (uncorrelated) locus. The algorithm begins by assigning each locus to each group according to a prior distribution. Using the Metropolis–Hastings algorithm, it attempts to reassign the group labels to each locus. At the end, it uses a special statistic, called the B-Statistic, to infer statistical significance from the hits sampled in MCMC. This approach avoids computing all interactions, but can still theoretically find high-order interactions. The number of MCMC rounds is the primary parameter that mediates runtime, as well as power. The suggested number of MCMC rounds is in the quadratic of the number of SNPs, which limits applicability of BEAM on large datasets.
%			- 3) Methods in the third category are machine learning approaches such as tree-based methods or support vector machines (SVM). For example, a popular ensemble approach, Random Forests 
%			- 4) Methods in the forth category rely on conditional search. In such a case, analyses are performed in stages (Evans et al., 2006; Li, 2008). A small subset of promising loci is identified in the first stage, normally using single locus methods, and multi-locus methods are used in the later stage(s) to model interactions based on the selection in the first stage. Stepwise regression has been widely used in this case and several different strategies have been studied in the literature. Methods based on conditional search can greatly reduce the computational burden by a couple of orders of magnitude, but with the risk of missing markers with small marginal effect. One should also notice that the conditional search category is more like a strategy rather than an approach. In addition to single-locus- based methods, any approaches discussed previously, especially the machine learning ones, can be used to search for candidates in the first stage.
%		- THIS METHOD: We extend the basic AdaBoost algorithm by incorporating an intuitive importance score based on Gini impurity to select candidate SNP
%		- Instead of trying to create a monolithic learner or model, ensemble systems attempt to create many heterogeneous versions of simpler learners, called weak learners. The opinions of these heterogeneous experts are then combined to formulate a complete picture of the data. 
%		- Usually, a SNP is selected to ensure largest homogeneity in the child nodes. In our implementation, we use the gain on Gini Impurity. Intuitively, when child nodes have lower impurity from a split based on an attribute (i.e. a SNP here), each child node will have purer classification. Therefore, the genotype frequencies from the two classes (case and control) are expected to be more different. 
%		- Usually decision trees are built with binary splits, where individuals with one value of the feature are placed into one group, and the remainder into the other. Since genotype data is three valued, we extend this to do a ternary split.
%		- Despite only using marginal effects to select SNPs, decision trees can still detect some interaction. Because of the recursive partitioning, lower nodes are effectively conditioned on the value of their parents.
%		- The core idea of AdaBoost is to draw bootstrap samples to increase the power of a weak learner. This is done by weighting the individuals when drawing the bootstrap sample. When a weak learner instance misclassifies an individual, the weight of that individual is increased (and increased more if the weak learner instance was otherwise accurate). Thus, hard to classify individuals are more likely to be included in future bootstrap samples. In the end, the ensemble votes for class labels weighting the weak learner instances by training set accuracy. 


%------------------------------------------------------------
% From: A Review for Detecting Gene-Gene Interactions Using Machine Learning Methods in Genetic Epidemiology (2013)
% \cite{koo2013review}
%	Topic: GWAS EPISTASIS METHODS (MACHINE LEARNING)
%
%		- EPISTASIS TYPES: 
%			- Moreover, there are various types of gene-gene interactions which are synthetic- interaction, epistatic interaction, and suppressive-interaction which are shown in Figure 1. These interactions are partic- ularly important due to the effect of a gene on individual phenotype is depending on more than one additional genes
%			- For instances, synthetic-interaction between two genes is that genes A and B are on different parallel pathways that can obtain the purple phenotype C. If either of the genes is knockout, the purple phenotype C still can be viewed. However, if both of the genes are knockout, it will result in a nonpurple phenotype. 
%			- Next, the example of epistatic-interaction that is the wild type holds a mixed purple and green phenotype of genes C and D. A gene knockout of gene B cannot obtain a purple phenotype of gene C, but green phenotype of gene D still can be seen. A gene knockout of gene A cannot obtain the green and purple phenotypes. 
%			- Furthermore, the example of suppressive-interaction is wild type phenotype showing a purple phenotype since gene A suppresses gene B and gene C is active. A gene knockout of gene B has no effect for result purple phenotype. A knockout of gene A results in a nonpurple phenotype since gene B is still suppressing gene C and if both of the genes A and B are knockout will result in wild type phenotype.
%
%		- ADD ALL METHODS FROM 
%			- Table 1: Summary of detect gene-gene interaction using neural network method.
%				COLUMN 1
%				No. Author
%				(1) Ritchie et al. [11]
%				(2) Tomita et al. [12]
%				Keedwell and Narayanan [13]
%				Motsinger et al. [14]
%				Ritchie et al. [15]
%				Motsinger-Reif et al. [16]
%				[17]
%				[18]
%				[4]
%				
%				COLUMN 2
%				Dataset Epistasimodel.
%				Childhood allergic asthma (CAA).
%				Artificial data experiments, rat spinal cord and yeast Saccharomyces Cerevisiae cell cycle.
%				Parkinson’s disease.
%				Alzheimer’s disease, breast’s disease, colorectal disease, and prostate’s disease.
%				Epitasis model.
%				Two-locus disease models, multiplicative and epistasis model.
%				Simulated human.
%				Genetic models.
%				
%				COLUMN 3
%				Description
%				GPNN and BPNN were used to model gene-gene interactions by using simulated data. The simulated data contains functional SNPs and nonfunctional SNPs which model the interaction between genes.
%				Artificial neural network was utilized with parameter decreasing method in order to analyse susceptible SNPs among the Japanese people.
%				Genetic algorithm which was implemented along with neural networks discovers gene-gene interactions in temporal gene expression dataset by elucidating the information between regulatory connections and interactions between genes, proteins, and other gene products.
%				GPNN had been used to optimize the architecture of neural network. This method can be used to enhance the identification of gene combinations associated with Parkinson’s disease.
%				GPNN had been used to detect gene-gene interactions and gene-environment interaction in studies of human disease to optimize the architecture of Neural Network by using simulated dataset.
%				GENN was utilized to discover gene-gene interactions that caused are by noise (for instance, genotyping error, missing data, phenocopy, and genetic heterogeneity) in high dimensional genetic epidemiological data.
%				NN had been used in simulation study to model the different kind of two-locus disease model by constructing six neural networks.
%				ATHENA had been used to discover the gene-gene interactions that influence complex human traits by integrating alternative tree-based crossover, back propagation, and domain knowledge in ATHENA.
%				QTGENN had applied GENN methods to quantitative traits in various types of simulated genetic models. This method had been successfully applied in single-locus models and two-locus models.
%
%
%			- ADD ALL: Table 2: Summary of detect gene-gene interaction using support vector machine method.
%				No. Author
%				Matchenko-
%				(1) Shimko and Dube
%				[23]
%				(2) Chen et al. [19]
%				(3) O ̈ zgu ̈ r et al. [24]
%				(4) Shen et al. [25]
%				(5) Ban et al. [26]
%				(6) Missiuro [21]
%				(7) Fang and Chiu [27]
%				(8) Zhang et al. [28] Marvel and
%				(9) Motsinger-Reif [29]
%				
%				Dataset
%				Simulated disease.
%				Real prostate cancer genotyping.
%				Prostate cancer.
%				Parkinson disease.
%				Type 2 diabetes mellitus-related genes.
%				Caenorhabditis elegans. COGA (genetics of
%				alcoholism). Human cancer.
%				Disease model, M1 and M2.
%				
%				Description
%				Both SVM and artificial neural network (ANN) were used to preselect the combination of SNP to test the importance of potential interactions between genes in complex disease.
%				SVM was applied in different kinds of combinatorial optimization methods which were recursive feature addition, recursive feature elimination, local search, and genetic algorithm.
%				Automatic method that was proposed to extract known genes-disease and infer unknown gene-disease association by using automatic literature mining based on dependency parsing and support vector machines.
%				Authors had employ two-stage method by using SVM with L1 penalty to detect gene-gene interactions for human complex disease.
%				SVM was used to predict the importance of gene-gene interactions in T2D in the studies of Korean cohort studies.
%				SVM was utilized in this research to detect interactions between gene in kinase families for Caenorhabditis elegans organism.
%				SVM-based PGMDR was introduced to study the interactions of gene-gene and gene-covariate in the presence or absence of main effects of genes.
%				Binary matrix shuffling filter (BMSF) as an efficient SVM search schemes was integrated with SVM to classify cancer tissue samples.
%				GESVM was applied in large dataset to select important features, parameters, or kernel in SVM.
%
%
%			- ADD ALL: %				Table 3: Summary of detect gene-gene interaction using random forest method.
%				No. Author
%				(1) Lunetta et al. [33]
%				(2) Jiang et al. [34]
%				(3) Schwarz et al. [35]
%				(4) Liu et al. [36]
%				(5) Winham et al. [32]
%				(6) Pan et al. [37]
%				(7) Staiano et al. [38] Chen and Ishwaran
%				
%				Dataset
%				H2M2, H4M2, H8M2, H16M2, H4M4, and H8M4.
%				Three simulated disease model.
%				Crohn’s disease.
%				NARAC1 and NARAC2.
%				Five models.
%				Bladder cancer.
%				Familial combined hyperlipidemia (FCH).
%				
%				Description
%				RF as a screening procedure to identify top-ranked true-associated SNPs which can cause disease without losing any interactions.
%				RF is used to recognize the cases that were against controls and to obtain the Gini importance which is used to measure the contribution of each SNP to the classification performance.
%				A new method of RJ based on basis RF knowledge was developed to facilitate a fast processing in the high-dimensional of genome-wide analysis data of gene-gene interactions.
%				RF is used to detect contributed gene-gene interactions for identifing RA susceptibility and to identify SNPs of RA patients to classify them into anticyclic citrullinated protein positive and healthy controls.
%				Focus on identifing rarely gene-gene interactions and detecting gene-gene interaction effects and their potential effectiveness on high-dimensional data using RF.
%				The proposed method of MINGRF is proposed to improve the performance of RF such as accuracy and computational time.
%				RF is used to identify gene-gene interactions that are involved in FCH. FCH increase the plasma triglycerides and/or total cholesterol level of patients and hence increase the risk of coronary heart disease.
%				RSF as new hunting pathway to detect gene correlation and genomic interactions from a high-dimensional genomic data.
%
%			- PROS AND CONS FOR EACH MOTHOD: ADD ALL!
%				Table 4: Strengths and weaknesses of neural networks, support vector machine, and random forests methods for detect gene-gene interactions.
%				
%				Methods
%				Neural network
%				Support vector machine (SVM)
%				Random forest (RF)
%				Random jungle (RJ)
%				
%				Strengths
%					Neural network
%					(i) NN is able to model the relationship between disease and single nucleotide polymorphism (SNP)
%					(ii) NN can make prediction on data where the disease outcome is unknown by learning the outcome given on a dataset (iii) NN is a method that can deal with large volumes of data
%					(iv) NN is suitable for genetic heterogeneity, high phenocopy rates, polygenic inheritance, and incomplete penetrance.
%					(v) GPNN and GENN are able to optimize the architecture of NN and possess high power to discover the presence of nonfunctional SNPs.
%					(vi) GPNN does not overfitting the data (vii) GPNN possesses high power in dealing with epitasis model with weak marginal effect
%					(viii) GENN outperform GPNN by optimiz NN in fewer generations
%					(ix) GENN possesses high power to detect high risk loci in complex disease
%
%					Support vector machine (SVM)
%					(i) SVM can deal with high dimension data set
%					(ii) SVM can be utilized to classify complex biological gene expression data (iii) Does not trap at local minima
%					(iv) Not prone to overfitting
%					(v) SVM is robust to noise
%					(vi) The output of SVM is more interpretable if compared to MDR (vii) Does not require user-defined decisions for classification
%					(viii) SVM is ready to be generalized to new structures
%
%					Random forest (RF)
%					(i) RF does not exhibit strong main effects which uncover interactions among genes. (ii) RF does not “overfit” the data.
%					(iii) SNPs predictive of a phenotype are identifying by RF.
%
%					Random jungle (RJ)
%					(i) RJ is able to analyze data on a genome-wide scale.
%					(ii) RJ has more computationally efficient than RF.
%
%			Weaknesses
%					Neural network
%					(i) Presence of black box
%					(ii) Difficult to list out all possible NN architecture and it causes the difficulty to find the optimal architecture
%					(iii) GPNN needed parallel processing environment
%					(iv) GPNN causes the high false positive rate to occur in three locus models
%					(v) The output of GPNN is binary expression, and it can be hard to interpret (for instance, up to 500 nodes)
%					(vi) Result of NN was hard to interpret due to the dimensionality problem
%					(vii) NN needs comprehensive cross-validation to confirm validity
%
%					Support vector machine (SVM)
%					(i) Presence of black box
%					(ii) SVM is restricted to pairwise classification
%					(iii) SVM cannot be directly used for feature selection
%					(iv) Result produced may be affected by the presence of missing data
%					(v) The power of SVM might reduce with the presence of genetic heterogeneity
%					(vi) Additional training maybe needed to correct the bias of prediction accuracy. However, it could be computationally expensive for the proposed procedure (vii) Accuracy produced by SVM might be suboptimal due to the SVM parameter C is forced to be one constant. Hence, a grid search for the parameter is needed by utilizing some promising SNP combinations in order to refine the results.
%
%					Random forest (RF) / Random jungle (RJ)
%					(i) Presence of black box
%					(ii) RF does not succeed in GWAS data. (iii) Sometimes RF is underestimating important scores of SNPs without marginal effects.
%					(iv) RF only detects interactions with large effect size.
%					If the main effects are weak, RJ fails to detect interactions.


%------------------------------------------------------------
% From: Bayesian inference of epistatic interactions in case-control studies (2007)
% \cite{zhang2007bayesian}
%	Topic: GWAS EPISTASIS METHOD
%
%		- Although some existing computational methods for identifying genetic interactions have been effective for small-scale studies, we here propose a method, denoted ‘bayesian epistasis association mapping’ (BEAM), for genome-wide case-control studies
%		- BEAM treats the disease-associated markers and their interactions via a bayesian partitioning model and computes, via Markov chain Monte Carlo, the posterior probability that each marker set is associated with the disease. 
%		- In the past century, scientists have made great progresses in mapping genes responsible for mendelian diseases. However, genetic variants underlying most common (or ‘complex’) diseases are non-mendelian.
%		- These variants are typically not rare in the population (42%). They show very little effect independently with low penetrance, but they may interact with each other in complex ways.
%		- It has been speculated that epistasis ubiquitously contributes to complex traits partly because of the sophisticated regulatory mechan- isms encoded in the human genome1. 
%		- EPI EXAMPLES: An increasing number of reports have indicated the presence of multilocus interactions in many human complex traits, such as breast cancer2, post-PTCA stenosis3, essential hypertension4, atrial fibrillation5 and type 2 diabetes6.
%		- GWAS EPISTASIS [Discussion]: We also applied BEAM to an association study of age-related macular degeneration (AMD)13, which included B100,000 SNP markers. Although BEAM did not find significant interactions in the AMD data set, it was able to discover two-way or three-way interactions among the B100,000 SNPs simu- lated based on the AMD data.
%
%		- EXISTING METHODS: 
%			- Several approaches have been developed to detect epistasis, including the combinatorial partitioning method (CPM)7, the restricted parti- tioning method (RPM)8, multifactor-dimensionality reduction (MDR)2, multivariate adaptive regression spline (MARS)9, the logistic regression method10 and backward genotype-trait association (BGTA)11. Although these methods all showed promise, they have been tested only on small data sets. 
%			- methods based on brute-force searches such as CPM and MDR are impractical for large data sets
%			- STEPWISE LOGISTIC REGRESSION: The stepwise logistic regression approach of ref. 12 works as follows: (i) all markers are individually tested and ranked for marginal associations with the disease; (ii) the top 10% of markers are selected, among which all k-way (k 1⁄4 2 or 3) interactions are tested and ranked for associations. The authors of ref. 12 also proposed an exhaustive logistic regression testing approach, which we choose not to consider in this study because of its prohibitive computational cost.  Note that even their stepwise approach can become computationally intractable for high-order interactions. 
%			- Recently, a simulation study12 explored the use of a stepwise logistic regression approach to identify two-way and three-way interactions. The authors demon- strated that searching for interactions in genome-wide association mapping can be more fruitful than traditional approaches that exclusively focus on marginal effects.
%
%		- BEAM METHOD:
%			- The BEAM algorithm takes case-control genotype marker data as input and produces, via MCMC simulations, posterior probabilities that each marker is associated with the disease and involved with other markers in epistasis. 
%			- The input genotyped markers should be in their natural genomic order when there is linkage disequilibrium (LD) among some of them. The method can be used either in a ‘pure’ bayesian sense or just as a tool to discover potential ‘hits’. For the former, one relies on the reported posterior probabilities to make inferential statements; as for the latter, one can take the reported hits and use another procedure to test whether these hits are statistically significant. 
%			- The latter approach is more robust to model selection and prior assumptions (such as Dirichlet priors with arbitrary parameters) and is less prone to the slow mixing problem in the MCMC computational procedure. We also propose the B statistic to facilitate the latter approach and show that it is more powerful than the standard w2 statistic for epistasis detections.
%			- For the non-epistasis model (model 1), all three epistasis mapping methods performed similarly to the single-marker w2 test (Fig. 1), indicating that the power for detecting marginal associations was not compromised by using the more complex models.
%			- Notably, results for model 4 suggest that stepwise methods can miss markers with small or no marginal effects, whereas BEAM can get these markers back through iterations.
%		- POWER ISSUES RELATED TO AF: 
%			- The power of association mapping can be greatly hampered by the discrepancy of allele frequencies between unobserved disease loci and associated genotyped markers15
%			- For data sets with large MAF discrepancies and moderate LD, the power of all methods suffered. 
%			- At the extreme case when the MAF discrepancy was maximized (that is, MAF 1⁄4 0.5), all methods had little power in detecting interaction associations
%			- The impact of LD on power seemed to be less profound than the effect of MAF discrepancy. 
%
% 	- GWAS ANALISYS:
%			- DATA: The data set contains 116,204 SNPs genotyped for 96 affected individuals and 50 controls.
%			- RESULTS: BEAM found no significant interactions associated with AMD from this data set. It is possible that the small sample size of 146 individuals is insufficient for detecting subtle epistasis interactions.

%------------------------------------------------------------
% From: FastEpistasis: a high performance computing solution for quantitative trait epistasis
% \cite{schupbach2010fastepistasis}
% TOPIC: eQTL-EPISTASIS MODEL
%
%		- We present FastEpistasis, an efficient parallel solution extending the PLINK epistasis module, designed to test for epistasis effects when analyzing continuous phenotypes.
%		- FastEpistasis is capable of testing the association of a continuous trait with all single nucleotide polymorphism (SNP) pairs from 500 000 SNPs, totaling 125 billion tests, in a population of 5000 individuals in 29, 4 or 0.5 days using 8, 64 or 512 processors.
%		- It tests epistatic effects in the normal linear regression of a quantitative response on marginal effects of each SNP and an interaction effect of the SNP pair, where SNPs are coded as additive effects, taking values 0,1 or 2. The test for epistasis reduces to testing whether the interaction term is significantly different from zero.
%		- The computations are based on applying the QR decomposition to derive least squares estimates of the interaction coefficient and its standard error. 


The definition of epistasis from a statistical perspective is a ``departure from a linear model" \cite{cordell2009detecting}. This means that in a logistic regression model the input for sample $s$ includes terms with each of the genotypes at loci $i$ and $j$), as well as an ``interaction term" $g_{s,i} \cdot g_{s,j}$ \cite{cordell2002epistasis}. 

\begin{eqnarray*} \label{eq:gwasLogRegH1}
    P( d_s | g_{s,i},g_{s,j}) & = & \phi[ \theta_0 + \theta_1 g_{s,i} + \theta_2 g_{s,j} + \theta_3 (g_{s,i} g_{s,j}) \\
    & & ... + \theta_4 c_{s,1} + ... + \theta_m c_{s,N_{cov}} ] \\
\end{eqnarray*}

where $d_s$ is disease status, $\phi(\cdot)$ is the sigmoid function, $c_{s,1}, c_{s,2}, ... $ are covariates for sample $s$.

Models involving interactions between more than two variants can be defined similarly, but require more parameters and extremely large samples are required to accurately fit them.

Several families of approaches for epistatic GWAS exist. Here we mention a few:

\begin{itemize}

\item Allele frequency: In \cite{ackermann2012systematic}, an analysis of imbalanced allele pair frequencies is performed under the assumption that an implicit test for fitness can be achieved looking for over/under-represented allele pairs in a given population. In another study \cite{zhao2006test} the authors infer that interactions can create LD in disease population under two-loci model, then they show how LD-based p-values can uncover interaction and sometimes (in their simulations) outperform logistic regression tests.

\item Bayesian model: In \cite{zhang2007bayesian}, a ``Bayesian partitioning model" is used by providing Dirichlet prior distributions for each partition and computing posterior probabilities using Markov chain Monte Carlo (MCMC) algorithms.  The methodology first test individual makers and picks only the top 10\% to further investigate for epistasis, because it is prohibitive to test all loci.

\item Machine learning: From a machine learning point of view, finding interacting variants is simply an \textit{``optimisation procedure is to find a set of parameters that allows the machine-learning model to most accurately predict class membership (e.g. affected vs unaffected)"} \cite{mckinney2006machine}. Several approaches have emerged to tackle the ``interaction problem" and used a variety of different techniques \cite{koo2013review, mckinney2006machine} , such as neural networks, cellular automata, random forests, multifactor dimensionality reduction, support vector machines, etc.

\end{itemize}

Although all these models have advantages under some assumptions, none of them seems to be a ``clear winner" over the rest \cite{cordell2009detecting}. All of these models suffer from the increase in number of tests that need to be performed, which raises two issues: i) multiple testing, which is often resolved by stringent significance threshold, and ii) computational feasibility, which is solved by efficient algorithms, parallelization, and heuristic approaches to quickly discard uninformative loci combinations. So far, no method for epistatic GWAS has been widely adopted and there is need of different approaches to be explored. In Chapter \ref{ch:gwas} we propose an approach to combine co-evolutionary models and GWAS epistasis of pairs of putatively interacting loci.


%---
\section{Thesis roadmap and Contributions}
%---

The original research presented in this thesis covers topics related to the computational and statistical methodologies related to the analysis of sequencing variants to unveil genetic links to complex disease. Broadly speaking, we address three types of problems: (i) Data processing of large datasets from high throughput biological experiments such as resequencing in the context of a GWAS (Chapter \ref{ch:bds}); (ii) functional annotations, i.e. calculating variant's impact at the molecular, cellular or even clinical level (Chapter \ref{ch:snpeff}); (iii) identification of genetic risk factors for complex disease using models that combine population-level and evolutionary-level data to detect putative epistatic interactions (Chapter \ref{ch:gwas}). When applicable, background material specific to each chapter is presented in a preface, together with an explanation of how that chapter ties in with the rest of the thesis.

This thesis comprises text and figures of articles that have either been published, submitted for publication, or ready to be submitted (waiting upon data embargo restrictions):
\\

\begin{description}
	
	\item[Chapter \ref{ch:bds}] ~ 
	
		\begin{enumerate}
			\item \textbf{P. Cingolani}, R. Sladek, and M. Blanchette. ``BigDataScript: a scripting language for data pipelines." Bioinformatics 31.1 (2015): 10-16.
		\end{enumerate}

		For this paper, PC conceptualized the idea and performed the language design and implementation. RS \& MB helped in designing robustness testing procedures. PC, RS \& MB wrote the manuscript.
		\\
	
	\item[Chapter \ref{ch:snpeff}] ~
	
		\begin{enumerate}[resume]
			\item \textbf{P. Cingolani}, A. Platts, M. Coon, T. Nguyen, L. Wang, S.J. Land, X. Lu, D.M. Ruden, et al. ``A program for annotating and predicting the effects of single nucleotide polymorphisms, snpeff: Snps in the genome of drosophila melanogaster strain $w^{1118}; iso-2; iso-3$". Fly, 6(2), 2012.
		\end{enumerate}

		For this paper, PC conceptualized the idea, implemented the program and performed testing.
		AP contributed several feature ideas, software testing and suggested improvements.
		XL, DR, SL, LW, TN, MC, LW performed mutagenesis and sequencing experiments.
		XL and DR performed the biological interpretation of the data.
		All authors contributed to the manuscript.
		\\

		SnpEff's accompanying publication (SnpSift):
	
		\begin{enumerate}[resume]		
			\item \textbf{P. Cingolani}, V. M. Patel, M. Coon, T. Nguyen, S. Land, D. M. Ruden, and X. Lu.`` Using drosophila melanogaster as a model for genotoxic chemical mutational studies with a new program, snpsift". Toxicogenomics in non-mammalian species, page 92, 2012.
		\end{enumerate}
		
		~ \\

		We used SnpEff \& SnpSift and developed a number of new functionalities in the context of two collaborative GWAS projects on type II diabetes:
	
		\begin{enumerate}[resume]
		
			\item M. McCarthy, T2D Genes Consortia. ``Variation in protein-coding sequence and predisposition to type 2 diabetes", Ready for submission.
			
			\item A. Mahajan, X. Sim, H. Ng, A. Manning, M. Rivas, H. Heather, A. Locke, N. Grarup, H. K. Im, \textbf{P. Cingolani}, et. al. ``Identification and Functional Characterization of G6PC2 Coding Variants Influencing Glycemic Traits Define an Effector Transcript at the G6PC2-ABCB11 Locus." PLoS genetics 11.1 (2015): e1004876-e1004876.
		
		\end{enumerate}
		~ \\
	
	\item[Chapter \ref{ch:gwas}] ~
	
		\begin{enumerate}[resume]
		\item \textbf{P. Cingolani}, R. Sladek, and M. Blanchette. ``A co-evolutionary approach for detecting epistatic interactions in genome-wide association studies". Ready for submission (data embargo restrictions).
		\end{enumerate}
	
		For this paper, PC designed the methodology under the supervision of MB and RS. PC implemented the algorithms. PC, RS \& MB wrote the manuscript. This work uses data from the T2D consortia, thus it cannot be published until the main T2D paper is accepted for publication (according to T2D data embargo).
		\\
	
	\item[Other contributions] ~	\linebreak
		During my thesis I have co-authored several other scientific articles (grouped by topic) published, submitted for publication, or ready to be submitted, not mentioned in this thesis:
		\\

	\item[Epigenetics] ~

		\begin{enumerate}[resume]
			\item \textbf{P. Cingolani}, X. Cao, R. Khetani, C.C. Chen, M. Coon, A. Bollig-Fischer, S. Land, Y. Huang, M. Hudson, M. Garfinkel, and others. ``Intronic Non-CG DNA hydroxymethylation and alternative mRNA splicing in honey bees." BMC genomics 14.1 (2013): 666.
			\item M. Senut, A. Sen, \textbf{P. Cingolani}, A. Shaik, S. Land, Susan J and D. M. Ruden. ``Lead exposure disrupts global DNA methylation in human embryonic stem cells and alters their neuronal differentiation." Toxicological Sciences (2014).
			\item D. M. Ruden, \textbf{P. Cingolani}, A. Sen, W. Qu, L. Wang, M. Senut, M. Garfinkel, V. Sollars, X. Lu, ``Epigenetics as an answer to Darwin's 'special difficulty' Part 2: Natural selection of metastable epialleles in honeybee castes", Frontiers in Genetics (2015).
			\item M. Senut, A. Sen, \textbf{P. Cingolani}, A. Shaik, S. Land, Susan J and D. M. Ruden. ``Lead exposure induces changes in 5-hydroxymethylcytosine clusters in CpG islands in human embryonic stem cells and umbilical cord blood", Submitted to `Epigenomics.
			\item M. Senut, \textbf{P. Cingolani}, A. Sen, Arko, A. Kruger, A. Shaik, H. Hirsch, S. Suhr, D. Ruden. ``Epigenetics of early-life lead exposure and effects on brain development." Epigenomics 4.6 (2012): 665-674.
		\end{enumerate}
		~ \\
	
	\item[GWAS \& Disease] ~
	
		\begin{enumerate}[resume]
			\item K. Oualkacha, Z. Dastani, R. Li, \textbf{P. Cingolani}, T. Spector, C. Hammond, J. Richards, A. Ciampi, C. Greenwood. ``Adjusted sequence kernel association test for rare variants controlling for cryptic and family relatedness." Genetic epidemiology 37.4 (2013): 366-376.
			\item S. Bongfen, I. Rodrigue-Gervais, J. Berghout, S. Torre, \textbf{P. Cingolani}, S. Wiltshire, G. Leiva-Torres, L. Letourneau, R. Sladek, M. Blanchette, and others. ``An N-ethyl-N-nitrosourea (ENU)-induced dominant negative mutation in the JAK3 kinase protects against cerebral malaria." PloS one 7.2 (2012): e31012.
			\item C. Meunier, L. Van Der Kraak, C. Turbide, N. Groulx, I. Labouba, Ingrid, \textbf{P. Cingolani}, M. Blanchette, G. Yeretssian, A. Mes-Masson, M. Saleh, and others. ``Positional mapping and candidate gene analysis of the mouse Ccs3 locus that regulates differential susceptibility to carcinogen-induced colorectal cancer." PloS one 8.3 (2013): e58733.
			\item G. Caignard, G. Leiva-Torres, M. Leney-Greene, B. Charbonneau, A. Dumaine, N. Fodil-Cornu, M. Pyzik, \textbf{P. Cingolani}, J. Schwartzentruber, J. Dupaul-Chicoine, and others. ``Genome-wide mouse mutagenesis reveals CD45-mediated T cell function as critical in protective immunity to HSV-1." PLoS pathogens 9.9 (2013): e1003637.
			\item M. Bouttier, D. Laperriere, M. Babak Memari, M. Verway, E. Mitchell, \textbf{P. Cingolani}, T. Wang, M. Behr, R. Sladek, M. Blanchette, S. Mader and J. White. ``Genomics analysis reveals elevated LXRα signaling reduces M. tuberculosis viability", Submitted to Journal of Clinical Investigation.
			\item M. Bouttier, D. Laperriere, M. Babak Memari, M. Verway, E. Mitchell, \textbf{P. Cingolani}, T. Wang, M. Behr, R. Sladek, M. Blanchette, S. Mader and J. White. ``Genomic analysis of enhancers engaged in M. tuberculosis-infected macrophages reveals that LXR signaling reduces mycobacterial burden", Submitted to PLOS Pathogens.
		\end{enumerate}	
		~ \\
	
	\item[Fuzzy logic] ~

		\begin{enumerate}[resume]
			\item \textbf{P. Cingolani} and Jesus Alcala-Fdez. ``jFuzzyLogic: a robust and flexible Fuzzy-Logic inference system language implementation." FUZZ-IEEE. 2012.
			\item \textbf{P. Cingolani} and Jesus Alcala-Fdez. ``jFuzzyLogic: a java library to design fuzzy logic controllers according to the standard for fuzzy control programming." International Journal of Computational Intelligence Systems (2013), vol 6, pages 65-75.
		\end{enumerate}	

\end{description}
